name,title,selftext,subreddit,title_text
t3_glhbet,Spark Partitions,,bigdata,spark partitions 
t3_glbdff,Hadoop Distributed File System - A comprehensive guide,"From a computing perspective, there are essentially 2 types of scaling — vertical and horizontal. In vertical scaling, we simply add more RAM and storage to a single computer/machine aka “node”. In horizontal scaling, we add more nodes connected through a common network, thereby increasing the overall capacity of the system. With that in mind, let’s dive in.

**Block Size**

&amp;#x200B;

[ File split into blocks](https://i.redd.it/qmd4qw5y2az41.gif)

When a file is saved in HDFS, the file is broken into smaller chunks or “blocks”, as can be seen in the GIF above. The number of blocks is dependent on the “Block Size”. The default is *128 MB* but can be changed/configured easily.

In our example, a 500 MB file needs to be broken into blocks of 128 MB. *500/128 = 3 blocks of 128 MB and 1 block of 116 MB.* The residual block space of 12 MB is returned back to the name node for usage elsewhere, thus preventing any wastage. This is true of any file system really, for example, Windows NTFS has a block size between 4 KB and 64 KB depending on file size (up to 256 TB). Considering petabytes and above for Big Data processing, KBs would be highly inefficient, as you can imagine. This is why HDFS has a block size of 128 MB.

**Replication Factor**

&amp;#x200B;

[ Replication](https://i.redd.it/grt1zi433az41.gif)

HDFS is a fault-tolerant and resilient system, meaning it prevents a failure in a node from affecting the overall system’s health and allows for recovery from failure too. In order to achieve this, data stored in HDFS is automatically replicated across different nodes.

How many copies are made? This depends on the “replication factor”. By default, it is set to 3 i.e. 1 original and 2 copies. This is also easily configurable.

In the GIF to the left, we see a file broken into blocks and each block replicated across other data nodes for redundancy.

**Storage and Replication Architecture**

&amp;#x200B;

[ Storage and Replication Architecture](https://preview.redd.it/wks8jpj63az41.png?width=2500&amp;format=png&amp;auto=webp&amp;s=ba698b9e9537064fb591e975f53d6367258fc9ff)

Hadoop Distributed File System (HDFS) follows a *Master — Slave* architecture, wherein, the ‘Name Node’ is the master and the ‘Data Nodes’ are the slaves/workers. This simply means that the name node monitors the health and activities of the data node. The data node is where the file is actually stored in blocks.

Let's continue with the same example of a file of size = 500 MB in the image above. With HDFS’ default block size of 128 MB, this file is broken into 4 blocks B1 — B4. Please note that A — E are our Data Nodes. With HDFS’ default replication factor of 3, the blocks are replicated across our 5 node cluster. Block B1 (in yellow) is replicated across Nodes A, B and D and so on and so forth (follow the coloured lines).Here, the Name Node maintains the metadata, i.e. data about data. *Which replica of which block of which file is stored in which node* is maintained in NN — replica 2 of block B1 of file xyz.csv is stored in node B.

So a file of size 500 MB requires a total storage capacity in HDFS of 1500 MB due to its replication. This is abstracted from the end users’ perspective and the user can only see 1 file of size 500 MB stored within HDFS.

Continue reading here - [Hadoop Distributed File System](https://towardsdatascience.com/hadoop-distributed-file-system-b09946738555)",bigdata,    file system  a comprehensive   from a computing perspective there are essentially   types of scaling — vertical   horizontal in vertical scaling we simply   more ram   storage to a single computermachine aka “ ” in horizontal scaling we   more     through a common network thereby increasing the overall capacity of the system with that in   let’s   in  block size  x   b     when a file is   in   the file is broken into smaller chunks or “blocks” as can be seen in the gif above the number of blocks is   on the “block size” the   is     mb but can be   easily  in our example a     mb file   to be broken into blocks of     mb           blocks of     mb     block of     mb the   block space of    mb is   back to the name   for usage elsewhere thus preventing any wastage this is true of any file system really for example   ntfs has a block size between   kb      kb   on file size up to     tb   petabytes   above for big   processing kbs   be highly inefficient as you can imagine this is why   has a block size of     mb  replication factor  x   b       is a faulttolerant   resilient system meaning it prevents a failure in a   from affecting the overall system’s health   allows for recovery from failure too in   to achieve this     in   is automatically   across      how many copies are   this   on the “replication factor” by   it is set to   ie   original     copies this is also easily configurable  in the gif to the left we see a file broken into blocks   each block   across other     for    storage   replication architecture  x   b         file system   follows a master — slave architecture wherein the ‘name  ’ is the master   the ‘   ’ are the slavesworkers this simply means that the name   monitors the health   activities of the     the     is where the file is actually   in blocks  lets continue with the same example of a file of size      mb in the image above with  ’   block size of     mb this file is broken into   blocks b  — b  please note that a — e are our     with  ’   replication factor of   the blocks are   across our     cluster block b  in yellow is   across   a b       so on   so forth follow the   lineshere the name   maintains the   ie   about   which replica of which block of which file is   in which   is   in nn — replica   of block b  of file xyzcsv is   in   b  so a file of size     mb requires a total storage capacity in   of      mb   to its replication this is   from the   users’ perspective   the user can only see   file of size     mb   within    continue   here   
t3_gkqdor,Laughing at Big Data – eBook – Great new insight into realities of IT,,bigdata,laughing at big   – ebook – great new insight into realities of it 
t3_gkw1lm,Why I called bullshit on the data lakehouse nonsense,,bigdata,why i   bullshit on the   lakehouse nonsense 
t3_gkqu9a,What Is Data Warehouse As a Service and Why Would You Need It,"One of the big headaches of a traditional data warehouse is its hardware and software infrastructure - data warehouses usually require a lot of data storage and computing power. With Data Warehouse As a Service (DWaaS), you get to outsource those infrastructure headaches to someone else.

[Understanding Data Warehouse-as-a-Service Benefits Today And Tomorrow](https://blog.panoply.io/data-warehouse-as-a-service-benefits-today-and-tomorrow) - the article explains how DWaaS makes infrastructure setup much easier, drastically cut or even eliminate the need of maintaining its infrastructure, lets you dynamically modify the scale of your data warehouse operation as your business circumstances change, and automate most the work of a traditional data warehouse engineering team.",bigdata,what is   warehouse as a service   why   you   it one of the big   of a     warehouse is its     software infrastructure    warehouses usually require a lot of   storage   computing power with   warehouse as a service   you get to outsource those infrastructure   to someone else     the article explains how   makes infrastructure setup much easier   cut or even eliminate the   of maintaining its infrastructure lets you     the scale of your   warehouse operation as your business circumstances change   automate most the work of a     warehouse engineering team
t3_gkeozm,Big Data: Its Impact and Significance,,bigdata,big   its impact   significance 
t3_gkdqs3,Computational social science #bigdatalearning #learning #socialnetworksanalysis #onlinecourse,"Hi from the University of California

Interested in learning more about Computational Social Science? The University of California is offering a Specialization on Coursera that features distinguished professors from all 10 UC campus. The specialization consists of five courses: ""[Computational Social Science Methods](https://www.coursera.org/learn/computational-social-science-methods?specialization=computational-social-science-ucdavis)"", ""[Big Data + A.I. + Ethics](https://www.coursera.org/learn/big-data-ai-ethics?specialization=computational-social-science-ucdavis)"", ""[Social Network Analysis](https://www.coursera.org/learn/social-network-analysis?specialization=computational-social-science-ucdavis)"", ""[Computer Simulations](https://www.coursera.org/learn/computer-simulations?specialization=computational-social-science-ucdavis)"", and a ""[Capstone Project](https://www.coursera.org/learn/css-capstone)"". At the end of the course, students will have web-scraped data from the web, visualized and analyzed their own social network, found hidden patterns with machine learning and natural language processing, and created agent-based computer simulations of artificial societies. If you are interested in learning more, please check out the link to the course page below!

[https://www.youtube.com/watch?v=kXmh90XOPcg&amp;feature=youtu.be](https://www.youtube.com/watch?v=kXmh90XOPcg&amp;feature=youtu.be)

[https://www.coursera.org/specializations/computational-social-science-ucdavis#courses](https://www.coursera.org/specializations/computational-social-science-ucdavis#courses)",bigdata,computational social science   learning socialnetworksanalysis onlinecourse hi from the university of california    in learning more about computational social science the university of california is offering a specialization on coursera that features   professors from all    uc campus the specialization consists of five courses           a   at the   of the course   will have     from the web       their own social network     patterns with machine learning   natural language processing       computer simulations of artificial societies if you are   in learning more please check out the link to the course page below    
t3_gkakk1,Webinar on How To Choose the Right Data Science Program For Your Career,,bigdata,webinar on how to choose the right   science program for your career 
t3_gk59h2,Role of Web Scraping in the E-commerce Industry,"[E commerce web scraping](https://www.loginworks.com/ecommerce-web-scraping) provides a bird’s eye view of pricing data, market dynamics, prevailing patterns, practices your competitors employ, and the challenges they face. Web scraping **collect product data from e-commerce websites**. Therefore, with all the above pointers in mind, you can position your product, which will give you an advantage over the others.

Explore full details on latest blog about [**Role of Web Scraping in the E-commerce Industry**](https://www.loginworks.com/blogs/role-of-web-scraping-in-ecommerce-industry/).",bigdata,role of web scraping in the ecommerce       a  ’s eye view of pricing   market   prevailing patterns practices your competitors employ   the challenges they face web scraping collect     from ecommerce websites therefore with all the above pointers in   you can position your   which will give you an   over the others  explore full   on latest blog about  
t3_gk2oof,Doing redesign of Statistics without Borders non-profit organization,"Hi everyone! I’m a UX designer student and my team is working on a redesign of a non-profit organization website related to statistics. We need your help with this 5-question survey, so we can gain insights about the website and the users.

Your help will be very appreciated! 

[https://docs.google.com/forms/d/e/1FAIpQLSeMewwIKETfifobRSyAq5liqqKUv\_tXvF3mArj96kPcgnWnGQ/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSeMewwIKETfifobRSyAq5liqqKUv_tXvF3mArj96kPcgnWnGQ/viewform?usp=sf_link)

Thank youuuuu!",bigdata,    of statistics without   nonprofit organization hi everyone i’m a ux       my team is working on a   of a nonprofit organization website   to statistics we   your help with this  question survey so we can gain insights about the website   the users  your help will be very       thank youuuuu
t3_gk1k0d,Understanding BDaaS and Its Types,,bigdata,      its types 
t3_gk8jo6,THE TEN BIG DATA COMMANDMENTS,,bigdata,the ten big     
t3_gjruxn,Utilizing the Big Data Card in the Time of Coronavirus Pandemic,,bigdata,utilizing the big     in the time of coronavirus   
t3_gjqqds,Launching for the first time: Data &amp; AI Association for the Middle East and Africa (DAIA) this May 16-17th!,"Check the post &amp; support; invite members from MEA to be represented.

[https://www.linkedin.com/posts/activity-6666360887521103872-yvZt](https://www.linkedin.com/posts/activity-6666360887521103872-yvZt)",bigdata,launching for the first time    ai association for the   east   africa   this may     th check the post  support invite members from mea to be    
t3_gjl34e,REVEALING WEALTH: USING BIG DATA TO FIGHT TAX EVASION – MARTYN JONES,,bigdata,revealing wealth using big   to fight tax evasion – martyn jones 
t3_gjobg6,WILL INDIA BEAT CHINA in population? 150 years of population data proved it. Will you believe it?,,bigdata,will   beat china in population     years of population     it will you believe it 
t3_gj1am1,Principles of lazy data documentation — and how to get your team onboard,"Great blog on how to make data documentations less painful. Curious as to how others have gained buy in from their team for data documentation testing?

https://blog.quiltdata.com/principles-of-lazy-data-documentation-and-how-to-get-your-team-onboard-bb674b78ea73",bigdata,principles of lazy     —   how to get your team   great blog on how to make     less painful curious as to how others have   buy in from their team for     testing  
t3_gjff61,Answering Key Questions about IBM’s MDM Version 11 Upgrade,"IBM brought together Initiate Master Data Service (MDS), InfoSphere MDM Server (MDM) and InfoSphere MDM Server for PIM into a single market offering as InfoSphere MDM v10.  The market offering contained four editions: standard, advanced, collaboration and enterprise.

In InfoSphere MDM v11, IBM further unified the products from a technology perspective.  Specifically, the legacy Initiate MDS and MDM Server products were **combined** together into a single technology platform.

This is a significant achievement that positions IBM to address the “MDM Journey” that is much talked about.  It allows clients to start with a Registry Style (or “Virtual Hub”, which is easier to start with and then transition to a Hybrid or Centralized Style (or “Physical Hub”).  The key differentiator is the true implementation of the Hybrid Style.

Know More:  [https://blogs.mastechinfotrellis.com/answering-key-questions-about-ibms-mdm-version-11-upgrade](https://blogs.mastechinfotrellis.com/answering-key-questions-about-ibms-mdm-version-11-upgrade)",bigdata,answering key questions about ibm’s   version      ibm brought together initiate master   service   infosphere   server     infosphere   server for pim into a single market offering as infosphere   v    the market offering   four       collaboration   enterprise  in infosphere   v   ibm further   the   from a technology perspective  specifically the legacy initiate       server   were   together into a single technology platform  this is a significant achievement that positions ibm to   the “  journey” that is much   about  it allows clients to start with a registry style or “virtual hub” which is easier to start with   then transition to a   or   style or “physical hub”  the key   is the true implementation of the   style  know more  
t3_gjdte3,Airflow Apache learning,"Could you please advice me what is the best tutorial or course for learning Airflow apache?
Or if someone has an experience, please DM.",bigdata,airflow apache learning   you please   me what is the best tutorial or course for learning airflow apache or if someone has an experience please  
t3_gjhjma,"Understanding society through #BigData, #SocialNetworkAnalysis, and #ComputerSimulations","Understanding society through #BigData, #SocialNetworkAnalysis, and #ComputerSimulations? #UofCalifornia offers an open online Specialization on #ComputationalSocialScience, teaching you things like #webscraping #data and using #machinelearning.

https://www.coursera.org/specializations/computational-social-science-ucdavis#courses

&amp;#x200B;

[https://ucdavis.app.box.com/s/30pgaszf7d2sf8h81va9vvtw60n4kq74/file/659780600371](https://ucdavis.app.box.com/s/30pgaszf7d2sf8h81va9vvtw60n4kq74/file/659780600371)",bigdata,  society through   socialnetworkanalysis   computersimulations   society through   socialnetworkanalysis   computersimulations uofcalifornia offers an open online specialization on computationalsocialscience teaching you things like webscraping     using machinelearning    x   b  
t3_gjaqx0,Repositorio de todas las emisiones de Sendero hacia Db2 v9.7 en Linux,"**CONOCENOS**

ESTAREMOS DE NUEVO PUBLICANDO TEMAS NUEVOS.!

Primera pagina sobre hechos SOCIALES, luego orientamos en tópicos de interés sobre Db2

[https://drive.google.com/drive/folders/0B4W0AfdZsJsdalZDcDZ1MnZWRkE](https://drive.google.com/drive/folders/0B4W0AfdZsJsdalZDcDZ1MnZWRkE)",bigdata,repositorio     las emisiones     hacia   v   en linux conocenos  estaremos   nuevo   temas nuevos  primera pagina sobre hechos sociales luego orientamos en tópicos   interés sobre    
t3_gj6w3a,What is Greenplum Database? Intro to the Big Data Database,,bigdata,what is greenplum   intro to the big     
t3_giziml,Blogged: Building a Telegram Bot Powered by Apache Kafka and ksqlDB,,bigdata,    a telegram bot   by apache kafka     
t3_giz2cp,Best Machine Learning Courses to Learn,,bigdata,best machine learning courses to learn 
t3_gis3xu,Understand Real Life Challenges from Big Data,,bigdata,  real life challenges from big   
t3_gimpq7,"FREE Data Science &amp; Visualization Virtual Summit May 19 - 20 (Harvard, Charter Communications, Quansight, Epigen Technologies, OmniSci) - FREE T-SHIRT","Register here: [http://www2.omnisci.com/l/298412/2020-05-06/8rh7d](http://www2.omnisci.com/l/298412/2020-05-06/8rh7d)

**Join OmniSci May 19 - 20 for our final FREE Virtual Summit** of our three-part summit series. During this summit you will hear from **Harvard Center for Geographic Analysis, Charter Communications, Quansight, Epigen Technologies, and OmniSci experts**. Sign up today and you will receive an **OmniSci branded Star Wars T-Shirt (see below)** and you'll be entered to win an **Apple HomePod.**

Here are some sessions that you won't want to miss:

**How to do Large Scale Data Research on a Slurm HPC Cluster with OmniSci** with Devika Kakkar, Geospatrial Data Scientist and Ben Lewis, Geospatial Technology Manager, , Harvard Center for Geographic Analysis

**OmniSci in Action: Learn from a Telecom Leader** with Jared Ritter, Senior Director Analytics &amp; Automation, Charter Communications

**Exploring without moving: Further adventures with Ibis, Altair and OmniSci, part 2** with Venkat Krishnamurthy, VP Product, OmniSci

Register here: [http://www2.omnisci.com/l/298412/2020-05-06/8rh7d](http://www2.omnisci.com/l/298412/2020-05-06/8rh7d)

&amp;#x200B;

https://preview.redd.it/5hpq5gn6yey41.png?width=1200&amp;format=png&amp;auto=webp&amp;s=6eb6747e742b60c555f98eb95f3c6d15393b9011",bigdata,free   science  visualization virtual summit may          charter communications quansight epigen technologies omnisci  free tshirt register here   join omnisci may        for our final free virtual summit of our threepart summit series   this summit you will hear from   center for geographic analysis charter communications quansight epigen technologies   omnisci experts sign up     you will receive an omnisci   star wars tshirt see below   youll be   to win an apple    here are some sessions that you wont want to miss  how to   large scale   research on a slurm hpc cluster with omnisci with   kakkar geospatrial   scientist   ben lewis geospatial technology manager    center for geographic analysis  omnisci in action learn from a telecom   with   ritter senior   analytics  automation charter communications  exploring without moving further   with ibis altair   omnisci part   with venkat krishnamurthy vp   omnisci  register here   x   b  
t3_gimaiy,What To Know About The Influence of Big Data on Business Financing,,bigdata,what to know about the influence of big   on business financing 
t3_gie8sb,"[Advice] Data Warehouse not cutting it for what we need to be looking at. Looking for an alternative, is a Data Lake right for us?","Not sure this is the best place to post, if there is a better place, please let me know.

I've been tasked with coming up with a proposal to provide new data reports, and dashboards. I have a small team (10 people) that I am putting together and is a great opportunity to investigate what our possibilities are. This is also a great time to bone up on training. My initial thought was to create a separate data warehouse / data mart, but I'm not sure creating a second warehouse is the right way to go.


What we have:

* 50 b2b Customers (40k end users)
* Each customer is set up with one of 3 main products. (on MS SQL or ORACLE)
* Each customer also has additional supplemental products that we host, on separate database of some sort, mostly MS SQL and a few on FoxPro. This would be for different services, including financial.
* service-now is used for support.
* Crystal reports with Crystal server
* Tableau desktop licenses (currently no one is using them, also no Tableau server)
* ~~Qlick view instance that will not be renewed this year (none of the customers want to keep using Qlick, its just too complex for end users)~~
* Outsourced Data warehouse. 
* Canned Cognos reports from the Date Warehouse(we can not directly edit or create new reports). We are also able to connect via ODBC if we beg hard enough.
* We also receive data sets on a yearly schedule, mostly in some sort of csv, tab or pdf flat file, right now these are hand entered into the main database.


There is always a large gap in the data warehouse loading from June till mid October, were no new data will be loaded. The data-warehouse is then loaded roughly every 2 week to 3 months, depending on the customer. Furthermore what the data warehouse collects is not modifiable. There are different data elements customers what to report on, but we are not able to use the warehouse for those purposes.  (If you haven't guessed by now, this is in the public sector.)


As a organization, we are swallowing the Amazon AWS kool-aid and are using the hosting more and more for network services (active directory), but have not for databases.


I saw the AWS S3 with Redshift, Athena or EMR. I'm not sure what we should realistically be looking at. Is a data lake something we should doing at our size?
I potentially have the money saved from Qlik to use for some of this.


I also saw Amazon Quick Sights, which I never heard of. Is that a viable alternative to Qlick/Tableau dashboarding?",bigdata,    warehouse not cutting it for what we   to be looking at looking for an alternative is a   lake right for us not sure this is the best place to post if there is a better place please let me know  ive been   with coming up with a proposal to   new   reports     i have a small team    people that i am putting together   is a great opportunity to investigate what our possibilities are this is also a great time to bone up on training my initial thought was to create a separate   warehouse    mart but im not sure creating a   warehouse is the right way to go   what we have      b b customers   k   users  each customer is set up with one of   main   on ms sql or oracle  each customer also has   supplemental   that we host on separate   of some sort mostly ms sql   a few on foxpro this   be for   services   financial  servicenow is   for support  crystal reports with crystal server  tableau   licenses currently no one is using them also no tableau server  qlick view instance that will not be   this year none of the customers want to keep using qlick its just too complex for   users      warehouse     cognos reports from the   warehousewe can not     or create new reports we are also able to connect via   if we beg   enough  we also receive   sets on a yearly   mostly in some sort of csv tab or   flat file right now these are     into the main     there is always a large gap in the   warehouse   from june till   october were no new   will be   the   is then   roughly every   week to   months   on the customer furthermore what the   warehouse collects is not   there are     elements customers what to report on but we are not able to use the warehouse for those purposes  if you havent   by now this is in the public sector   as a organization we are swallowing the amazon aws     are using the hosting more   more for network services active   but have not for     i saw the aws s  with   athena or emr im not sure what we   realistically be looking at is a   lake something we     at our size i potentially have the money   from qlik to use for some of this   i also saw amazon quick sights which i never   of is that a viable alternative to qlicktableau  
t3_gi7y7q,"Data Engineering Stack - collect, transform and visualize geospatial data","I'm making a side project, where I collect geospatial data by web scrapping and from OSM API. I've started with simple Java application, however, I would like to make it as a data flow, purely for learning purposes.

Unfortunately, my knowledge about tools, and mostly connecting them, is, well, low.

What is my goal?  
As a final result I want to visualize scrapped geospatial points on the map with the roads connecting them(from OSM).

Current flow:  
In standalone Java application I'm scrapping the data for geospatial points. There is a client consuming the OSM API for needed data.

What I think it might be useful:  
Use Apache Spark for collecting and transforming the data. Then use somehow GeoSpark, or Geotrellis, and Zeppelin to visualize the data. I was also thinking about using ES + Kibana for geodata, but it looks like the Zeppelin is enough.  
I feel comfortable to work with Java, then Scala.

What do you think? Are there any better tools I can use? Did I miss anything?",bigdata,  engineering stack  collect transform   visualize geospatial   im making a   project where i collect geospatial   by web scrapping   from osm api ive   with simple java application however i   like to make it as a   flow purely for learning purposes  unfortunately my   about tools   mostly connecting them is well low  what is my goal   as a final result i want to visualize   geospatial points on the map with the   connecting themfrom osm  current flow   in   java application im scrapping the   for geospatial points there is a client consuming the osm api for      what i think it might be useful   use apache spark for collecting   transforming the   then use somehow geospark or geotrellis   zeppelin to visualize the   i was also thinking about using es  kibana for   but it looks like the zeppelin is enough   i feel comfortable to work with java then scala  what   you think are there any better tools i can use   i miss anything
t3_gic8n2,An interview with StreamNative co-founder Sijie Guo about his experience contributing to the Pulsar framework for streaming data and its community.,,bigdata,an interview with streamnative   sijie guo about his experience contributing to the pulsar framework for streaming     its community 
t3_gic4vd,Free Data Strategy Canvas,,bigdata,free   strategy canvas 
t3_gi56f9,Dockerizing a Kafka Streams app,,bigdata,  a kafka streams app 
t3_gi9rr7,Governed Data Lake for Customer Critical Data Analytics,"Retail chains that have brick and mortar stores as well as online platforms often struggle in identifying the customers visiting their site. Even with all the information available at their disposal, the probability of identifying the customers accessing their website is a mere 30%.

This blog discusses on the system used to tracking and identifying customers who interact with our client’s online portal and then shop in-store and vice-versa.

Know More:  [https://blogs.mastechinfotrellis.com/governed-data-lake-customer-critical-data-analytics](https://blogs.mastechinfotrellis.com/governed-data-lake-customer-critical-data-analytics)",bigdata,    lake for customer critical   analytics retail chains that have brick   mortar stores as well as online platforms often struggle in   the customers visiting their site even with all the information available at their   the probability of   the customers accessing their website is a mere     this blog   on the system   to tracking     customers who interact with our client’s online portal   then shop instore   viceversa  know more  
t3_gi9hvv,What data mining in done in the fight against the Coronavirus,,bigdata,what   mining in   in the fight against the coronavirus 
t3_gi5z60,DataGene: Data Similarity Comparison in Python (50+ Functions),"This project might be appreciated by this community. It gives one the ability to compare your various datasets to ensure that they are similar for maintenance and modelling purpose.  It can be used to compare test and training sets, as well as original and synthetically general data.

See the GitHub project for additional details:  [https://github.com/firmai/datagene](https://github.com/firmai/datagene) 

If you want to run an interactive session use colab: [https://colab.research.google.com/drive/1QSDTKvNiwc1IRCX\_VYr9TRFusdX1gLMM?usp=sharing](https://colab.research.google.com/drive/1QSDTKvNiwc1IRCX_VYr9TRFusdX1gLMM?usp=sharing)",bigdata,    similarity comparison in python    functions this project might be   by this community it gives one the ability to compare your various   to ensure that they are similar for maintenance     purpose  it can be   to compare test   training sets as well as original   synthetically general    see the github project for         if you want to run an interactive session use colab 
t3_gi61sr,What is one issue that you faced implementing Hadoop framework in a company and how did you overcome that problem ?,Just trying to understand some real time problems of implementing Hadoop,bigdata,what is one issue that you   implementing   framework in a company   how   you overcome that problem  just trying to   some real time problems of implementing  
t3_gi4xk8,Mutual exclusion in mapreduce distributed systems,Why mutual exclusion is required in mapreduce distributed systems?,bigdata,mutual exclusion in     systems why mutual exclusion is   in     systems
t3_ghnqic,How to create streaming application with many input sources,"Hi all. I am struggling how to design an streaming application when many sources can change.

We have the application currently as a batch nightly process that recalculates all of the data each night. It joins 5 data sources together. All of these data sources can change and have new data coming in.


If this was a streaming application the more input streams the higher the cardinality of the data. It just seems to get really mess quickly and data could get out of sync which what it should be.

The business reason for streaming is cost savings.

How do you guys design an application like this?",bigdata,how to create streaming application with many input sources hi all i am struggling how to   an streaming application when many sources can change  we have the application currently as a batch nightly process that recalculates all of the   each night it joins     sources together all of these   sources can change   have new   coming in   if this was a streaming application the more input streams the higher the   of the   it just seems to get really mess quickly       get out of sync which what it   be  the business reason for streaming is cost savings  how   you guys   an application like this
t3_ghqmur,Converting your spark-submit command to Livy payload,"
On a general note, applications are submitted to Apache Spark  through `spark-submit` command. Apache Livy is a much more neat way of submitting your Spark applications through its [well-defined JSON schema](https://livy.incubator.apache.org/docs/latest/rest-api.html). Now, there existed no out-of-the-box tool to convert such a submit command to Livy payload JSON format. Therefore, I made one 🙂.

This library also supports the validation of Livy payloads through simple and well-known validation rules (https://spark.apache.org/docs/latest/configuration.html).

You can check out the source at [https://github.com/rounakdatta/livy-utils](https://github.com/rounakdatta/livy-utils). And the hosted demo is [here](https://rounakdatta.github.io/livy-utils/).",bigdata,converting your sparksubmit   to livy    on a general note applications are   to apache spark  through sparksubmit   apache livy is a much more neat way of submitting your spark applications through its   now there   no outofthebox tool to convert such a submit   to livy   json format therefore i   one 🙂  this library also supports the   of livy   through simple   wellknown   rules   you can check out the source at  
t3_gho6tb,Third Party Support Hortonworks Data Platform 2.6,"Hello, everyone,

I don't know if I'm in the right place, but I'll try my luck.

I am currently looking for a support provider for HDP 2.6, which is used commercially by us (23 of 25 possible servers, 3 clusters with 4 cores, 128GB memory, 48TB per system).

At the moment we get support through a framework contract, but only until 31.12.2020. We would like to extend this support without changing the version.

I would be grateful for suggestions.",bigdata,  party support hortonworks   platform    hello everyone  i   know if im in the right place but ill try my luck  i am currently looking for a support   for      which is   commercially by us    of    possible servers   clusters with   cores    gb memory   tb per system  at the moment we get support through a framework contract but only until          we   like to   this support without changing the version  i   be grateful for suggestions
t3_ghmex0,Looking for rent estimator to determine income. Zillow does not allow offline storage,"I have addresses for potential clients, and want to match this with income. My original idea was to scrape or use an API to gather home values and predict income. 

Zillow has blocked my scraper, and their API hasnt let me update my settings(Plus they say no offline data storage). 

My first client has about 10,000 homes needed, but they are a small local business(Zillow's max per day is 1k). I imagine in the future I may need more. 

Any suggestions how I can use an address to determine income? (As a note, this particular area has two types of homes, 100k homes and 500k homes. Both on the same street)",bigdata,looking for rent estimator to   income zillow   not allow offline storage i have   for potential clients   want to match this with income my original   was to scrape or use an api to gather home values     income   zillow has   my scraper   their api hasnt let me   my settingsplus they say no offline   storage   my first client has about       homes   but they are a small local businesszillows max per   is  k i imagine in the future i may   more   any suggestions how i can use an   to   income as a note this particular area has two types of homes    k homes      k homes both on the same street
t3_ghh3cd,Microsoft Azure Storage Tips for Big Data,,bigdata,microsoft azure storage tips for big   
t3_ghk9u6,Python Libraries Used for Web Scraping,"There are hundreds of web scraping  packages out there, but you need only a handful to scrap nearly any  site. This guide is an opinionated one. We have decided to feature the [most famous web scraping](https://www.loginworks.com/web-scraping-services) Python libraries that we like the most. We cover all the important bases together, and they are well-documented.",bigdata,python libraries   for web scraping there are   of web scraping  packages out there but you   only a   to scrap nearly any  site this   is an   one we have   to feature the   python libraries that we like the most we cover all the important bases together   they are  
t3_ghcs95,How does one prepare for a Data Engineering Intern position task,"as the title says, I am fairly new to this field in particular, I have done a fair amount of ML, and DL in the past, aswell as Python and R, and touched on cloud services abit, what should be the things I focus on now? since the things I learnt were heavily baised towards the machine learning side

Thanks",bigdata,how   one prepare for a   engineering intern position task as the title says i am fairly new to this   in particular i have   a fair amount of ml     in the past aswell as python   r     on   services abit what   be the things i focus on now since the things i learnt were heavily     the machine learning    thanks
t3_gh7jcx,Need help finding a data set about high school graduation or high school drop out rates by state in the US.,Hi I’m trying to find a data set about high school drop out rates or high school graduation rates by state in the US. Time period needs to be 2000-2019 or something at least around those years. Thank you in advance for your help.,bigdata,  help   a   set about high school   or high school   out rates by state in the us hi i’m trying to   a   set about high school   out rates or high school   rates by state in the us time     to be          or something at least   those years thank you in   for your help
t3_ggpp75,Looking for data sets to apply advanced analytics,"Hi,
I installed Cloudera manager and Hadoop services to work on advanced analytics. So my question is whare can I find huge data sets to do data process and advance analytics on it?
Also, could you advice an online tutorial or full scenario that explan Hadoop services in practice not theoretical?",bigdata,looking for   sets to apply   analytics hi i     manager     services to work on   analytics so my question is whare can i   huge   sets to     process     analytics on it also   you   an online tutorial or full scenario that explan   services in practice not theoretical
t3_ggv09z,Bigdata for beginners,,bigdata,  for beginners 
t3_ggpcro,"What's the best datastore of 30 million rows of an array of 512 int32s (the bert input ids)? Each array has an index, and we need to retrieve a couple thousand of these at a time.","I Tried pd.HDFStore, but it seems you can't store arrays with this, and retrieval seems to crash ram. Details: https://www.reddit.com/r/learnpython/comments/ggoyfa/appending_pandas_data_to_hdf_store_getting/",bigdata,whats the best   of    million rows of an array of     int  s the bert input   each array has an     we   to retrieve a couple   of these at a time i     but it seems you cant store arrays with this   retrieval seems to crash ram   
t3_ggecyi,Best Data Science Courses to Learn,,bigdata,best   science courses to learn 
t3_gg651b,"Resources to learn Hadoop, Hive, Spark?","I need to learn Hadoop, Hive and Spark for an internship I just started, can someone please take a look at this [**link**](https://www.javatpoint.com/what-is-big-data)**:** and let me know if it is a good resource to get started with these technologies?

If its not can you please provide me with some resources which I could use to learn these technologies?",bigdata,resources to learn   hive spark i   to learn   hive   spark for an internship i just   can someone please take a look at this     let me know if it is a   resource to get   with these technologies  if its not can you please   me with some resources which i   use to learn these technologies
t3_gg31u3,Apache Druid Production Setup in Google Cloud Platform with Dataproc cluster — Part 1,,bigdata,apache     setup in google   platform with   cluster — part   
t3_gg0xnr,One Week Reminder for the precisionFDA Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge,"As a friendly reminder for those of you participating in the [precisionFDA Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge](https://go.usa.gov/xv7sz), you have one week left before final submissions are due on May 18th.  

If you have any questions about the challenge, please feel free to post them in this thread and we will respond as quickly as possible.",bigdata,one week   for the   gaining new insights by     event anomalies using   open   challenge as a     for those of you participating in the   you have one week left before final submissions are   on may   th    if you have any questions about the challenge please feel free to post them in this     we will   as quickly as possible
t3_gg36kl,Airflow help: aggregating data across tasks?,"I'm a beginner with python and big data, and I'm trying to aggregate (sum) some columns and calculate the difference between each daily sum.

I have a dataset split into daily CSVs, each for each day. Each run waits for a new daily csv, then goes through a fetching, cleaning/transforming and loading tasks.

The dataset is about covid-19, and I'm trying to get the cumulative from each column (""confirmed"", ""deaths"", ""recovered"") and then subtract that to the previous day's respective cumulative value (calculates new daily 'cases').
My problem is I can't figure out how to get the previous run cumulative values and generally make it work. I've searched for similar cases but to no avail.

Here's my task's code, I know the If condition is crap.

`import pandas as pd
from datetime import datetime as dt
from datetime import date, timedelta

LOCAL_DIR='/tmp/'

def date_to_str(pendulum_date):
    return dt.fromtimestamp(pendulum_date.timestamp()).strftime(""%Y-%m-%d"")

def main(**kwargs):
    # Pass the execution date
    exec_date = date_to_str(kwargs[""execution_date""])
    
    # Create the dataframe from fetched data file
    dataset = pd.read_csv(LOCAL_DIR + exec_date + '_raw_fetch.csv')

    # convert column names to lower
    dataset.columns = [col.lower() for col in dataset.columns] 
    
    # Rename columns
    dataset.rename(columns={'province/state':'state','country/region':'country'}, inplace=True)
    
    # Convert 'date' column dates into '2020-01-22'
    dataset[""date""] = pd.to_datetime(dataset[""date""], format='%m/%d/%y')

##-----------------------------------------------------------------------------------##
    ## Append daily totals to total_daily if exec_date is not the same as start_date
    if exec_date != date_to_str(kwargs[""prev_execution_date""]):
        total_daily = total_daily.append(dataset.groupby('date').agg({ 
        'confirmed':'sum',
        'deaths':'sum',
        'recovered':'sum'
        }))
    else: total_daily = dataset.groupby('date').agg({ 
        'confirmed':'sum',
        'deaths':'sum',
        'recovered':'sum'
        })
    
    total_daily[['confirmed_new', 'deaths_new', 'recovered_new']] = total_daily[['confirmed', 'deaths', 'recovered']].diff()`",bigdata,airflow help aggregating   across tasks im a beginner with python   big     im trying to aggregate sum some columns   calculate the   between each   sum  i have a   split into   csvs each for each   each run waits for a new   csv then goes through a fetching cleaningtransforming     tasks  the   is about     im trying to get the cumulative from each column         then subtract that to the previous   respective cumulative value calculates new   cases my problem is i cant figure out how to get the previous run cumulative values   generally make it work ive   for similar cases but to no avail  heres my tasks   i know the if   is crap  import   as   from   import   as   from   import                 return      mainkwargs      pass the execution                       create the   from     file              rawfetchcsv       convert column names to lower                     rename columns       inplacetrue           convert   column   into                                  totals to   if   is not the same as       if                                                                else                                                              
t3_gfqyc2,"Wharton School Receives $5 Million to Launch Artificial Intelligence for Business, Extending Its Commitment to Analytics, Learning, and Engagement",,bigdata,wharton school receives   million to launch artificial intelligence for business   its commitment to analytics learning   engagement 
t3_gg1i3n,How AI Is Accelerating Drug Development for COVID-19,,bigdata,how ai is accelerating     for   
t3_gg3t4u,Successful practices with Spark reading datasource from remote machine,"**Problem:** I am working on a use case where data is generated by the IoT devices like sensors and stored locally with the nodes. These node/clusters are not part of my productions and but since they hold the data, I somehow want to read it in my spark structured stream.

**What am I doing?** I am using a TCP socket to read data from remote machine and create input datastream. But I very well realize that this is not a good practice as socket source should not be used for production applications! It does not support recovery.

**What I can not do:** I do not have have an access to FTP to read those files as vendor does not allow that. Neither I find moving those files into my on-premise S3 a good choice as this is going to consume exponential amount of space. I have considered mounting that source directory via NFS to my production cluster too. But not sure how good that is. 

I am fairly new to Spark and still learning it by doing things. Hence I need some expert opinions where I can have a workaround or hopefully smart solution for this.",bigdata,successful practices with spark     from remote machine problem i am working on a use case where   is   by the iot   like sensors     locally with the   these   are not part of my     but since they   the   i somehow want to   it in my spark   stream  what am i   i am using a tcp socket to     from remote machine   create input   but i very well realize that this is not a   practice as socket source   not be   for   applications it   not support recovery  what i can not   i   not have have an access to ftp to   those files as     not allow that neither i   moving those files into my onpremise s  a   choice as this is going to consume exponential amount of space i have   mounting that source   via nfs to my   cluster too but not sure how   that is   i am fairly new to spark   still learning it by   things hence i   some expert opinions where i can have a   or hopefully smart solution for this
t3_gfwtnp,Looking for a sample parquet file,"I am looking for a sample parquet file of fairly large size (1 GB+ preferred) .

Any suggestions?

&amp;#x200B;

Thank you",bigdata,looking for a sample parquet file i am looking for a sample parquet file of fairly large size   gb     any suggestions  x   b  thank you
t3_gfslyj,Augmented analytics: Are you ready?,,bigdata,  analytics are you   
t3_gfsbnw,How to find Top N similar rows in Spark/Spark SQL given id/row?,"I want to find similar rows, I have researched a bit but couldn't find much.I thought of groupby but doesn't seem efficient.I have some sample data like this

`{""customer"":""customer-13"",""attributes"":{""att-a"":""att-a-6"",""att-b"":""att-b-9"",""att-c"":""att-c-15"",""att-d"":""att-d-12"",""att-e"":""att-e-10"",""att-f"":""att-f-8"",""att-g"":""att-g-1"",""att-h"":""att-h-11"",""att-i"":""att-i-14"",""att-j"":""att-j-2""}}`

`{""customer"":""customer-14"",""attributes"":{""att-a"":""att-a-4"",""att-b"":""att-b-1"",""att-c"":""att-c-2"",""att-d"":""att-d-13"",""att-e"":""att-e-4"",""att-f"":""att-f-9"",""att-g"":""att-g-10"",""att-h"":""att-h-4"",""att-i"":""att-i-15"",""att-j"":""att-j-3""}}`

`{""customer"":""customer-15"",""attributes"":{""att-a"":""att-a-9"",""att-b"":""att-b-9"",""att-c"":""att-c-15"",""att-d"":""att-d-7"",""att-e"":""att-e-10"",""att-f"":""att-f-12"",""att-g"":""att-g-5"",""att-h"":""att-h-3"",""att-i"":""att-i-5"",""att-j"":""att-j-4""}}`

`{""customer"":""customer-16"",""attributes"":{""att-a"":""att-a-15"",""att-b"":""att-b-11"",""att-c"":""att-c-13"",""att-d"":""att-d-14"",""att-e"":""att-e-7"",""att-f"":""att-f-8"",""att-g"":""att-g-7"",""att-h"":""att-h-8"",""att-i"":""att-i-3"",""att-j"":""att-j-6""}}`

I give the customer Id and number N to find top similar customers. Any idea of how to go about it?

For example, given customer-20 find the top 5 similar customers.

I'm relatively new. Thanks",bigdata,how to   top n similar rows in sparkspark sql given   i want to   similar rows i have   a bit but     muchi thought of groupby but   seem efficienti have some sample   like this              i give the customer     number n to   top similar customers any   of how to go about it  for example given customer     the top   similar customers  im relatively new thanks
t3_gfr8sv,Apache Flink | No Java Required: Configuring Sources and Sinks in SQL,,bigdata,apache flink  no java   configuring sources   sinks in sql 
t3_gfljc7,Analytics on the edge using IBM Cloud Pak for Data,,bigdata,analytics on the   using ibm   pak for   
t3_gfpz0l,Costs around a Notebook for training on Spark,"Hi everyone!   
 I would like to play with some datasets using a Notebook (Zeppelin or other notebook). The data I imagine would be stored in a S3 bucket and I want to be able to import to and download from there after processing.    
Does anyone know what would be the costs for a setup? I know it all can be achieved in AWS but I wonder if someone has something similar and can give me a monthly estimate of the cost.    


The datasets can require let's say no more than 20GB in total, starting with just hundreds of MB and increase, but also cleaned up after a while.  
Thanks!",bigdata,costs   a notebook for training on spark hi everyone     i   like to play with some   using a notebook zeppelin or other notebook the   i imagine   be   in a s  bucket   i want to be able to import to     from there after processing       anyone know what   be the costs for a setup i know it all can be   in aws but i   if someone has something similar   can give me a monthly estimate of the cost       the   can require lets say no more than   gb in total starting with just   of mb   increase but also   up after a while   thanks
t3_gfl4de,Difference between Big Data and Machine Learning,,bigdata,  between big     machine learning 
t3_gfns7v,What Is the Importance of Scraped Social Media Data?,"The usage of scraping tools is increasing day by day. [Data scraped](https://www.loginworks.com/data-scraping) from social media is probably human behavior’s biggest and most complex data set. It provides brand-new opportunities for social scientists and business experts to understand people, groups, and society, and to discover the great wealth contained in the data.

Social media analytics study of methods, tools, and networks points out that early-time social media data analysts were traditional companies in the retail and finance industries that used social media analytics to leverage brand awareness, boost customer service, marketing strategies, and even detect fraud.",bigdata,what is the importance of   social     the usage of scraping tools is increasing   by     from social   is probably human behavior’s biggest   most complex   set it     opportunities for social scientists   business experts to   people groups   society   to   the great wealth   in the    social   analytics   of   tools   networks points out that earlytime social     analysts were   companies in the retail   finance   that   social   analytics to leverage   awareness boost customer service marketing strategies   even    
t3_gezig6,Big Data Engineers Path,,bigdata,big   engineers path 
t3_gfbane,What site should I use if I want online training over Summer?,"Hi, I'm an OSEH major looking for certificates to do over the summer (Since I can't really do anything else right now)

Are there any specific sites I should go to for ideal training? I'm willing to pay if it means I get quality training out of it.",bigdata,what site   i use if i want online training over summer hi im an oseh major looking for certificates to   over the summer since i cant really   anything else right now  are there any specific sites i   go to for   training im willing to pay if it means i get quality training out of it
t3_gfczeb,This Professor Says We've Been Looking At The Coronavirus Data Wrong,,bigdata,this professor says weve been looking at the coronavirus   wrong 
t3_gfbqpf,Decoding Role of Big Data in Policy Outcomes,,bigdata,  role of big   in policy outcomes 
t3_gfhdal,Learn Machine Learning From Scratch Practically -Udemy Course - 199.99$ OFF (FREE Limited Time), **Course for Anyone looking to Learn All About Machine Learning :** [https://www.web-learning.tech/2020/05/learn-machine-learning-from-scratch.html](https://www.web-learning.tech/2020/05/learn-machine-learning-from-scratch.html),bigdata,learn machine learning from scratch practically   course        off free   time  course for anyone looking to learn all about machine learning  
t3_gf8npq,Should data science be considered its own discipline?,,bigdata,    science be   its own   
t3_gf9oov,"METODOLOGÍA DE BOX JENKINS VS REDES NEURONALES ARTIFICIALES PARA CONSTRUIR UN MODELO DE PRONÓSTICO DEL PRECIO DE COMPRA DE CIERRE MENSUAL DE LAS ACCIONES DEL BANCO DE CRÉDITO DEL PERÚ EN LA BOLSA DE VALORES DE LIMA, ABRIL DE 2005 HASTA FEBRERO DE 2018",,bigdata,    box jenkins vs   neuronales artificiales para construir un     pronóstico   precio   compra   cierre mensual   las acciones   banco       perú en la bolsa   valores   lima abril        hasta febrero        
t3_gf16yf,Automate Data Quality with Informatica IDQ,"Data Quality is the process of understanding the quality of data attributes such as data types, data patterns, existing values, and so on. Data quality is also about capturing the score of an attribute based on some specific constraints. For example, get the count of records for which the attribute value is NULL, or find the count of records for which a date attribute does not fit into the specified Date Pattern. 

Know More:  [https://blogs.mastechinfotrellis.com/automate-data-quality-informatica-idq](https://blogs.mastechinfotrellis.com/automate-data-quality-informatica-idq)",bigdata,automate   quality with informatica     quality is the process of   the quality of   attributes such as   types   patterns existing values   so on   quality is also about capturing the score of an attribute   on some specific constraints for example get the count of   for which the attribute value is null or   the count of   for which a   attribute   not fit into the     pattern   know more  
t3_ges0ki,Check Out The Upcoming Federal Crowdsourcing Webinar Series: A precisionFDA Strike for Public Health,"Did you know that the U.S. Food and Drug Administration (FDA) regulates products that account for roughly 25% of the U.S. economy? That’s right: The Agency reviews and approves a lot more than drugs and vaccines. Its purview extends to medical devices, dietary supplements, foods, and more—and it’s constantly monitoring for their potential health risks to the public.

PrecisionFDA, a secure, cloud-based platform that aids the study and analyses of large biological datasets, is helping to inform big data analyses by hosting crowdsourced data science challenges.

If you are interested in learning more about precisionFDA’s mission and how they help monitor FDA regulated products, you can virtually attend the next Federal Crowdsourcing Webinar Series: A precisionFDA Strike for Public Health, hosted by Challenge.gov and Digital.gov on May 12th. Event participants will learn how crowdsourced challenges are helping the FDA better understand evolving areas of science around large biological data sets and apply that understanding to inform regulatory science.

For more information and to register, please visit [Digital.gov](https://go.usa.gov/xvwHR)!",bigdata,check out the upcoming     webinar series a   strike for public health   you know that the us           regulates   that account for roughly    of the us economy that’s right the agency reviews   approves a lot more than     vaccines its purview   to       supplements     more—  it’s constantly monitoring for their potential health risks to the public    a secure   platform that   the     analyses of large biological   is helping to inform big   analyses by hosting     science challenges  if you are   in learning more about  ’s mission   how they help monitor       you can virtually   the next     webinar series a   strike for public health   by challengegov     on may   th event participants will learn how   challenges are helping the   better   evolving areas of science   large biological   sets   apply that   to inform regulatory science  for more information   to register please visit  
t3_geqt2n,COVID-19 Has a Data Governance Problem,,bigdata,  has a   governance problem 
t3_geuszg,Is AI overhyped?,,bigdata,is ai   
t3_gekrqw,A deep-dive on how http3 works,,bigdata,a   on how  works 
t3_gelkxy,Does anyone have good resources for me to read/listen on what big data says about healthy relationships?,,bigdata,  anyone have   resources for me to   on what big   says about healthy relationships 
t3_geqxth,"Best Coronavirus Projections, Predictions, Dashboards and Data Resources",,bigdata,best coronavirus projections         resources 
t3_gem5xq,Telegram channel - Data Science Digest - Join us today!,,bigdata,telegram channel    science    join us   
t3_gelg9m,"The big-data “wow wow” factor – big data, analytics and trading",,bigdata,the   “wow wow” factor – big   analytics     
t3_gel2bv,EXPLORACIÓN Y MANIPULACIÓN DE DATOS EN RSTUDIO,,bigdata,exploración y manipulación     en   
t3_gedo95,Seeking Participants for Focus Group Research on COVID-19 Impact (college undergrads who took calculus in spring 2020),"National study is recruiting undergraduates who took a calculus course in spring semester for a focus group study on student experiences during the COVID-19 pandemic.  Participants in this study will fill out a survey and take part in a 1-hour focus group where you will be asked about negative and positive experiences you had related to your transition away from campus and learning remotely. Focus group participants will be compensated $50 for their time. Results from this study may help universities better support students during these unprecedented, challenging times. This study is funded by the National Science Foundation. To indicate your interest in participating, please fill out this survey and we will contact you with further information: [https://pennstate.qualtrics.com/jfe/form/SV\_af0mJB3k6lUVQLr](https://pennstate.qualtrics.com/jfe/form/SV_af0mJB3k6lUVQLr)",bigdata,seeking participants for focus group research on   impact college   who took calculus in spring      national   is recruiting   who took a calculus course in spring semester for a focus group   on   experiences   the      participants in this   will fill out a survey   take part in a  hour focus group where you will be   about negative   positive experiences you     to your transition away from campus   learning remotely focus group participants will be      for their time results from this   may help universities better support     these   challenging times this   is   by the national science   to   your interest in participating please fill out this survey   we will contact you with further information 
t3_gebsuk,Accelerate innovation with AI for app modernization,,bigdata,accelerate innovation with ai for app   
t3_ge098q,HDP and HDF... what is gonna happen to our open source customers?,"Simple but very important question... we have some customers with HDP and HDF running dev and production environments... at this point in time, we even have new customers asking us to install new Cloudera or Hortonworks clusters, but they don't want to have the cost of licensing... 

What is going to happen to HDP? In the future can will we be able to use Cloudera/Hortonworks repos to install HDP?

I've been trying to build Ambari via Apache website and it has been a huge pain in the ass...(build problems).

Many thanks to everyone who can help with some info, insight, opinion regarding this matter.",bigdata,      what is gonna happen to our open source customers simple but very important question we have some customers with       running       environments at this point in time we even have new customers asking us to install new   or hortonworks clusters but they   want to have the cost of licensing   what is going to happen to   in the future can will we be able to use   repos to install    ive been trying to   ambari via apache website   it has been a huge pain in the   problems  many thanks to everyone who can help with some info insight opinion   this matter
t3_gdyf28,Leveraging the power of Jupyter Notebooks,"Hey everyone!

Wanted to share this article with everyone for data scientist or budding data scientists. A majority of people like to use Jupyter notebook for their data analysis, data cleaning or data pipeline using python script. It could be the case that you’re not using the full potential of your machine and essentially end up spending more time on tasks which otherwise should’ve been executed quicker. Here’s the medium article on ‘Leveraging the power of Jupyter notebooks’

https://towardsdatascience.com/leveraging-the-power-of-jupyter-notebooks-26b4b8d7c622",bigdata,leveraging the power of jupyter notebooks hey everyone    to share this article with everyone for   scientist or     scientists a majority of people like to use jupyter notebook for their   analysis   cleaning or   pipeline using python script it   be the case that you’re not using the full potential of your machine   essentially   up   more time on tasks which otherwise  ’ve been   quicker here’s the   article on ‘leveraging the power of jupyter notebooks’  
t3_gdxpky,25 Best Data Science Courses Online 2020,,bigdata,   best   science courses online      
t3_gdtoe2,How AI And Big Data Are Transforming The Customer Experience,,bigdata,how ai   big   are transforming the customer experience 
t3_gdy7rb,ESTADÍSTICA DESCRIPTIVA PARA LA INVESTIGACIÓN,,bigdata,    para la investigación 
t3_gdtdap,The Definitive Data Scientist Setup,,bigdata,the     scientist setup 
t3_gdm4b6,How fast does big data change? Planning on taking a course made 4 years ago.,"Hello, title says it all.

I want to get to know the big data world (not becoming an expert, just learning about it and be able to work in projects and grow my knowledge if I like the field). The thing is, I found a course that looks interesting, and it takes a couple of months or three to complete, but it was made in 2016. 

I wonder if the technologies would have changed enough to be stupid to take the course or it is fine and I would learn the basis anyway.

Could someone guide me in what path to follow here?

The series of courses im planning on taking are the specialization in Big Data from the University of California San Diego in the coursera platform: https://www.coursera.org/specializations/big-data

Thanks for any help you could give me in this topic.",bigdata,how fast   big   change planning on taking a course     years ago hello title says it all  i want to get to know the big     not becoming an expert just learning about it   be able to work in projects   grow my   if i like the   the thing is i   a course that looks interesting   it takes a couple of months or three to complete but it was   in        i   if the technologies   have   enough to be   to take the course or it is fine   i   learn the basis anyway    someone   me in what path to follow here  the series of courses im planning on taking are the specialization in big   from the university of california san   in the coursera platform   thanks for any help you   give me in this topic
t3_gdsu0t,Here Is How You Can Apply Software Development Best Practices to Analytics Pipelines,,bigdata,here is how you can apply software   best practices to analytics pipelines 
t3_gdliyo,When a Data Warehouse Can’t Keep it Real-Time,,bigdata,when a   warehouse can’t keep it realtime 
t3_gdxg0i,LAUGHING@BIGDATA,"LAUGHING@BIGDATA

Real insider views on Agile, AI, data, deep learning, data warehousing, data lakes, IT management and leadership, and much more.

Amazing! Incredible! Marvellous! The next best thing in IT!

Possibly polemic, quaintly contrarian &amp; immediately insightful. It informs, educates &amp; entertains. Amusing food for thought and action! And there's a lot of it. You won't be left indifferent.

USA: http://www.amazon.com/dp/B086HS6VWX
UK: http://www.amazon.co.uk/dp/B086HS6VWX
Germany: http://www.amazon.de/dp/B086HS6VWX
France: http://www.amazon.fr/dp/B086HS6VWX
Spain: http://www.amazon.es/dp/B086HS6VWX
Italy: http://www.amazon.it/dp/B086HS6VWX
Netherlands: http://www.amazon.nl/dp/B086HS6VWX
Japan: http://www.amazon.co.jp/dp/B086HS6VWX
Brazil: http://www.amazon.com.br/dp/B086HS6VWX
Canada: http://www.Amazon.ca/dp/B086HS6VWX
Mexico: http://www.amazon.com.mx/dp/B086HS6VWX
Australia: http://www.amazon.com.au/dp/B086HS6VWX
India: http://www.amazon.in/dp/B086HS6VWX

Thanks a million!

Martyn 'Data'  Jones",bigdata,     real   views on agile ai     learning   warehousing   lakes it management       much more  amazing   marvellous the next best thing in it  possibly polemic quaintly contrarian    insightful it informs    entertains amusing   for thought   action   theres a lot of it you wont be left    usa  uk  germany  france  spain  italy     japan  brazil     mexico  australia      thanks a million  martyn    jones
t3_gdp9q0,"I'm considering computer science and statistics for college major. At first I was going to major in both and minor in philosophy, then realized if I majored in stats and minored in CS and philosophy and do master's in CS I'd apply beloved field (stats) with professional skills (CS)"," I want to work with stats.  However CS opens up much more employers in  big data, ML, ect.  That's why I was thinking the master's degree  (professional  skills) should be CS and the undergrad (area of interest)  should be stats (with minors in CS and philosophy).   However, I'm  reading in Q that I have it backwards.   The recommendation is to switch  it so master's is stats and undergrad is CS (with minors in stats and   philosophy).    What do I do!!??",bigdata,im   computer science   statistics for college major at first i was going to major in both   minor in philosophy then   if i   in stats     in cs   philosophy     masters in cs   apply     stats with professional skills cs  i want to work with stats  however cs opens up much more employers in  big   ml ect  thats why i was thinking the masters    professional  skills   be cs   the   area of interest    be stats with minors in cs   philosophy   however im    in q that i have it     the   is to switch  it so masters is stats     is cs with minors in stats     philosophy    what   i  
t3_gdeval,How to Get Your Digital Data Game on with DataOps,,bigdata,how to get your     game on with   
t3_gddclm,Big Data &amp; Cloud Computing: The Roles &amp; Relationships,,bigdata,big      computing the roles  relationships 
t3_gdjdj1,An interview with Amar Arsikere about the complexities of data operations at enterprise scale and the approach that Infoworks taken to make it manageable.,,bigdata,an interview with amar arsikere about the complexities of   operations at enterprise scale   the approach that infoworks taken to make it manageable 
t3_gd8gqt,HIVE NIFI ORC java.lang.IllegalArgumentException: Column has the wrong number of index entries found: 0 expected: 1,"I have a pipeline in **NIFI** and everything works fine except sometimes(very rarely) some record go to the retry queue in **PUTHIVESTREAMING** processor when trying to write to **HIVE**. The nifi logs show the exception

&amp;#x200B;

`ERROR [put-hive-streaming-0] o.a.h.h.streaming.AbstractRecordWriter Unable to close org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater[hdfs://hdfscluster/user/hive/metastore/bi_sureyield_db.db/bi_sureyield_events/year=2020/month=1/day=10/delta_127318799_127318808/bucket_00000] due to: Column has wrong number of index entries found: 0 expected: 1 java.lang.IllegalArgumentException: Column has wrong number of index entries found: 0 expected: 1 at org.apache.orc.impl.WriterImpl$TreeWriter.writeStripe(WriterImpl.java:695) at org.apache.orc.impl.WriterImpl$StructTreeWriter.writeStripe(WriterImpl.java:2147) at org.apache.orc.impl.WriterImpl.flushStripe(WriterImpl.java:2661) at org.apache.orc.impl.WriterImpl.close(WriterImpl.java:2834) at org.apache.hadoop.hive.ql.io.orc.WriterImpl.close(WriterImpl.java:321) at org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.close(OrcRecordUpdater.java:502) at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.closeBatch(AbstractRecordWriter.java:218) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl$6.run(HiveEndPoint.java:998) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl$6.run(HiveEndPoint.java:995) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.closeImpl(HiveEndPoint.java:994) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.markDead(HiveEndPoint.java:760) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.commit(HiveEndPoint.java:854) at org.apache.nifi.util.hive.HiveWriter$4.call(HiveWriter.java:237) at org.apache.nifi.util.hive.HiveWriter$4.call(HiveWriter.java:234) at org.apache.nifi.util.hive.HiveWriter.lambda$null$3(HiveWriter.java:373) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.nifi.util.hive.HiveWriter.lambda$callWithTimeout$4(HiveWriter.java:373) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 2020-04-29 14:55:39,542 ERROR [put-hive-streaming-0] o.a.h.h.streaming.AbstractRecordWriter Unable to close org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater[hdfs://hdfscluster/user/hive/metastore/bi_sureyield_db.db/bi_sureyield_events/year=2020/month=1/day=10/delta_127318799_127318808/bucket_00001] due to: null java.nio.channels.ClosedChannelException: null at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1521) at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:104) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58) at java.io.DataOutputStream.write(DataOutputStream.java:107) at org.apache.orc.impl.PhysicalFsWriter$BufferedStream.spillToDiskAndClear(PhysicalFsWriter.java:286) at org.apache.orc.impl.PhysicalFsWriter.finalizeStripe(PhysicalFsWriter.java:337) at org.apache.orc.impl.WriterImpl.flushStripe(WriterImpl.java:2665) at org.apache.orc.impl.WriterImpl.close(WriterImpl.java:2834) at org.apache.hadoop.hive.ql.io.orc.WriterImpl.close(WriterImpl.java:321) at org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.close(OrcRecordUpdater.java:502) at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.closeBatch(AbstractRecordWriter.java:218) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl$6.run(HiveEndPoint.java:998) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl$6.run(HiveEndPoint.java:995) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.closeImpl(HiveEndPoint.java:994) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.markDead(HiveEndPoint.java:760) at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.commit(HiveEndPoint.java:854) at org.apache.nifi.util.hive.HiveWriter$4.call(HiveWriter.java:237) at org.apache.nifi.util.hive.HiveWriter$4.call(HiveWriter.java:234) at org.apache.nifi.util.hive.HiveWriter.lambda$null$3(HiveWriter.java:373) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.nifi.util.hive.HiveWriter.lambda$callWithTimeout$4(HiveWriter.java:373) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(`[`Thread.java:748`](https://Thread.java:748)`)`

&amp;#x200B;

&amp;#x200B;

I'm unable to understand where the issue lies and how to fix it.",bigdata,hive nifi orc javalangillegalargumentexception column has the wrong number of   entries         i have a pipeline in nifi   everything works fine except sometimesvery rarely some   go to the retry queue in puthivestreaming processor when trying to write to hive the nifi logs show the exception  x   b  error     unable to close      to column has wrong number of   entries         javalangillegalargumentexception column has wrong number of   entries         at orgapacheorcimplwriterimpltreewriterwritestripewriterimpljava    at orgapacheorcimplwriterimplstructtreewriterwritestripewriterimpljava     at orgapacheorcimplwriterimplflushstripewriterimpljava     at orgapacheorcimplwriterimplclosewriterimpljava     at   at   at   at   at   at     at   at   at   at   at   at orgapachenifiutilhivehivewriter callhivewriterjava    at orgapachenifiutilhivehivewriter callhivewriterjava    at   at     at   at   at   at javautilconcurrentfuturetaskrunfuturetaskjava    at   at   at                      error     unable to close      to null   null at   at   at   at   at   at orgapacheorcimplphysicalfswriterfinalizestripephysicalfswriterjava    at orgapacheorcimplwriterimplflushstripewriterimpljava     at orgapacheorcimplwriterimplclosewriterimpljava     at   at   at   at   at   at     at   at   at   at   at   at orgapachenifiutilhivehivewriter callhivewriterjava    at orgapachenifiutilhivehivewriter callhivewriterjava    at   at     at   at   at   at javautilconcurrentfuturetaskrunfuturetaskjava    at   at   at     x   b  x   b  im unable to   where the issue lies   how to fix it
t3_gcyfsl,Looking for something to do and want to learn about Computational Social Science?,"Hello everyone!

I took this course and I think this group would be interested in, called Computational Social Science on Coursera. The course is offered by the University of California and features professors from all 10 UC campuses. The specialization consists of five courses: ""[Computational Social Science Methods](https://www.coursera.org/learn/computational-social-science-methods?specialization=computational-social-science-ucdavis)"", ""[Big Data + A.I. + Ethics](https://www.coursera.org/learn/big-data-ai-ethics?specialization=computational-social-science-ucdavis)"", ""[Social Network Analysis](https://www.coursera.org/learn/social-network-analysis?specialization=computational-social-science-ucdavis)"", ""[Computer Simulations](https://www.coursera.org/learn/computer-simulations?specialization=computational-social-science-ucdavis)"", and a ""[Capstone Project](https://www.coursera.org/learn/css-capstone)"". The project involves analyzing your own social network and creating computer simulations of artificial societies. If you would like to further your knowledge of big data and machine learning, I would definitely recommend you check out this course. Also, Coursera is offering these courses for **FREE** due to COVID-19. I would recommend for you to check it out if you’re interested!",bigdata,looking for something to     want to learn about computational social science hello everyone  i took this course   i think this group   be   in   computational social science on coursera the course is   by the university of california   features professors from all    uc campuses the specialization consists of five courses           a   the project involves analyzing your own social network   creating computer simulations of artificial societies if you   like to further your   of big     machine learning i       you check out this course also coursera is offering these courses for free   to   i     for you to check it out if you’re  
t3_gd4jpo,Big Data &amp; Cloud Computing: The Roles &amp; Relationships,,bigdata,big      computing the roles  relationships 
t3_gcfhnt,What kind of massive user data exists? (preferably free),"EDIT: To clarify, looking for something that has First and Last name, maybe address? Right now I'm starting with US voter registry. 

Looking to do some ML/Data science, but not sure where to get user data. I imagine facebook/instagram would only work for a short time before I'm blocked/request limited.

It could be as simple as voter registration data or leaked databases. Wasnt there some big republican database leak a while ago?

Any suggestions would be great.",bigdata,what   of massive user   exists preferably free   to clarify looking for something that has first   last name maybe   right now im starting with us voter registry   looking to   some   science but not sure where to get user   i imagine facebookinstagram   only work for a short time before im      it   be as simple as voter registration   or     wasnt there some big republican   leak a while ago  any suggestions   be great
t3_gcr7ro,EXPLORACIÓN Y MANIPULACIÓN DE DATOS EN RSTUDIO,,bigdata,exploración y manipulación     en   
t3_gc6uxd,What is your preferred Worfklow tool?,"What do you guys use to schedule and chain spark jobs together?

I am on an on prem cloudera Hadoop cluster",bigdata,what is your   worfklow tool what   you guys use to     chain spark jobs together  i am on an on prem     cluster
t3_gc9v5i,COVID-19 Is No Worse Than the Flu — And Other Dangerous Myths Busted With Data,,bigdata,  is no worse than the flu —   other   myths   with   
t3_gc7acc,GRAFICANDO DISTRIBUCIONES CONTINUAS CON MEDIAS NO CENTRALES CON R,,bigdata,    continuas con   no centrales con r 
t3_gbxq7i,Apache Griffin,"Are there any Apache Griffin users / experts here?

If so, can you share experiences, dos, don'ts etc.?",bigdata,apache griffin are there any apache griffin users  experts here  if so can you share experiences     etc
t3_gbpabj,Are We Listening? A View Into Americans' Compliance With COVID-19 Movement Restrictions,,bigdata,are we listening a view into americans compliance with   movement restrictions 
t3_gbfygy,Big Data to track Covid-19,"I've just read the opening chapter to Viktor Mayor Schonberger book Big Data: A Revolution That Will Transform How We Live, Work, and Think. It describes how big data from Google was able to track the H1N1 virus and how google data was more efficient it tracking the spread of the virus than CDC. ""With it, by the time the next pandemic comes around, the world will have a better tool at its disposal to predict and thus prevent its spread."" I'm just wondering in what ways big data is being used to track the current pandemic?",bigdata,big   to track   ive just   the opening chapter to viktor mayor schonberger book big   a revolution that will transform how we live work   think it   how big   from google was able to track the h n  virus   how google   was more efficient it tracking the   of the virus than   with it by the time the next   comes   the   will have a better tool at its   to     thus prevent its   im just   in what ways big   is being   to track the current  
t3_gbspi4,Top 15 Most Sold Video Games,,bigdata,top    most     games 
t3_gbiyau,DISTRIBUCIONES CONTINUAS EN R,,bigdata,  continuas en r 
t3_gbi7qs,25 Best Data Science Courses Online 2020,,bigdata,   best   science courses online      
t3_gbhj74,Providing Streaming Joins as a Service at Facebook,,bigdata,  streaming joins as a service at facebook 
t3_gba781,How Big Data Is Impacting Healthcare,,bigdata,how big   is impacting healthcare 
t3_gb3aol,Forecasting COVID 19,,bigdata,forecasting      
t3_gauf01,Apache Flink: Memory Management Improvements with Apache Flink 1.10,,bigdata,apache flink memory management improvements with apache flink     
t3_gaqpr8,"Calculating speed, bearing and distance using Kafka Streams Processor API","I experiment with Kafka Streams using the public transport API in Warsaw, which results in the article below. Any comments are welcome :-)

[https://medium.com/@zorteran/calculating-speed-bearing-and-distance-using-kafka-streams-processor-api-9e95834b9e3d](https://medium.com/@zorteran/calculating-speed-bearing-and-distance-using-kafka-streams-processor-api-9e95834b9e3d)",bigdata,calculating   bearing     using kafka streams processor api i experiment with kafka streams using the public transport api in warsaw which results in the article below any comments are welcome   
t3_gawzmz,TEORÍA DE PROBABILIDAD EN R - PARTE 2,,bigdata,teoría     en r  parte   
t3_gaokl5,Corporate video for Big Data Company tips?,"Hi there, so i'm an communications intern at a big data analytics and training company. I've been tasked to make a corporate video for the company

It's my first week and i am still VERY much inexperienced witg the big data industry. (But i was still sought out for some reason)

Any recommendations on how i should structutr the video?",bigdata,corporate   for big   company tips hi there so im an communications intern at a big   analytics   training company ive been   to make a corporate   for the company  its my first week   i am still very much   witg the big     but i was still sought out for some reason  any   on how i   structutr the  
t3_gaoh89,Boost Up Pandas Dataframes,"Hi y’all!

As a 21st century data engineer or data scientist, Pandas is the most favorite framework to operate on a dataset. But there are better alternatives to Pandas data frames like Median, Dask, Ray and PySpark. Here’s quick info, usage and installation instructions to use these powerful frameworks while dealing with huge amounts of data. Check out my medium article on ‘Boost up Pandas Dataframes’

https://towardsdatascience.com/boost-up-pandas-dataframes-46944a93d33e

Would love to hear your thoughts and suggestions for alternatives to Pandas!",bigdata,boost up     hi y’all  as a   st century   engineer or   scientist   is the most favorite framework to operate on a   but there are better alternatives to     frames like     ray   pyspark here’s quick info usage   installation instructions to use these powerful frameworks while   with huge amounts of   check out my   article on ‘boost up    ’      love to hear your thoughts   suggestions for alternatives to  
t3_gahho3,Free Online Talk | Reinforcement Learning Explained: Overview and Applications,"[https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd](https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd)

Outline:

\- Introduction to reinforcement learning and its framework  
\- RL solutions: model-based methods  
\- RL solutions: model-free methods  
\- Deep reinforcement learning  
\- Real-world applications: Alpha Go, Self-driving cars, Robotics, finance, etc.",bigdata,free online talk  reinforcement learning   overview   applications   outline     to reinforcement learning   its framework    rl solutions        rl solutions          reinforcement learning      applications alpha go   cars robotics finance etc
t3_gagwlu,Saving Money on Data Engineering in the Cloud,,bigdata,saving money on   engineering in the   
t3_gab6je,How much potential does big-data backed psychographic microtargeting and psychological nudging techniques have to give the power to influence behavior?,,bigdata,how much potential       psychographic microtargeting   psychological   techniques have to give the power to influence behavior 
t3_gae0un,A closer look at the role of big data during the COVID-19 crisis,,bigdata,a closer look at the role of big     the   crisis 
t3_gadhan,Data Engineer (ETL Focus) opportunity @ Global Media Company,"A recent podcast guest (AI In Action Podcast) Is looking for a ETL Data Engineer who wants to work in a fast paced, innovative leading global media company!

The Company: A globally recognized Media Company in NYC are seeking a ETL Data Engineer to build pipelines for their digital platforms which reach millions of users daily!

The Tech: Exposure to fast paced, high impact projects, utilizing advanced Large volume, real time data sets within Cloud using Technologies like: **Snowflake, Airflow, AWS.**

What you’ll be doing: Play a crucial role in creating data systems that move data into and out of storage, transforming it to enable people and machines to do work in a streamlined way.

Reach out to [philip@alldus.com](mailto:philip@alldus.com) for more info!",bigdata,  engineer etl focus opportunity  global   company a recent   guest ai in action   is looking for a etl   engineer who wants to work in a fast   innovative   global   company  the company a globally     company in nyc are seeking a etl   engineer to   pipelines for their   platforms which reach millions of users    the tech exposure to fast   high impact projects utilizing   large volume real time   sets within   using technologies like snowflake airflow aws  what you’ll be   play a crucial role in creating   systems that move   into   out of storage transforming it to enable people   machines to   work in a   way  reach out to    for more info
t3_gad0ah,Parquet vs ORC,"Want to get community’s pulse on preference between Parquet vs ORC for analytic/SQL use-cases. Assumption that the pipeline may have both Spark and Presto as the execution engines.

[View Poll](https://www.reddit.com/poll/gad0ah)",bigdata,parquet vs orc want to get community’s pulse on preference between parquet vs orc for analyticsql usecases assumption that the pipeline may have both spark   presto as the execution engines   
t3_ga240r,The Advantages and Disadvantages Of Real-Time Big Data Analytics,,bigdata,the       of realtime big   analytics 
t3_g9pfb8,Data Science Notebooks — A Primer,"Going forward, we’re keeping an eye on three big developments in the data science notebook space, as our research suggests notebook users are hoping for improvements across three vectors: 1) set-up and management, 2) collaboration, and 3) visualizations.  

 [https://medium.com/memory-leak/data-science-notebooks-a-primer-4af256c8f5c6](https://medium.com/memory-leak/data-science-notebooks-a-primer-4af256c8f5c6)",bigdata,  science notebooks — a primer going   we’re keeping an eye on three big   in the   science notebook space as our research suggests notebook users are hoping for improvements across three vectors   setup   management   collaboration     visualizations     
t3_g9m8t9,Best Spark Certification,"Hello,

I want to prepare a Spark certification, and I doubt to go with Databricks ([https://academy.databricks.com/exam/databricks-certified-associate-developer](https://academy.databricks.com/exam/databricks-certified-associate-developer)) or Cloudera ([https://www.cloudera.com/about/training/certification/cca-spark.html](https://www.cloudera.com/about/training/certification/cca-spark.html))

&amp;#x200B;

What do you recommend?

&amp;#x200B;

Thank you very much :)",bigdata,best spark certification hello  i want to prepare a spark certification   i   to go with    or     x   b  what   you    x   b  thank you very much 
t3_g9os58,ALMACENES DE DATOS - DATA WAREHOUSE,,bigdata,almacenes        warehouse 
t3_g9nwik,Introducing our High-Performance Elasticsearch Connector for Presto,,bigdata,  our highperformance elasticsearch connector for presto 
t3_g9q9hs,The Power of Data Integration Strategy for Your Business,"&amp;#x200B;

https://preview.redd.it/9lmumt8k2lv41.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=9ce4d7d2a026fe5e816ad6e9996f39848f084919

This is my article about the power of Data Integration Strategy for businesses any size -  [https://spd.group/data-unification/the-power-of-data-integration/](https://spd.group/data-unification/the-power-of-data-integration/) . I want to point out the importance of having a DI Strategy in the first place, talk about the business initiatives that could use it, DI approaches for businesses. Also, I would like to mention the most common applications and benefits of Cloud Data Integration. What do you think about it? If you have any thoughts about it feel free to contact me. I would love to talk about it and learn more!",bigdata,the power of   integration strategy for your business x   b    this is my article about the power of   integration strategy for businesses any size     i want to point out the importance of having a   strategy in the first place talk about the business initiatives that   use it   approaches for businesses also i   like to mention the most common applications   benefits of     integration what   you think about it if you have any thoughts about it feel free to contact me i   love to talk about it   learn more
t3_g9ol5x,Top 6 Xero Analytics And Reporting Tools For SMBs,"Xero is widely recognized as the best online accounting software for small businesses - it lets you log in from anywhere, with any device, to get a real-time view of your financials. The key areas where businesses need Xero add-ons are reporting and advanced analytics. While reporting gives you great insight into your business, advanced analytics helps you to make better decisions  - the following analysis puts together a list of the best reporting and advanced analytics add-ons for Xero: [Top 6 Xero Analytics And Reporting Tools For SMBs](https://blog.panoply.io/top-6-xero-analytics-and-business-intelligence-integrations)",bigdata,top   xero analytics   reporting tools for smbs xero is     as the best online accounting software for small businesses  it lets you log in from anywhere with any   to get a realtime view of your financials the key areas where businesses   xero   are reporting     analytics while reporting gives you great insight into your business   analytics helps you to make better     the following analysis puts together a list of the best reporting     analytics   for xero  
t3_g9mx8t,How the Lack of Good Data Is Hampering the COVID-19 Response,,bigdata,how the lack of     is hampering the   response 
t3_g95yjh,Scalable Efficient Big Data Pipeline Architecture,,bigdata,scalable efficient big   pipeline architecture 
t3_g94ck7,Variables in Apache Airflow: The Guide,,bigdata,variables in apache airflow the   
t3_g96l5r,Should Big data engineers excel in Machine Learning ?,"Hello comunnity of reddit. I have  a software engineering backround and I am currentely studying Machine Learning but I feel like its not for me because to excel at it one must really enjoy the statistical background of it , I enjoy technical stuff more then the scientific side so I am thinking of switching to big data in fact i have already started an oline course. Do Big data engineers have to be good at Machine Learning and deepl learning or they should just have a genuine understaning of it?",bigdata,  big   engineers excel in machine learning  hello comunnity of   i have  a software engineering     i am currentely   machine learning but i feel like its not for me because to excel at it one must really enjoy the statistical   of it  i enjoy technical stuff more then the scientific   so i am thinking of switching to big   in fact i have     an oline course   big   engineers have to be   at machine learning     learning or they   just have a genuine   of it
t3_g95fld,"What is anomaly detection, and why you need it?",,bigdata,what is anomaly     why you   it 
t3_g8wrs8,Learn how to improve the throughput of your data pipelines with Akka Streams,[https://aleksandarskrbic.github.io/power-of-akka-streams/](https://aleksandarskrbic.github.io/power-of-akka-streams/),bigdata,learn how to improve the throughput of your   pipelines with akka streams 
t3_g8so0l,Advantages of Data-driven Marketing Over Traditional Marketing,,bigdata,  of   marketing over   marketing 
t3_g8akto,Kafka ACLs on Kubernetes over Istio mTLS,,bigdata,kafka acls on kubernetes over istio mtls 
t3_g7vkew,An Intro into the Lambda Architecture,,bigdata,an intro into the   architecture 
t3_g81tcc,Recommended Big Data Course,"Hi,
Can anyone recommend a big data course that is offered in the weekends (online) and have an active online community / class participation (group discussions, group projects, etc)",bigdata,  big   course hi can anyone   a big   course that is   in the   online   have an active online community  class participation group   group projects etc
t3_g84jyh,Request for comments,"Hello all,

I  have to design and enterprise grade big data pipeline for a very large  data sets. It's geographical location related data which will keep  coming in periodically (no fixed interval). The source could static  files or could also be a Kafka stream. Can someone please suggest what  do I use for:

1. **Scheduling:**  I have used autosys in the past for this purpose but this time I want  to use something like Oozie or airflow. Something more advance. But I am  curious what do others use.
2. **Processing:** I want to use Spark in batch &amp; stream processing modes. Are there any other options what you have experience with?
3. **ETL (in cloud):** Traditionally I have used SSIS &amp; SQL. But this time I want to use Azure data factory. Will that be a wise choice?
4. **Data lineage**: Basically I have never have kept any provision for data lineage in the past. What have you used for reliable data lineage?
5. **Data Quality**:  I have used plain old python scripts for Data quality checks in the  past. Does anyone have experience with better data quality tools?

Any other suggestions about building big Data ETL pipeline in general will be much appreciated.",bigdata,request for comments hello all  i  have to     enterprise   big   pipeline for a very large    sets its geographical location     which will keep  coming in   no   interval the source   static  files or   also be a kafka stream can someone please suggest what    i use for       i have   autosys in the past for this purpose but this time i want  to use something like oozie or airflow something more   but i am  curious what   others use   processing i want to use spark in batch  stream processing   are there any other options what you have experience with   etl in     i have   ssis  sql but this time i want to use azure   factory will that be a wise choice     lineage basically i have never have kept any provision for   lineage in the past what have you   for reliable   lineage     quality  i have   plain   python scripts for   quality checks in the  past   anyone have experience with better   quality tools  any other suggestions about   big   etl pipeline in general will be much  
t3_g85iwp,An Island Nation with Legalized Drugs," 

Hello all,

I am here with my plan for my [subreddit](https://www.reddit.com/r/TheRedditNation/) Reddit Nation

If you are interested in joining a (hopefully) newly formed island nation then this is the place to be.

I have prepared a plan that explains how we will go about doing such a task. More detailed elaborations for each component of the plan will be given at request.

Obviously the first question is where we would have our island. I have identified places of interest. One place of interest is the Nation of Belize. Belize is a small nation with a low GDP of 2 Billion and they are selling off most of their island. Islands with 50-100 acres can be acquired for around 500,000; give or take 50,000. From the people (Government Officials) I have contacted, they are perfectly okay with this plan.

Now here is a 5 step proposal of how this will come to fruition (I will expand on how we will finance this)

1. Acquire the island
2. Set up the initial infrastructure  

   1. Form of Government that I propose is a constitutional monarchy  

      1. Country will be run as a democracy (two chamber form of parliament)
      2. Noble titles such as Duke, Earl, Count, Lord, Baron etc will be sold off to help initially finance the island.
   2. Establishment of the Government Building  

      1. For now this is where the government will convene until more infrastructure is added and the island is improved.
      2. People will be made citizens at this location
   3. Establishment of Civil Services  

      1. Waste collection, Police, Social Services
   4. Establishment of a Port  

      1. This will allow supplies to enter the island.
      2. This will allow for the island to participate with the rest of the world economically(I will expand on this later).
      3. The port will function as the entrance point and exit until the runway can be constructed.
   5. Establishment of Resident Housing  

      1. This temporary housing will be until the island can be developed and more permanent buildings can be put up.
      2. Resident Housing and the Government building will be set up near the port until island development is completed
   6. Set up a massive solar powered crypto mining operation to help finance further development of the island  

      1. A partnership with big mining companies can be brokered to have this set up
3. Make the Island a desirable location  

   1. Set up the island as a luxury city  

      1. Free housing for citizens
      2. Free healthcare for citizens
      3. Free schooling and university for citizens
   2. Build resorts and legalize gambling on the island  

      1. The revenue generated from such activities will help finance further development of the island
      2. With the revenue generated from this, the island will be able to provide for the residents a luxury city
      3. The gambling industry investments will help to finance the island as well
   3. Make the island a banking haven  

      1. Set up a bank on the island
      2. No KYC laws will be enforced on the island
      3. This will attract forgien investment into the island
   4. Allow cryptocurrency companies to conduct business without oversight and for miners to set up large scale operations using solar powered energy
   5. No income tax,sales tax, capital gains tax or corporate on the island  

      1. Commercial businesses such as casinos, resorts, and banks will just pay slightly inflated property taxes
      2. This will help attract a lot more forgien investment as well
      3. Will attract companies to set up offices here and thus bring jobs to the island.
4. Expand on the island infrastructure  

   1. Build an airport. The islands have enough space to accommodate runways for planes even up to jumbo jets
   2. Build more free luxury housing for residents
   3. Establish schools and universities
   4. Establish libraries
   5. Establish Museums
   6. Establish a healthcare system
5. Enjoy the luxury haven of an Island that we have built

Now for the question of how much this island will cost and how we will get the funding

According to my calculations (you can look at them below) it will cost us $2,600,000

* The Island itself  

   * 500,000
* The Port  

   * 200,000 to build  

      * This will benefit us later when we receive forgien investments
   * 500,000 for 250 passenger ferry  

      * We will budget around 100,000 for operating costs
* Initial Buildings  

   * 1,000,000  

      * These buildings will be permanently built but occupied until the island is built and then will be sold off to recoup the investment.
      * Buildings include
* Civil Services  

   * Around 50,000 to purchase initial equipment and medical supplies
   * Another 100,000 to purchase ATVs
   * Another 100,000 to purchase off roading vehicles
* Bitcoin mining operation  

   * We will get the money for the operation by partnering with a large company
* Water Purification  

   * 50,000
* Food Supply  

   * We will purchase MRE’s and other preserved food products for the island
   * An exact count of this cannot be made until we move further into the plan
* Further development of the island  

   * This will be financed by the forgien investments in regards to banking, gambling and tourism along with money raised from property taxes

Now in regards to how we will raise that sum

* We will sell off noble titles (this is why we will set up a constitutional monarchy)  

   * While no price has been established yet, the titles for sale can include  

      * Duke, Duchess.
      * Marquess, Marchioness.
      * Earl, Countess.
      * Viscount, Viscountess.
      * Baron, Baroness.
* A small percentage of the island (less than 25%) will be earmarked for sale immediately. The part earmarked for sale will be part of the planned downtown district/capital and developers of this land will be allowed to build whatever structures they want and won't have the problem of jumping through hoops like zoning laws.  

   * On a 60 acre island, 15 acres would be marked for sale  

      * The acres on this haven of an island will be sold for 250,000 each so the island would raise 4.5 million and the sale of said acres will help kickstart the massive infrastructure of the island and provide the island with adequate funding.
* Once the island is set up, the island will continue to profit from digital activities which will allow it to not have to charge an income tax or corporate tax  

   * Digital activities include the solar powered bitcoin mining operation  

      * We will partner with an existing bitcoin company for this
   * Solar powered web hosting
* People can become citizens for a fee of $10 which will mainly go to the printing of their ID and filing in the government archives but the island will profit a few dollars off of this

If you have made it this far, please join the [subreddit](https://www.reddit.com/r/TheRedditNation/) for this plan

More information to come soon, thank you.",bigdata,an   nation with        hello all  i am here with my plan for my     nation  if you are   in joining a hopefully newly     nation then this is the place to be  i have   a plan that explains how we will go about   such a task more   elaborations for each component of the plan will be given at request  obviously the first question is where we   have our   i have   places of interest one place of interest is the nation of belize belize is a small nation with a low   of   billion   they are selling off most of their     with       acres can be   for          give or take       from the people government officials i have   they are perfectly okay with this plan  now here is a   step proposal of how this will come to fruition i will   on how we will finance this    acquire the     set up the initial infrastructure         form of government that i propose is a constitutional monarchy            country will be run as a   two chamber form of parliament         noble titles such as   earl count   baron etc will be   off to help initially finance the        establishment of the government              for now this is where the government will convene until more infrastructure is     the   is           people will be   citizens at this location      establishment of civil services            waste collection police social services      establishment of a port            this will allow supplies to enter the           this will allow for the   to participate with the rest of the   economicallyi will   on this later         the port will function as the entrance point   exit until the runway can be        establishment of   housing            this temporary housing will be until the   can be     more permanent   can be put up           housing   the government   will be set up near the port until     is        set up a massive solar   crypto mining operation to help finance further   of the              a partnership with big mining companies can be   to have this set up   make the   a   location         set up the   as a luxury city            free housing for citizens         free healthcare for citizens         free schooling   university for citizens        resorts   legalize gambling on the              the revenue   from such activities will help finance further   of the           with the revenue   from this the   will be able to   for the   a luxury city         the gambling   investments will help to finance the   as well      make the   a banking haven            set up a bank on the           no kyc laws will be   on the           this will attract forgien investment into the        allow cryptocurrency companies to   business without oversight   for miners to set up large scale operations using solar   energy      no income taxsales tax capital gains tax or corporate on the              commercial businesses such as casinos resorts   banks will just pay slightly   property taxes         this will help attract a lot more forgien investment as well         will attract companies to set up offices here   thus bring jobs to the       on the   infrastructure           an airport the   have enough space to   runways for planes even up to jumbo jets        more free luxury housing for        establish schools   universities      establish libraries      establish museums      establish a healthcare system   enjoy the luxury haven of an   that we have built  now for the question of how much this   will cost   how we will get the      to my calculations you can look at them below it will cost us           the   itself                the port               to             this will benefit us later when we receive forgien investments            for     passenger ferry           we will            for operating costs  initial                            these   will be permanently built but   until the   is built   then will be   off to recoup the investment             civil services                to purchase initial equipment     supplies     another        to purchase atvs     another        to purchase off   vehicles  bitcoin mining operation        we will get the money for the operation by partnering with a large company  water purification                 supply        we will purchase mre’s   other       for the       an exact count of this cannot be   until we move further into the plan  further   of the          this will be   by the forgien investments in   to banking gambling   tourism along with money   from property taxes  now in   to how we will raise that sum   we will sell off noble titles this is why we will set up a constitutional monarchy        while no price has been   yet the titles for sale can                        marquess marchioness        earl countess        viscount viscountess        baron baroness  a small percentage of the   less than    will be   for sale   the part   for sale will be part of the           of this   will be   to   whatever structures they want   wont have the problem of jumping through hoops like zoning laws        on a    acre      acres   be   for sale           the acres on this haven of an   will be   for        each so the     raise    million   the sale of   acres will help kickstart the massive infrastructure of the       the   with      once the   is set up the   will continue to profit from   activities which will allow it to not have to charge an income tax or corporate tax          activities   the solar   bitcoin mining operation           we will partner with an existing bitcoin company for this     solar   web hosting  people can become citizens for a fee of    which will mainly go to the printing of their     filing in the government archives but the   will profit a few   off of this  if you have   it this far please join the   for this plan  more information to come soon thank you
t3_g7weff,Data Race | Top 10 National Parks | Most Visited National Parks | From 1904 - 2019,,bigdata,  race  top    national parks  most   national parks  from            
t3_g7va8j,Looking for a Data Set with corresponding repeatable Report.,"Hello, I'm a AI  student looking for a data set for an assignment.

I need to find a academic paper/report that uses a data set as part of some development/research.

My goal is then to get the same data set and recreate what was outlined in the report.

I'm looking for a pretty simple data-set/report that uses something such as a FF-NN / Linear Regression Model  that i can the work with in R.

Has anyone got any inspiration for me?

Cheers in advance!

C",bigdata,looking for a   set with   repeatable report hello im a ai    looking for a   set for an assignment  i   to   a   paperreport that uses a   set as part of some    my goal is then to get the same   set   recreate what was   in the report  im looking for a pretty simple   that uses something such as a ffnn  linear regression    that i can the work with in r  has anyone got any inspiration for me  cheers in    c
t3_g7srcy,Sample alignment for doing survival analysis,"Hello All,

I have a question regarding sample alignment for doing survival analysis. How do we do that? Is there any R or python package for doing that? How we should align the patient based on their failure event?

Any kind of help would be appreciated in advance.

Best",bigdata,sample alignment for   survival analysis hello all  i have a question   sample alignment for   survival analysis how   we   that is there any r or python package for   that how we   align the patient   on their failure event  any   of help   be   in    best
t3_g7sh88,ETL Engineer opportunity - Leading Global Media Company - NYC,"I'm looking for an ETL Engineer who is an expert in developing pipelines who have experienced using technologies like: [**#airflow**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6659784610953723904&amp;keywords=%23airflow&amp;originTrackingId=2KhI%2BHlGSqmT44whQuY%2BYQ%3D%3D), [**#spark**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6659784610953723904&amp;keywords=%23spark&amp;originTrackingId=2KhI%2BHlGSqmT44whQuY%2BYQ%3D%3D) &amp; [**#snowflake**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6659784610953723904&amp;keywords=%23snowflake&amp;originTrackingId=2KhI%2BHlGSqmT44whQuY%2BYQ%3D%3D).  

The opportunity is with a Leading Global Media company based in NYC reaching millions of users daily, leading digital media innovation.  

Contact [**philip@alldus.com**](mailto:philip@alldus.com) for more information!",bigdata,etl engineer opportunity    global   company  nyc im looking for an etl engineer who is an expert in   pipelines who have   using technologies like           the opportunity is with a   global   company   in nyc reaching millions of users         innovation    contact    for more information
t3_g7sdya,Consider Best Machine Learning Courses to Learn,,bigdata,  best machine learning courses to learn 
t3_g7moca,Still no Palantir training pipeline or public docs after all these years.,"Despite having done it just fine with spark and some python, it would appear we will have some access and tasking with Palantir Foundry. 

I get that Peter T’s baby is shrouded in secret but ... it’s the data they need to hide not the process. 

It’s dumb that after all this time you still can’t get a formal education until the day you’re finally sitting down with it in front of you.

I’m not worried, I’m sure it’s friendly and functional. I’m just finding a complete inability to prepare for the work ahead of time idiotic.",bigdata,still no palantir training pipeline or public   after all these years   having   it just fine with spark   some python it   appear we will have some access   tasking with palantir     i get that peter t’s baby is   in secret but  it’s the   they   to   not the process   it’s   that after all this time you still can’t get a formal   until the   you’re finally sitting   with it in front of you  i’m not   i’m sure it’s     functional i’m just   a complete inability to prepare for the work   of time  
t3_g7mak3,New to big data need some clarification,"Hello everybody, litteraly I m new to big data I know juste the title and that mean handling a massive amount of data, so i want to learn this tool ans how it works, what are the principales ans the fundamentals of big data and how to use it?! 
Also if there is some books or courses that can help me ""i check some on coursera but i want something free ""
Also i heard that the apache spark is the best tool to handle this kind of data.
Uhmm I m study statistics and i want to apply iton bug data like machine learning and modeling also if There a tool to have good plots .
Thanks uu un advance.",bigdata,new to big     some clarification hello   litteraly i m new to big   i know juste the title   that mean   a massive amount of   so i want to learn this tool ans how it works what are the principales ans the   of big     how to use it  also if there is some books or courses that can help me i check some on coursera but i want something free  also i   that the apache spark is the best tool to   this   of   uhmm i m   statistics   i want to apply iton bug   like machine learning     also if there a tool to have   plots  thanks uu un  
t3_g7f9vx,Big Data and The Future of Driving,,bigdata,big     the future of   
t3_g7h3j8,A new role in data science: the data science architect,,bigdata,a new role in   science the   science architect 
t3_g72bgm,Critical insights at your fingertips: how self-service analytics can help keep your business on track,,bigdata,critical insights at your fingertips how selfservice analytics can help keep your business on track 
t3_g6mv9x,Apache flink with RocksDB: For terabytes of keyed state ?,"The book: Stream Processing with Apache Flink mentions:

&gt;. Users have reported applications with state sizes of multiple terabytes leveraging RocksDBStateBackend

How does this actually work for deployments?

* From what I understand, when we restore a job from a savepoint, all the operator state data will be shipped from the savepoint location on HDFS to each of the task managers. **If the state is in the order of terabytes, then every deployment will result in a very long downtime** if all this state needs to be transferred.
* Or is it possible, that in the the case of RocksDB, we can configure **lazy loading**, wherein keyed state is retrieved from HDFS as and when required, and then cached on the local disk.

Does anyone know which of the above scenarios is actually the case ? (long downtimes vs lazy loading). Thank you!",bigdata,apache flink with   for terabytes of   state  the book stream processing with apache flink mentions   users have   applications with state sizes of multiple terabytes leveraging    how   this actually work for     from what i   when we restore a job from a savepoint all the operator state   will be   from the savepoint location on   to each of the task managers if the state is in the   of terabytes then every   will result in a very long   if all this state   to be    or is it possible that in the the case of   we can configure lazy   wherein   state is   from   as   when     then   on the local      anyone know which of the above scenarios is actually the case  long   vs lazy   thank you
t3_g6wedp,10 Top Types of Data Analysis Methods,,bigdata,   top types of   analysis   
t3_g6l0t1,Please help: Essay about characteristics of big data platforms,"Hi, for an university application I'm supposed to write a 1000 word scientific essay about the **characteristics of big data platforms**. I can't find good papers on the general architecture of big data platforms. So far I could find.

Primary characteristics

* Scalability (both horizontal and vertical)
* I/O performance

Seconday characteristics

* Real-time processing
* Fault tolerance
* Data size

What are some other characteristics? Are there more good resources/papers on big data platforms.

Any help is appreciated",bigdata,please help essay about characteristics of big   platforms hi for an university application im   to write a        scientific essay about the characteristics of big   platforms i cant     papers on the general architecture of big   platforms so far i      primary characteristics   scalability both horizontal   vertical  io performance    characteristics   realtime processing  fault tolerance    size  what are some other characteristics are there more   resourcespapers on big   platforms  any help is  
t3_g6l6di,Technological Deflation,"**PART 1/4 - FREE MARKET?**

First off, let's set the scene.

The stock market is telling you nothing about the real economy anymore.

Economic fundamentals have never mattered as little for the stock market as has been the case during this 11-year bull market.

The correlation between gross-domestic-product growth and the direction of the S&amp;P 500 Index has only been 7% in this cycle - historically it has been 30% to 70%.

Why?

Well, it is the Central Banks, led by the Fed, who printed their way out of the Recession in '08.

In doing so, they have papered over the cracks, and we have seen the longest economic expansion is US history.

However, this is not a particularly meritocratic process: money creation itself increases inequality via the [Cantillon Effect](https://www.youtube.com/watch?v=rv5xl1AEeQs), as money printing leads to asset price inflation, which disproportionately benefits the rich and hurts the poor.

Former Federal Reserve Chairman Paul Volcker told the New York Times in 2018:

“The central issue is we’re developing into a plutocracy. We’ve got an enormous number of enormously rich people that have convinced themselves that they’re rich because they’re smart and constructive.""

The reality of course is that this is largely not the case - it is because the game is rigged in their favour.

Now, it is important to emphasise the fact that the path we have taken has resulted in the highest living standards we have seen in human history.

*However*, the issue, particularly since the US completely abandoned the gold standard in 1971, is that debt has exploded to obscene levels.

We are not operating in a free market if it takes $185 trillion of debt over the last 20 years to create 'growth'.

In fact, the global debt to GDP ratio hit an all-time high of 322% in the third quarter of 2019.

Inflation means that your dollar loses value and thus your purchasing power goes down.

Deflation means that the value of your dollar goes up and your purchasing power goes up.

That's a good thing right? You get more goods and services for less.

Well, no.

If you have deflation, debt explodes in real terms and you can never pay it back.

As the economy is based on debt, if you allow deflation, then you have to reset the debt. 

This is why central banks fear deflation so much.

However, the major force driving the human race is technological progress - and this stops for no mortal...

&amp;#x200B;

**PART 2/4 - TECHNOLOGICAL DEFLATION:**

The increased abundance created by technology will result in massive job losses.

[Throughout history](https://timeline.com/robots-have-been-about-to-take-all-the-jobs-for-more-than-200-years-5c9c08a2f41d), doom porn enthusiasts have screamed that the machines are coming for jobs. This is *not* a new phenomenon.

All technological revolutions are deflationary - since they create ""supply side shocks"", meaning that they allow for more intensive use of resources and thus higher production. With more goods being produced, all other things being equal, the price of those goods will fall. 

In the last 20 years or so, software has disrupted and replaced many established goods and services.

It is in the *next 20 years* that another disruptive technology is set to take the stage: AI

According to Steve Schwarzman, the co-founder and CEO of The Blackstone Group who has a net worth of $17.6BN, ""This is going to touch everyone's life....you're not going to be able to get away from this technology""

Moreover, this virus will only accelerate this trend towards tech. [Zoom is a fantastic example](https://uk.reuters.com/article/us-health-coronavirus-zoom/zoom-pulls-in-more-than-200-million-daily-video-users-during-worldwide-lockdowns-idUKKBN21K1C7) of exactly this.

Old legacy economic systems were not built for this tech deflation, and the thing about exponential growth is that we humans do not intuitively understand it. 

As an example, if you folded a piece of paper 50 times, of course you can only fold it seven times, but if you could fold it 50 times, it would reach the Sun!

&amp;#x200B;

**PART 3/4 - IMPLICATIONS FOR SOCIETY:**

The question is: how does this play out?

In the long term, it is the fundamental structure of the economic system that has a significant impact on people's lives, not who is President for 4 to 8 years.

In reality, politicians have limited power and are effectively all puppets. We have seen [what happens when a President doesn't stay in their lane](https://www.youtube.com/watch?v=1q91RZko5Gw)...

One could argue that the two main mechanisms of control are:

1. Divide and Conquer and
2. [Order from Chaos](https://www.youtube.com/watch?v=jfx7PnMtCeY)

As we have seen many times in the past, herd psychology is [worryingly easy to manipulate](https://www.youtube.com/watch?v=M26-B44qQIs)...

Speaking of the censorship, in his book Antifragile, Nassim Taleb discusses the anti fragility of information. 

Information feeds more on attempts to harm it than it does on efforts to promote it.

A fantastic example of this process is what has happened in the last week with London Real: they were banned on LinkedIn and David Icke's interview was censored. Now, regardless of what you think of this particular channel or your thoughts on David Icke and the theories provided, censoring information in this way *actually spreads it more virally*.

It's fascinating to observe how many views the videos regarding the bans and censorship have relative to the others. And the [impact this has had on subscribers](https://socialblade.com/youtube/user/londonrealtv).

It is always easier to blame a bigger enemy (or create a new one) rather than to admit it's a structural problem.

Therefore, you avoid short term pain...whatever the cost.

The real question is [if and when this situation will lead to social unrest](https://www.youtube.com/watch?v=U4GMUamUjT8)...

&amp;#x200B;

**PART 4/4 - INTELLECTUAL CAPITALISM:**

The depth and width of jobs impacted by AI will continue to increase in the future, now this will not necessarily happen straight away.

However, our transition from commodity capitalism to intellectual capitalism in inevitable and the people and nations who fight against this trend will be on the wrong side of history.

From a practical investment perspective, and disclaimer this is not investment advice, network effects are a crucial aspect to consider moving forwards.

Essentially, this means that the value of the network increases with each additional user - all of the tech monopolies have exhibited this property.

An asset which could in time demonstrate very strong network effects is Bitcoin.

[Looking at the market cap relative to other asset classes](https://www.visualcapitalist.com/worlds-money-markets-one-visualization-2017/), Bitcoin provides an asymmetric investment opportunity.

Only time will tell...

[https://www.youtube.com/watch?v=7nFbKzt-uwE](https://www.youtube.com/watch?v=7nFbKzt-uwE)",bigdata,technological   part     free market  first off lets set the scene  the stock market is telling you nothing about the real economy anymore  economic   have never   as little for the stock market as has been the case   this   year bull market  the correlation between   growth   the   of the sp       has only been   in this cycle  historically it has been    to     why  well it is the central banks   by the   who   their way out of the recession in     in   so they have   over the cracks   we have seen the longest economic expansion is us history  however this is not a particularly meritocratic process money creation itself increases inequality via the   as money printing   to asset price inflation which   benefits the rich   hurts the poor  former   reserve chairman paul volcker   the new york times in       “the central issue is we’re   into a plutocracy we’ve got an enormous number of enormously rich people that have   themselves that they’re rich because they’re smart   constructive  the reality of course is that this is largely not the case  it is because the game is   in their favour  now it is important to emphasise the fact that the path we have taken has   in the highest living   we have seen in human history  however the issue particularly since the us completely   the     in      is that   has   to obscene levels  we are not operating in a free market if it takes     trillion of   over the last    years to create growth  in fact the global   to   ratio hit an alltime high of     in the   quarter of       inflation means that your   loses value   thus your purchasing power goes      means that the value of your   goes up   your purchasing power goes up  thats a   thing right you get more     services for less  well no  if you have       in real terms   you can never pay it back  as the economy is   on   if you allow   then you have to reset the     this is why central banks fear   so much  however the major force   the human race is technological progress    this stops for no mortal  x   b  part     technological    the       by technology will result in massive job losses      porn enthusiasts have   that the machines are coming for jobs this is not a new phenomenon  all technological revolutions are    since they create supply   shocks meaning that they allow for more intensive use of resources   thus higher   with more   being   all other things being equal the price of those   will fall   in the last    years or so software has       many       services  it is in the next    years that another   technology is set to take the stage ai    to steve schwarzman the     ceo of the blackstone group who has a net worth of    bn this is going to touch everyones lifeyoure not going to be able to get away from this technology  moreover this virus will only accelerate this     tech   of exactly this    legacy economic systems were not built for this tech     the thing about exponential growth is that we humans   not intuitively   it   as an example if you   a piece of paper    times of course you can only   it seven times but if you     it    times it   reach the sun  x   b  part     implications for society  the question is how   this play out  in the long term it is the   structure of the economic system that has a significant impact on peoples lives not who is   for   to   years  in reality politicians have   power   are effectively all puppets we have seen    one   argue that the two main mechanisms of control are        conquer        as we have seen many times in the past   psychology is    speaking of the censorship in his book antifragile nassim taleb   the anti fragility of information   information   more on attempts to harm it than it   on efforts to promote it  a fantastic example of this process is what has   in the last week with   real they were   on       ickes interview was   now   of what you think of this particular channel or your thoughts on   icke   the theories   censoring information in this way actually   it more virally  its fascinating to observe how many views the     the bans   censorship have relative to the others   the    it is always easier to blame a bigger enemy or create a new one rather than to   its a structural problem  therefore you   short term painwhatever the cost  the real question is    x   b  part     intellectual capitalism  the       of jobs   by ai will continue to increase in the future now this will not necessarily happen straight away  however our transition from   capitalism to intellectual capitalism in inevitable   the people   nations who fight against this   will be on the wrong   of history  from a practical investment perspective     this is not investment   network effects are a crucial aspect to   moving    essentially this means that the value of the network increases with each   user  all of the tech monopolies have   this property  an asset which   in time   very strong network effects is bitcoin    bitcoin   an asymmetric investment opportunity  only time will tell  
t3_g6gwc3,Automated Data Validation Tools (for Hadoop),"Hello

I am trying to gather info on automated data validation tools (preferably open source) available to validate data in Hadoop cluster.

We are a Python and Cloudera shop and are looking to automate validation of data coming out of our applications as well as data persisted in Hive/Impala/Kudu and HDFS.

looking for pointers and experiences in your work on how this s done today.

E. G.

May be setting up a rule like this data cannot have duplicate ID s.

Some of the rules engines I looked in PyPi have latest commits years ago. Not sure if they are even maintained anymore.

Thanks for your time and inputs.",bigdata,      tools for   hello  i am trying to gather info on       tools preferably open source available to     in   cluster  we are a python     shop   are looking to automate   of   coming out of our applications as well as     in        looking for pointers   experiences in your work on how this s      e g  may be setting up a rule like this   cannot have     s  some of the rules engines i   in pypi have latest commits years ago not sure if they are even   anymore  thanks for your time   inputs
t3_g6alfo,Data capital: Mining for gold in the data decade,,bigdata,  capital mining for   in the     
t3_g60q1u,Data science in the study of history,,bigdata,  science in the   of history 
t3_g5zvkn,"Building an Industry Classifier With The Latest Scraping, NLP and Deploy...",,bigdata,  an   classifier with the latest scraping nlp     
t3_g655jf,SPMF data mining library usable in Python,"If you're using Python and want to use [SPMF](http://www.philippe-fournier-viger.com/spmf/index.php), the large Java data mining library, I just created a Python wrapper for it. [GitHub link for spmf-py](https://github.com/LoLei/spmf-py)

I didn't want to resort to using Java in my Python pipeline, so I'm using it like this now.

Just wanted to share!",bigdata,spmf   mining library usable in python if youre using python   want to use   the large java   mining library i just   a python wrapper for it    i   want to resort to using java in my python pipeline so im using it like this now  just   to share
t3_g5yzn8,Why Python is Still the Ruling Language in the AI world,,bigdata,why python is still the ruling language in the ai   
t3_g607cg,Top 5 ways how Big Data and smartphone data fight the Coronavirus,,bigdata,top   ways how big     smartphone   fight the coronavirus 
t3_g5xyyd,Bullshit at the Data Lakehouse,,bigdata,bullshit at the   lakehouse 
t3_g5dkac,I'm a small business owner looking to understand and leverage the data I have available; any resource recommendations to learn about doing so?,"I co-founded 7 small restaurants (it's a chain) and 2 years into operation have a lot of data (customer-related, staff, financial, back-of-house) and want to know better what to do with it. Does anyone have say, a book, that they would recommend to put me in the right direction?

edit: So I think since my business is expanding rapidly now is a good time to invest in data management – but I don’t know what types of data are going to be most useful to me, how to structure the data, or how to present the data to extract the most value.

I want to understand the overall data that I could be capturing and analysing, select at least one major area of that data and to develop a model for how that data could best be used to help the chain further improve their business by giving us an edge over competitors.

Examples of data areas include:

• Data about customers, such as what they order, when, where, combined with demographic information  
• Data about suppliers and supplies, for example what is used, when, where, cost, condition, supplier, mass of waste  
•Data about stores and possible store locations, about costs and issues with transport  
•Data about staff

How I see it is as the chain grows, the opportunity to leverage data increases, as does the risk of not understanding or monitoring the right data, hence why I'm at this point.

Does that make sense? :)

Thanks!",bigdata,im a small business owner looking to     leverage the   i have available any resource   to learn about   so i     small restaurants its a chain     years into operation have a lot of     staff financial backofhouse   want to know better what to   with it   anyone have say a book that they     to put me in the right      so i think since my business is     now is a   time to invest in   management – but i  ’t know what types of   are going to be most useful to me how to structure the   or how to present the   to extract the most value  i want to   the overall   that i   be capturing   analysing select at least one major area of that     to   a   for how that     best be   to help the chain further improve their business by giving us an   over competitors  examples of   areas    •   about customers such as what they   when where   with   information   •   about suppliers   supplies for example what is   when where cost   supplier mass of waste   •  about stores   possible store locations about costs   issues with transport   •  about staff  how i see it is as the chain grows the opportunity to leverage   increases as   the risk of not   or monitoring the right   hence why im at this point    that make sense   thanks
t3_g5tt1w,Big Data Certifications with Big Bucks Potential,,bigdata,big   certifications with big bucks potential 
t3_g5f0ou,How is AI impeding Covid-19?,,bigdata,how is ai     
t3_g54qbl,Apache Airflow: The ExternalTaskSensor demystified,,bigdata,apache airflow the externaltasksensor   
t3_g4rqul,An interview with Eventador CEO Kenny Gorman about the challenges of building a managed service for streaming data to simplify building real time applications,,bigdata,an interview with   ceo kenny gorman about the challenges of   a   service for streaming   to simplify   real time applications 
t3_g4ziw0,6 Examples of Using Big Data in Business,,bigdata,  examples of using big   in business 
t3_g4rbr7,Big data to fight COVID-19 and other diseases.,"Collecting total health histories of all patients of COVID-19 may provide a rapid way of determining which medicines could be effective in combating it.

Big data to fight COVID-19 and other diseases.
https://medium.com/@rgregoryclark/big-data-to-fight-covid-19-and-other-diseases-10cfd217920f#af33-568739e56954",bigdata,big   to fight     other   collecting total health histories of all patients of   may   a   way of   which     be effective in combating it  big   to fight     other   
t3_g48pi4,Python Data Engineering Tools: The Next Generation,,bigdata,python   engineering tools the next generation 
t3_g469ow,What is Streaming Big Data Analytics or Real-time Analytics and its benefits?,,bigdata,what is streaming big   analytics or realtime analytics   its benefits 
t3_g3tvia,Hive GC overhead limit exceeded,,bigdata,hive gc   limit   
t3_g3yarp,"Playing with the Rapid7 Open Datasets, Round 2",,bigdata,playing with the   open       
t3_g3nvyg,books/articles on big data from an economic/marketing point of view,"Hey everyone , so im an econ student , and i got a report to make on "" how companies use big data to influence and change consumer's behavior "" and im looking for some books or articles that talk on big data from a marketing POV . or just a books thats not too technical that can serve me as an introduction to big data , thanks",bigdata,booksarticles on big   from an economicmarketing point of view hey everyone  so im an econ      i got a report to make on  how companies use big   to influence   change consumers behavior    im looking for some books or articles that talk on big   from a marketing pov  or just a books thats not too technical that can serve me as an   to big    thanks
t3_g3pq0u,Storing a dataset.,"Hi everyone,
I wondered if Azure Blob Storage is the best/cheapest option for storing pictures belonging to a dataset? 

At the moment it contains 1200 images at 3mb each but will grow in size in the future.

What would you suggest to me otherwise?

I will be extremely grateful for any feedback
Thiemo Seys",bigdata,storing a   hi everyone i   if azure blob storage is the bestcheapest option for storing pictures belonging to a     at the moment it contains      images at  mb each but will grow in size in the future  what   you suggest to me otherwise  i will be extremely grateful for any   thiemo seys
t3_g3oayp,[OC] Coronavirus Deaths per Population in each Country (JAN-APR),,bigdata,  coronavirus   per population in each country janapr 
t3_g3n5hv,Best Machine Learning Courses to Learn,,bigdata,best machine learning courses to learn 
t3_g3d6nr,"Data ""Content"" Management System","Any open source content management systems for data? For instance, I would to be able to organise, categorise, and preview data through a browser.",bigdata,  content management system any open source content management systems for   for instance i   to be able to organise categorise   preview   through a browser
t3_g3mk2m,LAUGHING@BIGDATA - THE GREATEST DATA STORY EVER TOLD!,"A new ebook about Agile, AI, data, deep learning, IT, machine learning and more.

It's highly polemic, contrarian and insightful. It informs, educates and entertains. And there's a lot of it. You won't be left indifferent.

Here's an update on developments.

For greater convenience my Brand new ebook ***Laughing@BigData*** (Kindle Edition) is now available at the following Amazon locations:

USA (around 9.98 USD): [https://www.amazon.co.uk/dp/B086HS6VWX](https://www.amazon.co.uk/dp/B086HS6VWX)

United Kingdom (around 7.99 GBP): [https://www.amazon.co.uk/dp/B086HS6VWX](https://www.amazon.co.uk/dp/B086HS6VWX)

Germany (around 8.99 EUR): [https://www.amazon.de/dp/B086HS6VWX](https://www.amazon.de/dp/B086HS6VWX)

France (around 8.99 EUR): [https://www.amazon.fr/dp/B086HS6VWX](https://www.amazon.fr/dp/B086HS6VWX)

Spain (around 8.99 EUR): [https://www.amazon.fr/dp/B086HS6VWX](https://www.amazon.fr/dp/B086HS6VWX)

Italy (around 8.99 EUR): [https://www.amazon.it/dp/B086HS6VWX](https://www.amazon.it/dp/B086HS6VWX)

Netherlands (around 8.99 EUR): [https://www.amazon.nl/dp/B086HS6VWX](https://www.amazon.nl/dp/B086HS6VWX)

Japan (around 1,099 YEN): [https://www.amazon.co.jp/dp/B086HS6VWX](https://www.amazon.co.jp/dp/B086HS6VWX)

Brazil (around 24.99 BRL): [https://www.amazon.com.br/dp/B086HS6VWX](https://www.amazon.com.br/dp/B086HS6VWX)

Canada(around 9.99 CAD): [https://www.Amazon.ca/dp/B086HS6VWX](https://www.amazon.ca/dp/B086HS6VWX)

Mexico (around 149.99 MXN): [https://www.amazon.com.mx/dp/B086HS6VWX](https://www.amazon.com.mx/dp/B086HS6VWX)

Australia (around 1.99 AUD): [https://www.amazon.com.au/dp/B086HS6VWX](https://www.amazon.com.au/dp/B086HS6VWX)

India (around 449 INR): [https://www.amazon.in/dp/B086HS6VWX](https://www.amazon.in/dp/B086HS6VWX)

Please consider sharing these links and a recommendation with friends, connections, groups, colleagues, partners, peers, family and bosses.

Oiling the wheels-of-industry during COVID-19.

Thanks a million! Stay safe and keep well!

Martyn",bigdata,   the greatest   story ever   a new ebook about agile ai     learning it machine learning   more  its highly polemic contrarian   insightful it informs     entertains   theres a lot of it you wont be left    heres an   on    for greater convenience my   new ebook       is now available at the following amazon locations  usa                     gbp   germany       eur   france       eur   spain       eur   italy       eur           eur   japan        yen   brazil        brl             mexico         mxn   australia                   inr   please   sharing these links   a   with   connections groups colleagues partners peers family   bosses  oiling the        thanks a million stay safe   keep well  martyn
t3_g38bt5,Reminder for the precisionFDA Detecting Adverse Event Anomalies Using FDA Open Data Challenge,"There is one month left to advance techniques for the surveillance and detection of adverse events associated with FDA products in the [precisionFDA Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge.](https://go.usa.gov/xv9Jh) Final submissions are due on May 18th. A new discussion is now available on the [challenge site](https://precision.fda.gov/discussions/70-adverse-event-anomalies-challenge-guidance-tips-and-tricks-for-ensuring-a-successful-submission) with additional submission guidelines, tips, and tricks! 

If you have any questions about the challenge, please feel free to post them in this thread, and we will respond as quickly as possible. 

Thank you again for your continued support!",bigdata,  for the       event anomalies using   open   challenge there is one month left to   techniques for the surveillance     of   events   with     in the   final submissions are   on may   th a new   is now available on the   with   submission   tips   tricks   if you have any questions about the challenge please feel free to post them in this     we will   as quickly as possible   thank you again for your   support
t3_g321cy,Need advice regarding handling heavy data and training," 

Hi peoples,

I wondered if you can give some advice regarding organizing my data - and what do you think is the best thing to do.

Background: We are training local DNN where the input data can be huge - sometimes \~50MB per frame, plus much smaller label - between 50KB and 5-6MB.

Because of this huge size, we must train and work via SSD’s. (So we have multiple external (usb 3.0 based) SSD’s and 2 internal nvmes - 1st problem). 

All of our data loading process is based on folders and paths through dataframes. Which is a huge problem - as it’s not scalable at all and every change of the folders or trying to generate more data from the existing frames may fill the disks - and then we need to move the folders to other drives. - it’s bad.

We need to change the way we work - to some sort of DB that can handle big files (\~50MB) and can be spread over multiple drives and can be queried fast -&gt; e.g - geting label of type X of frame Y.

What do you suggest me to do? Even just give me the buzzwords to deeply dive into it.

I will be extremely grateful. 

Harel",bigdata,        heavy     training    hi peoples  i   if you can give some     organizing my      what   you think is the best thing to      we are training local   where the input   can be huge  sometimes   mb per frame plus much smaller label  between   kb     mb  because of this huge size we must train   work via  ’s so we have multiple external usb       ’s     internal nvmes   st problem   all of our     process is   on     paths through   which is a huge problem  as it’s not scalable at all   every change of the   or trying to generate more   from the existing frames may fill the      then we   to move the   to other    it’s    we   to change the way we work  to some sort of   that can   big files   mb   can be   over multiple     can be   fast  eg  geting label of type x of frame y  what   you suggest me to   even just give me the   to     into it  i will be extremely grateful   harel
t3_g37qe6,"Free April 30 Talk on Data for Good with Jeannette Wing, Director of Columbia University's Data Science Institute","April 30, Jeannette Wing, Director of Columbia University's Data Science Institute, presents the ACM TechTalk ""[Data for Good: Ensuring the Responsible Use of Data to Benefit Society](https://webinars.on24.com/acm/wing?partnerref=red).""

The novel capabilities we derive from data science will drive our cars, treat disease, and keep us safe. At the same time, such capabilities risk leading to biased, inappropriate, or unintended action. The design of data science solutions requires both excellence in the fundamentals of the field and expertise to develop applications which meet human challenges without creating even greater risk.

In this talk, Wing presents the mission of the Data Science Institute and highlights of its educational and research activities—all with the aim of ensuring the responsible use of data to benefit society .

[Registration](https://webinars.on24.com/acm/wing?partnerref=red) is free.",bigdata,free april    talk on   for   with jeannette wing   of columbia universitys   science institute april    jeannette wing   of columbia universitys   science institute presents the acm techtalk    the novel capabilities we   from   science will   our cars treat     keep us safe at the same time such capabilities risk   to   inappropriate or   action the   of   science solutions requires both excellence in the   of the     expertise to   applications which meet human challenges without creating even greater risk  in this talk wing presents the mission of the   science institute   highlights of its     research activities—all with the aim of ensuring the responsible use of   to benefit society     is free
t3_g36j7b,A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node. (- from appendix of Ares Le Mandat 4th ed), This is data going back to 1896 that shows how the Dow Jones performed  during times when Mars was within 30 degrees of the lunar node. The data  contains the daily percentage changes of the Dow Jones since 1896. This  information was extrapolated from sources believed to be reliable  regarding stock market data. [https://zenodo.org/record/3711110](https://zenodo.org/record/3711110),bigdata,a hypothesis that the   reserve can set interest rates   on the movements of the planet mars here i have   going back to      that shows how the   jones   when mars was within      of the lunar    from   of ares le    th    this is   going back to      that shows how the   jones      times when mars was within      of the lunar   the    contains the   percentage changes of the   jones since      this  information was   from sources   to be reliable    stock market   
t3_g2z9bg,Is It Still “Good” in 2020 to be a Big Data Scientist,"Data Scientist is undoubtedly a lucrative job role, but is it that sexy of a job as talked about? The statistics dictate that while data science jobs remain the most sought after for young graduates, the ever-surging remunerations linked to the profile have somewhat flattened.

Glassdoor and Stack Overflow, two globally renowned web portals that advertise about open data science positions globally, reported saturation in the remunerations of a data scientist due to increased number of young professionals applying for the role.

big data scientist, big data career, data analyst, data science professionals, data science career, best data science certification, data science and big data analytics

[https://www.digitaldailys.com/is-it-still-good-in-2020-to-be-a-big-data-scientist.html](https://www.digitaldailys.com/is-it-still-good-in-2020-to-be-a-big-data-scientist.html)",bigdata,is it still “ ” in      to be a big   scientist   scientist is   a lucrative job role but is it that sexy of a job as   about the statistics   that while   science jobs remain the most sought after for young   the eversurging remunerations   to the profile have somewhat        stack overflow two globally   web portals that   about open   science positions globally   saturation in the remunerations of a   scientist   to   number of young professionals applying for the role  big   scientist big   career   analyst   science professionals   science career best   science certification   science   big   analytics  
t3_g2ycwz,A quick and dirty way to monitor data arriving on Kafka,,bigdata,a quick     way to monitor   arriving on kafka 
t3_g2zrck,FACEBOOK YOUR PORTAL FOR BIG DATA,"Facebook: the largest social networking site in the whole universe with over more than 2.2 billion active users monthly. It should come as no surprise to find the platform process 2.5 billion content pieces along with 500+ terabytes each day.

Facebook has been able to pull 2.5 billion likes actions and nearly 300 million photos in just a single day, plus it scans about 105 terabytes of data each hour through Hive. Are you taken by surprise?

big data analytics, big data analytics certification, big data career, data science professionals, data science career, best data science certification, big data analyst, data science and big data analytics

[https://www.appclonescript.com/facebook-your-portal-big-data](https://www.appclonescript.com/facebook-your-portal-big-data)",bigdata,facebook your portal for big   facebook the largest social networking site in the whole universe with over more than    billion active users monthly it   come as no surprise to   the platform process    billion content pieces along with     terabytes each    facebook has been able to pull    billion likes actions   nearly     million photos in just a single   plus it scans about     terabytes of   each hour through hive are you taken by surprise  big   analytics big   analytics certification big   career   science professionals   science career best   science certification big   analyst   science   big   analytics  
t3_g2qfbx,Do you have to be good at web developpement (JEE ) to be a good big data engineer?,,bigdata,  you have to be   at web   jee  to be a   big   engineer 
t3_g2f45o,How to increase speed of machine learning model applying to a collection of texts?,,bigdata,how to increase   of machine learning   applying to a collection of texts 
t3_g2ecwm,Free Data Collection Online Hands-on Workshop | 14-May-2020,,bigdata,free   collection online   workshop    may     
t3_g223ca,"Meteorological radar can be used to indentify, track and realize long term analysis of convective thunderstorms, that are one of the possible causes of flash floods. These datasets can be exploited for applications like nowcasting, alert systems based on neural networks, and sensors deployment.",,bigdata,meteorological   can be   to   track   realize long term analysis of convective   that are one of the possible causes of flash   these   can be   for applications like nowcasting alert systems   on neural networks   sensors   
t3_g1tnmv,Ten cheatsheets for data science and machine learning,,bigdata,ten cheatsheets for   science   machine learning 
t3_g1yl5u,Helpful classes to learn Machine Learning for Finance and Trading.,,bigdata,helpful classes to learn machine learning for finance     
t3_g22hjf,"[Hiring] Senior Machine Learning Engineer (Austin, TX only)","  

A company based in Austin, TX is searching for a Senior Machine Learning Engineer to join their team. Their company is working with several government and commercial companies that is currently going through a big growth period. There are working with interesting projects including software radiation detection, creating web interfaces, to new large scale radiation detection.

\*USC only / Must be open to acquiring security clearance\*

**Required Skills &amp; Experience**

* Extensive experience with an OOP language (Python, Java, etc)
* Experience using ML frameworks/libraries (Scitkit, Pandas, Tensorflow, Spark, etc)
* Bachelor’s      Degree in computer science preferred: or equivalent combination of      education and experience

**Desired Skills &amp; Experience**

* Prior work using AWS, big data architectures is a plus

&amp;#x200B;

**The Offer**

&amp;#x200B;

* Competitive      Salary: Up to $160K/year, DOE

You will receive the following benefits:

&amp;#x200B;

* Full      benefits
* Remote      options as needed
* Work      hard/play hard team environment

**Please email me directly if interested: courtney.hughes@workbridgeassociates.com**

&amp;#x200B;

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.",bigdata,  senior machine learning engineer austin tx only     a company   in austin tx is searching for a senior machine learning engineer to join their team their company is working with several government   commercial companies that is currently going through a big growth   there are working with interesting projects   software     creating web interfaces to new large scale      usc only  must be open to acquiring security clearance    skills  experience   extensive experience with an oop language python java etc  experience using ml frameworkslibraries scitkit   tensorflow spark etc  bachelor’s        in computer science   or equivalent combination of          experience    skills  experience   prior work using aws big   architectures is a plus  x   b  the offer  x   b   competitive      salary up to    kyear    you will receive the following benefits  x   b   full      benefits  remote      options as    work          team environment  please email me   if      x   b  applicants must be currently   to work in the   states on a fulltime basis now   in the future    associates part of the motion recruitment network   it staffing solutions contract contracttohire     hire in major north american markets our unique expertise in  ’s highest   tech skill sets   with our   networks     of our local technology markets results in an exemplary track   with     clients
t3_g1vn0p,Install Apache Ranger on Cloudera Enterprise 5.13.1,"Hi! 

Is that possible to install Apache Ranger on Cloudera Enterprise 5.13.1?   
Looking around I find only confusing things.

Thanks!",bigdata,install apache ranger on   enterprise      hi   is that possible to install apache ranger on   enterprise         looking   i   only confusing things  thanks
t3_g1vtd6,An interview with Rookout's CTO about the importance of including non-technical roles in the data collection process,,bigdata,an interview with rookouts cto about the importance of   nontechnical roles in the   collection process 
t3_g1vppx,4 Data Trends to Watch in 2020," 4 [**#Data**](https://www.linkedin.com/feed/hashtag/?keywords=data&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6656231700969988096) Trends to Watch in 2020: 1) 👌 Data quality, 2)📖Data Catalogs, 3) 🔭KPI observability, and 4)🚣 Streaming 

 [https://medium.com/memory-leak/4-data-trends-to-watch-in-2020-491707902c09](https://medium.com/memory-leak/4-data-trends-to-watch-in-2020-491707902c09)",bigdata,      to watch in             to watch in        👌   quality  📖  catalogs   🔭kpi observability    🚣 streaming    
t3_g1tia4,Flink Serialization Tuning Vol. 1: Choosing your Serializer - if you can,,bigdata,flink serialization tuning vol   choosing your serializer  if you can 
t3_g1scx9,Learn Data Science from Best Courses,,bigdata,learn   science from best courses 
t3_g1exf2,Apache Druid vs. Time-Series Databases,,bigdata,apache   vs timeseries   
t3_g130k1,The Different Types of Data Analytics,,bigdata,the   types of   analytics 
t3_g1kele,Azure vs AWS: Which Cloud platform to choose for Big Data &amp; Analytics solutions?,,bigdata,azure vs aws which   platform to choose for big    analytics solutions 
t3_g1arfp,The Importance of Building a Data-Centric Culture,,bigdata,the importance of   a   culture 
t3_g181g9,Data Science is Changing the World for the Better: Here’s How,,bigdata,  science is changing the   for the better here’s how 
t3_g1009u,Python or Julia? What’s Your Best Bet for Data Science,"Both Python and Julia are considered as strong programming languages for data science professionals. While Python is a much older language and Julia the most recent one to become the preferred language for data science. 

data science professionals, Python programming language, data scientist skills, data science platforms, Julia, Data Science Industry, data science and big data analytics, Best Data science certifications    

[https://medium.com/@taylor.mark110/python-or-julia-whats-your-best-bet-for-data-science-25af6d05961b#d7c1](https://medium.com/@taylor.mark110/python-or-julia-whats-your-best-bet-for-data-science-25af6d05961b#d7c1)",bigdata,python or julia what’s your best bet for   science both python   julia are   as strong programming languages for   science professionals while python is a much   language   julia the most recent one to become the   language for   science     science professionals python programming language   scientist skills   science platforms julia   science     science   big   analytics best   science certifications      
t3_g13sk7,Most Used OS of all Time.,,bigdata,most   os of all time 
t3_g12abm,Companies that collect sensor based data ?,"
Hello ,

I’m looking for a startup that uses sensors ( cameras , motion sensors ...etc) to collect data for customers 

I want to know if there is any startups doing this and their business model please 

Thank you!",bigdata,companies that collect sensor       hello   i’m looking for a startup that uses sensors  cameras  motion sensors etc to collect   for customers   i want to know if there is any startups   this   their business   please   thank you
t3_g0gg0u,The Rise of Data Science,,bigdata,the rise of   science 
t3_g0qw83,Supercomputing the spread of the coronavirus in busy indoor spaces,,bigdata,supercomputing the   of the coronavirus in busy   spaces 
t3_g0idlj,"Kicking the Tires on Airflow, Apache’s workflow management platform – Architecture Overview, Installation and sample Azure Cloud Deployment Pipeline in Python","Part 1 - [http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-1/](http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/)

Part 2 - [http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/](http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/)",bigdata,kicking the tires on airflow apache’s workflow management platform – architecture overview installation   sample azure     pipeline in python part      part    
t3_g0i4is,Why a Big Data Analyst is a Great Career Choice,"Massive volumes of data are flowing into all organizations today. From big to small and across all industry verticals, there is a variety of data coming in – from sales to customer preferences to employee records. All this data can be a valuable source of useful insights that could guide an organization to make sensible strategic decisions on its future course of action. And that precisely is where data science comes in.

A [data analytics career](https://www.dasca.org/data-science-certifications/big-data-analyst) is a highly sought after option these days, for professionals from many different backgrounds. The work of a big data analyst is critical to organizations, and there is a shortage of suitably skilled professionals to meet the demand. The salaries and benefits packages too are very impressive.

data analytics career, big data analyst, big data analytics certification, DASCA Essential Knowledge Framework, Senior Big Data Analyst (SBDA™), data science and big data analytics

[https://www.atoallinks.com/2020/why-a-big-data-analyst-is-a-great-career-choice](https://www.atoallinks.com/2020/why-a-big-data-analyst-is-a-great-career-choice)",bigdata,why a big   analyst is a great career choice massive volumes of   are flowing into all organizations   from big to small   across all   verticals there is a variety of   coming in – from sales to customer preferences to employee   all this   can be a valuable source of useful insights that     an organization to make sensible strategic   on its future course of action   that precisely is where   science comes in  a   is a highly sought after option these   for professionals from many     the work of a big   analyst is critical to organizations   there is a shortage of suitably   professionals to meet the   the salaries   benefits packages too are very impressive    analytics career big   analyst big   analytics certification   essential   framework senior big   analyst  ™   science   big   analytics  
t3_g0lpvy,Data processing with Akka Actors: Part II,"Just finished Data processing with Akka Actors blog series, check it out. Source code available!

[https://aleksandarskrbic.github.io/akka-actors-2/](https://aleksandarskrbic.github.io/akka-actors-2/)

\#scala #akka #jvm #bigdata",bigdata,  processing with akka actors part ii just     processing with akka actors blog series check it out source   available    scala akka jvm  
t3_g0hiqx,Step-by-Step Guide to Building a Big Data Career,"Starting on a journey to build a [successful data science career](https://www.dasca.org/) mostly goes through a lot of confusion and insecurities. It is certainly intimidating to enter an industry as lucrative as data science. Questions like what all skills are necessary to be learned, and what to do about learning them haunt a young college graduate. They are unsure of what programming languages and coding skills would be mandatory to appear or get shortlisted, for a job interview in the first place. 

Big Data Career, data science career, big data engineer, data science industry, big data certification, big data professionals, data science and big data analytics

[https://www.techmughal.com/2020/04/10/step-by-step-guide-to-building-a-big-data-career](https://www.techmughal.com/2020/04/10/step-by-step-guide-to-building-a-big-data-career)",bigdata,stepbystep   to   a big   career starting on a journey to   a   mostly goes through a lot of confusion   insecurities it is certainly   to enter an   as lucrative as   science questions like what all skills are necessary to be     what to   about learning them haunt a young college   they are unsure of what programming languages     skills   be   to appear or get   for a job interview in the first place   big   career   science career big   engineer   science   big   certification big   professionals   science   big   analytics  
t3_g0hfoq,25 Best Data Science Courses 2020,,bigdata,   best   science courses      
t3_g0blbm,SQL to DE Pipeline transition,,bigdata,sql to   pipeline transition 
t3_g0bf2m,What is Power BI? Architecture of Power BI,,bigdata,what is power bi architecture of power bi 
t3_g08s78,FREE Data Visualization Virtual Summit,"Join OmniSci for a FREE Virtual Summit April 28 - 30! Register today and you will be entered to win a 64BG Apple TV 4K Register here: [http://www2.omnisci.com/l/298412/2020-04-06/87l68](http://www2.omnisci.com/l/298412/2020-04-06/87l68)

**Features Sessions:** 

**Using Altair, Ibis, and Vega for interactive exploration of OmniSci with Saul Shanabrook, Software Developer, Quansight -** Altair is a lovely tool that lets you build up complex interactive charts in Python. Ibis is also a lovely tool that lets you use a Pandas, like API to compose SQL expressions in OmniSci and other backends. By tying them together you can use the familiar syntax of Pandas, combined with the expressive power of Vega and Vega Lite, to visualize large amounts of data stored in OmniSci. This talk will walk through a number of examples of using this pipeline and then go through how it works.

**Finding Purposefully Hidden Sites with GPUs and ML with Dr. Mike Flaxman &amp; Adam Edelam, OmniSci -** OmniSci has recently partnered with the Center for Nonproliferation Studies (CNS) and Planet to demonstrate how daily satellite imagery, machine learning for feature extraction, and interactive analytics can help make the world safer. OmniSci’s GPU database technology lets the CNS combine several factors into a suitability model considering roads and their relationships to terrain. CNS leveraged an amazing new machine learning product from Planet - a monthly road change dataset at 5 meter resolution. They combined this with absolute elevation, percent slope and topographic position. Since there are less than 20 known sites, CNS elected to use a “human in the loop” process to empower analysts to assess the parameters of known sites semi-manually, and then to search for similar sites across the full country. This allowed them to discover hundreds of potential new sites, which CNS plans to further explore and then monitor. Register for the Virtual Summit to learn more.

**How to Improve the Telecommunications Network with Fast, Scalable Data -** Operating and monitoring a modern telecommunications network, especially with the ongoing rollout of 5G, requires teams of Network Performance Management Engineers to find and pinpoint anomalies, trace the root causes, and make quick decisions with enormous collections of data. Fortunately, there are new technologies that can handle billions of rows of data with millisecond filtering and visualization times, that can make that massive telecom data an asset instead of a barrier. In this webinar, we’ll take a look at three common use cases facing telecom companies today: network performance management, optimizing 5G network signal propagation, and mobile data offloading. We’ll show how an analytics platform that is accelerated with GPUs can handle the outsized data volume, performance and geospatial features inherent to these use cases, and we’ll demo the solution on the OmniSci platform.",bigdata,free   visualization virtual summit join omnisci for a free virtual summit april        register     you will be   to win a   bg apple tv  k register here   features sessions   using altair ibis   vega for interactive exploration of omnisci with saul shanabrook software   quansight  altair is a lovely tool that lets you   up complex interactive charts in python ibis is also a lovely tool that lets you use a   like api to compose sql expressions in omnisci   other   by tying them together you can use the familiar syntax of     with the expressive power of vega   vega lite to visualize large amounts of     in omnisci this talk will walk through a number of examples of using this pipeline   then go through how it works    purposefully   sites with gpus   ml with   mike flaxman      omnisci  omnisci has recently   with the center for nonproliferation   cns   planet to   how   satellite imagery machine learning for feature extraction   interactive analytics can help make the   safer omnisci’s gpu   technology lets the cns combine several factors into a suitability         their relationships to terrain cns   an amazing new machine learning   from planet  a monthly   change   at   meter resolution they   this with absolute elevation percent slope   topographic position since there are less than    known sites cns   to use a “human in the loop” process to empower analysts to assess the parameters of known sites semimanually   then to search for similar sites across the full country this   them to     of potential new sites which cns plans to further explore   then monitor register for the virtual summit to learn more  how to improve the telecommunications network with fast scalable    operating   monitoring a   telecommunications network especially with the ongoing rollout of  g requires teams of network performance management engineers to     pinpoint anomalies trace the root causes   make quick   with enormous collections of   fortunately there are new technologies that can   billions of rows of   with   filtering   visualization times that can make that massive telecom   an asset   of a barrier in this webinar we’ll take a look at three common use cases facing telecom companies   network performance management optimizing  g network signal propagation   mobile     we’ll show how an analytics platform that is   with gpus can   the     volume performance   geospatial features inherent to these use cases   we’ll   the solution on the omnisci platform
t3_fzyne8,Is Python a good starting point to get into big data career?,,bigdata,is python a   starting point to get into big   career 
t3_fzz6zc,Will general AI take over the world?,,bigdata,will general ai take over the   
t3_fznrph,"Books that you recommend,useful to stsrt learning Big data",,bigdata,books that you   to stsrt learning big   
t3_fzqq98,Would you be interested in reading a career site solely focused on helping people get a job at HPC and Big Data companies?,"If so, comment yes, and what job title and companies you’d want to work at?",bigdata,  you be   in   a career site solely   on helping people get a job at hpc   big   companies if so comment yes   what job title   companies you’  want to work at
t3_fz4oke,Best “drag and drop” tools? (FREE?),"Hey, everyone. I’m currently using RapidMiner to mine, analyse and visualise data. I’m quite satisfied with it. But I was wondering if there’s better FREE softwares/ tools than this one which don’t need any programming knowledge to operate. 

So, any suggestions? Would be great help. Thanks!",bigdata,best “     ” tools free hey everyone i’m currently using   to mine analyse   visualise   i’m quite   with it but i was   if there’s better free softwares tools than this one which  ’t   any programming   to operate   so any suggestions   be great help thanks
t3_fz6jat,World Population Growth,,bigdata,  population growth 
t3_fywbsj,Trying to start a career in big data under Microsoft stack. What should I learn? I am a t-sql developer (approx 3 yrs),,bigdata,trying to start a career in big     microsoft stack what   i learn i am a tsql   approx   yrs 
t3_fydokq,How Big Data Affects Business,,bigdata,how big   affects business 
t3_fyweb3,How Google is Teaching a Robot Dog to Learn to Move like a Real Dog,,bigdata,how google is teaching a robot   to learn to move like a real   
t3_fyiau0,Leetcode necessary for big data interviews for tech companies?,"Are algo/data structure questions (aka studying leetcode) necessary for big data interviews? Or more of the understanding of the big data side? How important is studying leetcode for big data engineer interview for a big tech company?

What are the interviews for big data engineer like for big tech companies?",bigdata,  necessary for big   interviews for tech companies are   structure questions aka     necessary for big   interviews or more of the   of the big     how important is     for big   engineer interview for a big tech company  what are the interviews for big   engineer like for big tech companies
t3_fyojsd,Job requires Apache Spark w/ Java &amp; Cassandra, I am experience w/ Java but have never used Spark or Cassandra before. Are there any online tutorials/ courses you guys recommend to learn these technologies? Much appreciated!,bigdata,job requires apache spark w java     i am experience w java but have never   spark or   before are there any online tutorials courses you guys   to learn these technologies much  
t3_fyf12r,Running Kafka in local dev,,bigdata,running kafka in local   
t3_fynubz,The Good the Bad of Big Data in the Criminal Justice System,,bigdata,the   the   of big   in the criminal justice system 
t3_fxwgqd,Tinkerer - Implementing A Fast Queryable Storage with Apache Avro and Azure Block Blobs,,bigdata,tinkerer  implementing a fast queryable storage with apache avro   azure block blobs 
t3_fxtx4j,Stateful Functions 2.0 - An Event-driven Database on Apache Flink,,bigdata,stateful functions     an     on apache flink 
t3_fxlnjm,Storing raw data in Files vs Kafka,"Hello Data Geeks, I have an architectural discussion question:  
What is the right way to store data on the remote offline server?

Conditions:

1. Server has limited connectivity
2. We consume large amounts of data 12,000 samples per second and store them as files on that server until the point when we get connection and upload them to the BlobStorage

I am thinking to switch the module which stores data as files to send them to the Kafka, so we can consume the data from outside instead of sending them to blob storage.   


Questions:

1. Can Kafka handle persistence?
2. Do you think we need to keep both? Files / Kafka buffer?
3. How reliable Kafka in terms of buffering data?",bigdata,storing raw   in files vs kafka hello   geeks i have an architectural   question   what is the right way to store   on the remote offline server       server has   connectivity   we consume large amounts of         samples per     store them as files on that server until the point when we get connection     them to the blobstorage  i am thinking to switch the   which stores   as files to   them to the kafka so we can consume the   from     of   them to blob storage      questions    can kafka   persistence     you think we   to keep both files  kafka buffer   how reliable kafka in terms of buffering  
t3_fxa1ts,Hazelcast Jet · Open-Source Distributed Stream Processing,,bigdata,hazelcast jet · opensource   stream processing 
t3_fxhd23,Looking for travel between states data,"Does anyone know of a good place to find data on travel numbers between states? I'm looking for something that could give an aggregate impression of how often people travel between given states.

Ex: New York and New Jersey share a lot of travel but New York and Montana do not.

I'm not looking for strict geographical adjacency either. ",bigdata,looking for travel between states     anyone know of a   place to     on travel numbers between states im looking for something that   give an aggregate impression of how often people travel between given states  ex new york   new jersey share a lot of travel but new york   montana   not  im not looking for strict geographical   either 
t3_fx1ven,Data processing with Akka Actors: Part I,"Here is my first blog post, where I described how to design actor based apps with a simple yet representative example. Anyone interested in Scala or Akka feel free to check it out.

[https://aleksandarskrbic.github.io/akka-actors-1/](https://aleksandarskrbic.github.io/akka-actors-1/)",bigdata,  processing with akka actors part i here is my first blog post where i   how to   actor   apps with a simple yet representative example anyone   in scala or akka feel free to check it out  
t3_fxax5r,Data Science and How Data Scientists Add Value to Business,,bigdata,  science   how   scientists   value to business 
t3_fx43im,Need help with research paper :),"Hi there,

I'm doing a research paper in the area of behavioural economics with the title 'The Relationship between Conscientiousness and Individual Level of Loss Aversion moderated by Emotional Regulation'. It consist of 3 questionnaires a) IPIP Conscientiousness Scale, b) Emotion Regulation Questionnaire and c) A lottery Choice Task. This questionnaire will take approximately 7-10 minutes. I appreciate all the help I can get as I still need alot more participants to tabulate the data. Much thanks!

[https://forms.gle/cVBrpuqrDe1uRtd87](https://forms.gle/cVBrpuqrDe1uRtd87)

Thank you! And hope you enjoy the survey and learn a thing or two from it. **Appreciate it alot!**",bigdata,  help with research paper  hi there  im   a research paper in the area of behavioural economics with the title the relationship between conscientiousness     level of loss aversion   by emotional regulation it consist of   questionnaires a ipip conscientiousness scale b emotion regulation questionnaire   c a lottery choice task this questionnaire will take approximately     minutes i appreciate all the help i can get as i still   alot more participants to tabulate the   much thanks    thank you   hope you enjoy the survey   learn a thing or two from it appreciate it alot
t3_fx5j17,Best Data Science Courses to Learn,,bigdata,best   science courses to learn 
t3_fx5f1c,New data marketplace - datacy,"*At  datacy, we are making data work for everyone. We're helping people to  earn money from their data, almost effortlessly. And allowing businesses  to source richer and more accurate data without engaging in dodgy  practices or breaching consumer privacy.*

We offer: **100% custom datasets.**  There are no pre-defined datasets on datacy. We enable buyers to pool  and aggregate data from thousands of people on demand, simply by  selecting various filters - allowing them to build  100% custom and  structured datasets, tailored to their needs. 

**Transparency &amp; ease of use**.  Buyers can transparently explore the marketplace (similar to Kaggle)  and our pricing algorithm allows them to see exactly what they'll pay  and why. They can also save any dataset to watchlist, get real time  updates on prices and buy data in any format.

&amp;#x200B;

https://preview.redd.it/47azq6vn5lr41.jpg?width=2356&amp;format=pjpg&amp;auto=webp&amp;s=0bb9fa588e248fb78f541abdf3140c4b55990917

data marketplace for data buyers

Later this month we will be opening the platform to a small number of early users.

We   believe our data should only be used in the way we want and benefit us   directly. This is your chance to be a part of that mission✊. So help  us  spread the word about datacy📢, tell your friends, and get them to  join  the platform. We cannot do this without you!

We look forward to having you onboard!

The datacy team",bigdata,new   marketplace    at    we are making   work for everyone were helping people to  earn money from their   almost effortlessly   allowing businesses  to source richer   more accurate   without engaging in    practices or breaching consumer privacy  we offer     custom    there are no     on   we enable buyers to pool    aggregate   from   of people on   simply by  selecting various filters  allowing them to        custom          to their     transparency  ease of use  buyers can transparently explore the marketplace similar to kaggle    our pricing algorithm allows them to see exactly what theyll pay    why they can also save any   to watchlist get real time    on prices   buy   in any format  x   b      marketplace for   buyers  later this month we will be opening the platform to a small number of early users  we   believe our     only be   in the way we want   benefit us     this is your chance to be a part of that mission✊ so help  us    the   about  📢 tell your     get them to  join  the platform we cannot   this without you  we look   to having you    the   team
t3_fwpqob,Best Way to Find Similarities in Datasets?,"Can someone point me in the right direction on how I can compare multiple data sets? I’ll also need to have the similarities extracted or highlighted in some way.

It would be cool if as a new data set is added, the program/tool could tell me if it finds a duplicate in a previously added/uploaded data set.

I’m familiar (enough) to work my way around Python, Excel, and SQL. If I have a technique name or some sort of reference documentation, I can take it from there.",bigdata,best way to   similarities in   can someone point me in the right   on how i can compare multiple   sets i’ll also   to have the similarities   or   in some way  it   be cool if as a new   set is   the programtool   tell me if it   a   in a previously     set  i’m familiar enough to work my way   python excel   sql if i have a technique name or some sort of reference   i can take it from there
t3_fwlb85,An interview about how Cherre builds and maintains a knowledge graph of commercial real estate data and how it enables them to answer valuable questions,,bigdata,an interview about how cherre     maintains a   graph of commercial real estate     how it enables them to answer valuable questions 
t3_fwzlsl,Difference Between Bigdata and Hadoop,,bigdata,  between       
t3_fwqtp5,Found this ebook for data folks who are switching to remote work and need some help.,"Link - [https://atlan.com/resources/ultimate-guide-remote-data-teams/](https://atlan.com/resources/ultimate-guide-remote-data-teams/)

&amp;#x200B;

[Guide for Remote Data Team](https://preview.redd.it/nk6a01mh5gr41.png?width=945&amp;format=png&amp;auto=webp&amp;s=ae1cafbfe29abdbc61e5d74c57e825d4f264217c)",bigdata,  this ebook for   folks who are switching to remote work     some help link    x   b   
t3_fwg4z6,Export RedShift System Tables And Views To S3,,bigdata,export   system tables   views to s  
t3_fwk269,Helpful beginner's blog about data integrity and how to practice it,,bigdata,helpful beginners blog about   integrity   how to practice it 
t3_fw43h8,The Economics of Artificial Intelligence,,bigdata,the economics of artificial intelligence 
t3_fvzttk,what are the best tools to learn in big data,"I'm a data analyst, I use Python pandas a lot in my job, I'm looking to learn more about big data, Amy recommended course on good tools to learn?",bigdata,what are the best tools to learn in big   im a   analyst i use python   a lot in my job im looking to learn more about big   amy   course on   tools to learn
t3_fvx6bk,Apache Flink | Advanced Flink Application Patterns: Dynamic Updates of Application Logic,,bigdata,apache flink    flink application patterns     of application logic 
t3_fw55qw,Big Data Analytics in Government: How the Government Sector Leverages Data Insights,,bigdata,big   analytics in government how the government sector leverages   insights 
t3_fw4vw9,Accelerating data-driven discoveries,,bigdata,accelerating     
t3_fvstvc,I have an intermediate knowledge of SQL. What technologies should I learn next to start a career in big data?,,bigdata,i have an     of sql what technologies   i learn next to start a career in big   
t3_fvzp8r,How are top data teams making the move to remote?,Sign up for our upcoming webinar series where data leaders from around the world share their #1 tips and learnings to managing remote teams. Learn more here: [https://atlan.com/webinar/remote-data-teams/](https://atlan.com/webinar/remote-data-teams/),bigdata,how are top   teams making the move to remote sign up for our upcoming webinar series where     from   the   share their   tips   learnings to managing remote teams learn more here 
t3_fvvc75,The Analytics Speakeasy Ep2 - Collaborative Analytics | Neebo,Learn about 'Collaborative #Analytics: The Next Wave' with David Menninger of Ventana Research on Tuesday April 7th at 10am PST / 1pm ET / 7pm CET. Join us!,bigdata,the analytics speakeasy ep   collaborative analytics  neebo learn about collaborative analytics the next wave with   menninger of ventana research on   april  th at   am pst   pm et   pm cet join us
t3_fvuhte,How Big Data And AI Are Aiding The Fight Against Pandemics - Acuvate,,bigdata,how big     ai are   the fight against    acuvate 
t3_fvu47e,Analyze RedShift user activity logs With Athena,,bigdata,analyze   user activity logs with athena 
t3_fvnq97,HDP 3.0 or CDP?,"Hi, we currently use hortonworks data platforms 2.6.5 in my organization.  We want to move our cluster to a higher version.  We are wondering whether it is better to switch to hortonworks data platform 3.0 or is it better to switch to Cloudera data platform?  Has anyone had a similar choice?  What option did you choose and why this one?",bigdata,     or   hi we currently use hortonworks   platforms     in my organization  we want to move our cluster to a higher version  we are   whether it is better to switch to hortonworks   platform    or is it better to switch to     platform  has anyone   a similar choice  what option   you choose   why this one
t3_fvrfl0,Four Big Factors Shaping the Future of Data Science,,bigdata,four big factors shaping the future of   science 
t3_fv95tv,The connection between Spark Streaming and Apache Kafka using Java,,bigdata,the connection between spark streaming   apache kafka using java 
t3_fuvwhp,The Big Data Revolution,,bigdata,the big   revolution 
t3_fuqkrt,I want to make a Customizable Big Data Visualization Music Streaming Search Engine PC app inspired on this map (QUESTIONS)," inspired on this map [http://sixdegrees.hu/last.fm/interactive\_map.html](http://sixdegrees.hu/last.fm/interactive_map.html)   


 What If the map in the link was a full screen, updated with new info from all of the artists in the world, with a new fast interface UI design and animations, With music that was meant to be played and discovered? What if it was connected to many other audio maps? A new Golden Age in Music is about to be unleashed .  


I don't care who profits form it, when you play a song the money can go to the person/music streaming website which hosts the file.

I have 116 features, fast UI interface and different visualization methods written down in detail on a mindmap.

I'm a UI Designer (with no UI skills), I don't know anything about code, but I can Be creative.

**So until I die, which skills could I learn to make a fake web app DEMO VIDEO of the potential PC app?**

**Which software should I use to make the map demo?**

**Which specific type of experts should I talk to make the demo?**",bigdata,i want to make a customizable big   visualization music streaming search engine pc app   on this map questions    on this map        what if the map in the link was a full screen   with new info from all of the artists in the   with a new fast interface ui     animations with music that was meant to be       what if it was   to many other   maps a new   age in music is about to be        i   care who profits form it when you play a song the money can go to the personmusic streaming website which hosts the file  i have     features fast ui interface     visualization   written   in   on a    im a ui   with no ui skills i   know anything about   but i can be creative  so until i   which skills   i learn to make a fake web app     of the potential pc app  which software   i use to make the map    which specific type of experts   i talk to make the  
t3_furak5,A song about big data.,,bigdata,a song about big   
t3_fu824h,10 Open Source Data Science Projects to Make you Industry Ready!,,bigdata,   open source   science projects to make you     
t3_fu7v7k,10 Open Source Data Science Projects to Make you Industry Ready!,,bigdata,   open source   science projects to make you     
t3_fu7njk,Best Data Science Courses to Learn,,bigdata,best   science courses to learn 
t3_fu98fk,Big Data Tools Organisations Can Deploy To Optimise Business Intelligence Strategy,"*The dissertation focused on analyzing the big data tools that organizations can use in optimizing business intelligence. In the analysis, the researcher limited the scope to five big data tools that included SAP BI, Power BI, Hadoop, Oracle BI, and Tableau. The researcher used an exploratory research in which primary and secondary data were analyzed. In the primary research, 35 data analysts were involved as the respondents. The researcher thus used questionnaire to collect the data from the respondents and analyzed it using descriptive statistics in form of tables and charts. In the data analysis, the researcher found that Power BI was the most popular big data tool among the respondents. The findings also showed that Power BI and SAP BI were most popular among the small-sized business organizations that have a workforce of 1-20 employees.* [Big Data Dissertation](http://www.study-aids.co.uk/ict/ict0070.html)  


[Big Data Dissertation](https://preview.redd.it/v9wao0zi3mq41.jpg?width=564&amp;format=pjpg&amp;auto=webp&amp;s=76e107a7a7ca931e66aeb93cfe82bda3098d2441)",bigdata,big   tools organisations can   to optimise business intelligence strategy the     on analyzing the big   tools that organizations can use in optimizing business intelligence in the analysis the researcher   the scope to five big   tools that   sap bi power bi   oracle bi   tableau the researcher   an exploratory research in which primary       were   in the primary research      analysts were   as the   the researcher thus   questionnaire to collect the   from the       it using   statistics in form of tables   charts in the   analysis the researcher   that power bi was the most popular big   tool among the   the   also   that power bi   sap bi were most popular among the   business organizations that have a workforce of     employees        
t3_fucax3,Dolt for Git Noobs,"Dolt is a SQL database with Git versioning and distribution semantics.

I just posted a blog called [Dolt for Git Noobs](https://www.dolthub.com/blog/2020-04-03-dolt-for-git-noobs/) that explains the basics of versioning, branching, and merging so anyone without prior Git knowledge can get started using Dolt today.  
The blog also includes a step-by-step tutorial of how to create a simple Dolt repository that uses versioning, branching, and merging to facilitate open collaboration on data.  
Also, check out [DoltHub](https://www.dolthub.com/). It hosts a number of versioned open datasets, including the most comprehensive [COVID-19](https://www.dolthub.com/repositories/Liquidata/corona-virus) dataset available.

Thanks!",bigdata,  for git noobs   is a sql   with git versioning     semantics  i just   a blog     that explains the basics of versioning branching   merging so anyone without prior git   can get   using       the blog also   a stepbystep tutorial of how to create a simple   repository that uses versioning branching   merging to facilitate open collaboration on     also check out   it hosts a number of   open     the most comprehensive     available  thanks
t3_fu1qhi,Top 10 Reasons to Learn Apache Kafka,,bigdata,top    reasons to learn apache kafka 
t3_ftog3x,4 Tips for Remote Data Teams to Improve Productivity,,bigdata,  tips for remote   teams to improve   
t3_ftrg3k,Accounting for extreme events in predictive models,"Hello community! I am researching ways to account for extreme or anomalous events in predictive models. For example, if I am predicting revenue or consumer demand, what are some ways to account for events, like natural disasters or COVID-19 in my models so that the go-forward predictions once that anomalous data starts to be included in my training data are not completely out of whack. Any suggestions?",bigdata,accounting for extreme events in     hello community i am researching ways to account for extreme or anomalous events in     for example if i am   revenue or consumer   what are some ways to account for events like natural   or   in my   so that the     once that anomalous   starts to be   in my training   are not completely out of whack any suggestions
t3_ftpawt,How to use mobile Big Data analysis for Covid-19 stayAtHome control,,bigdata,how to use mobile big   analysis for   stayathome control 
t3_ftk2ap,Learn Machine Learning from Scratch 2020,,bigdata,learn machine learning from scratch      
t3_ftjyno,"Morrisons not liable for massive staff data leak, court rules",,bigdata,morrisons not liable for massive staff   leak court rules 
t3_fthydv,[News] The 27th International Conference on Neural Information Processing (ICONIP2020)," ICONIP'20 Full Call  


\# Content  
The 27th International Conference on Neural Information Processing (ICONIP2020) aims to provide a leading international forum for researchers, scientists, and industry professionals who are working in neuroscience, neural networks, deep learning, and related fields to share their new ideas, progresses and achievements.  


ICONIP2020 will be held in Bangkok, Thailand, during November 18 - November 22, 2020 and will be collocated with several events, including ACML, iSAI-NLP, JIST-KG, and AIoT.  


\# Topics  
ICONIP2020 will deliver keynote speeches, invited talks, full paper presentations, posters, tutorials, workshops, social events, etc. Topics include but are not limited to:  
A. Theory and Algorithms  
A1. Causality and explainable AI  
A2. Computational intelligence  
A3. Control and decision theory  
A4. Constraint and uncertainty theory  
A5. Machine learning  
A6. Neurodynamics  
A7. Neural network models  
A8. Optimization  
A9. Pattern recognition  
A10. Time series analysis  
B. Computational and Cognitive Neurosciences  
B1. Affective and cognitive learning  
B2. Biometric systems/interfaces  
B3. Brain-machine interface  
B4. Computational psychiatry  
B5. Decision making and control  
B6. Neuroeconomics  
B7. Neural data analysis  
B8. Reasoning and consciousness  
B9. Sensory perception  
B10. Social cognition  
C. Human Centered Computing  
C1. Bioinformatics  
C2. Biomedical information  
C3. Healthcare  
C4. Human activity recognition  
C5. Human-centred design  
C6. Human–computer interaction  
C7. Neuromorphic hardware  
C8. Recommender systems  
C9. Social networks  
C10. Sports and rehabilitation  
D. Applications  
D1. Big data analysis  
D2. Computational finance  
D3. Image processing and computer vision  
D4. Data mining  
D5. Information security  
D6. Information retrieval  
D7. Multimedia information processing  
D8. Natural language processing  
D9. Robotics and control  
D10. Web search and mining  


\# Important Dates  
\## Conference Track  
\* Paper Submission Deadline: May 1, 2020  
\* Paper Notification Date: August 15, 2020  
\* Paper Camera-Ready Deadline: September 15, 2020  


\## Tutorials  
\* Tutorial Proposal Deadline: May 1, 2020  
\* Tutorial Proposal Notification Date: May 8, 2020  
\* Tutorial Material/Website Deadline: September 15, 2020  
\* Tutorials Date: Nov. 18, 2020  


\## Workshops/Special Sessions  
\* Workshops/Special Sessions Proposal Deadline: May 1, 2020  
\* Workshops/Special Sessions Proposal Notification Date: May 8, 2020  
\* Workshops/Special Sessions Material/Website Deadline: September 15, 2020  
\* Workshops/Special Sessions Date: Nov. 18, 2020  


\# Paper Information  
Papers should be written in English and follow the Springer LNCS format. Paper submissions are single-blind review, so author names can be shown in the submission. The submission of a paper implies that the paper is original and has not been submitted under review or is not copyright-protected elsewhere and will be presented by an author if accepted. All submitted papers will be refereed by experts in the field based on the criteria of originality, significance, quality, and clarity.  


The Proceedings will be published in the Springer’s series of Lecture Notes in Computer Science. Selected papers will be reviewed and published in some special issues of SCI journals.  


Final papers after acceptance will normally be 10 pages with a maximum of 12 pages in length. The page count includes everything, including references and appendices. Please follow Springer’s proceedings LaTeX templates provided in Overleaf (https://www.overleaf.com/latex/templates/springer-lecture-notes-in-computer-science/kzwwpvhwnvfj#.WuA4JS5uZpi).  


\# Submission site  
\* https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2Ficonip2020  


\# Conference site  
\* http://iconip2020.apnns.org/  


\# Organizing Committee  
\* Honorary Co-Chairs  
\* Jonathan Chan, King Mongkut’s University of Technology Thonburi, Thailand  
\* Irwin King, The Chinese University of Hong Kong, Hong Kong  
\* General Co-Chairs  
\* Andrew Leung, City University of Hong Kong, Hong Kong  
\* James Kwok, The Hong Kong University of Science and Technology, Hong Kong  
\* Program Co-Chairs  
\* Haiqin Yang, Ping An Life, China  
\* Kitsuchart Pasupa, King Mongkut's Institute of Technology Ladkrabang, Thailand  
\* Local Arrangements Co-Chairs  
\* Vithida Chongsuphajaisiddhi, King Mongkut University of Technology Thonburi, Thailand  
\* Finance Co-Chairs  
\* Vajirasak Vanijja, King Mongkut's University of Technology Thonburi, Thailand  
\* Seiichi Ozawa, Kobe University, Japan  
\* Special Sessions Co-Chairs  
\* Kaizhu Huang, Xi'an Jiaotong Liverpool University, China  
\* Raymond Chi-Wing Wong, The Hong Kong University of Science and Technology, Hong Kong  
\* Tutorial Co-Chairs  
\* Zenglin Xu, Harbin Institute of Technology, Shenzhen, China  
\* Jing Li, The Hong Kong Polytech University, Hong Kong  
\* Proceedings Co-Chairs  
\* Xinyi Le, Shanghai Jiao Tong University, China  
\* Jinchang Ren, University of Strathclyde, United Kingdom  
\* Publicity Co-Chairs  
\* Zeng-Guang Hou, Institute of Automation, Chinese Academy of Sciences, China  
\* Ka-Chun Wong, City University of Hong Kong, Hong Kong",bigdata,  the   th international conference on neural information processing iconip      iconip   full call      content   the   th international conference on neural information processing iconip     aims to   a   international forum for researchers scientists     professionals who are working in neuroscience neural networks   learning       to share their new   progresses   achievements     iconip     will be   in bangkok     november     november           will be   with several events   acml isainlp jistkg   aiot      topics   iconip     will   keynote speeches   talks full paper presentations posters tutorials workshops social events etc topics   but are not   to   a theory   algorithms   a  causality   explainable ai   a  computational intelligence   a  control     theory   a  constraint   uncertainty theory   a  machine learning   a      a  neural network     a  optimization   a  pattern recognition   a   time series analysis   b computational   cognitive neurosciences   b  affective   cognitive learning   b  biometric systemsinterfaces   b  brainmachine interface   b  computational psychiatry   b    making   control   b  neuroeconomics   b  neural   analysis   b  reasoning   consciousness   b  sensory perception   b   social cognition   c human   computing   c  bioinformatics   c    information   c  healthcare   c  human activity recognition   c        c  human–computer interaction   c  neuromorphic     c    systems   c  social networks   c   sports   rehabilitation     applications     big   analysis     computational finance     image processing   computer vision       mining     information security     information retrieval       information processing     natural language processing     robotics   control     web search   mining      important      conference track    paper submission   may           paper notification   august            paper     september              tutorials    tutorial proposal   may           tutorial proposal notification   may           tutorial materialwebsite   september            tutorials   nov              workshopsspecial sessions    workshopsspecial sessions proposal   may           workshopsspecial sessions proposal notification   may           workshopsspecial sessions materialwebsite   september            workshopsspecial sessions   nov              paper information   papers   be written in english   follow the springer lncs format paper submissions are   review so author names can be shown in the submission the submission of a paper implies that the paper is original   has not been     review or is not   elsewhere   will be   by an author if   all   papers will be   by experts in the     on the criteria of originality significance quality   clarity     the   will be   in the springer’s series of lecture notes in computer science   papers will be       in some special issues of sci journals     final papers after acceptance will normally be    pages with a maximum of    pages in length the page count   everything   references     please follow springer’s   latex templates   in overleaf       submission site          conference site          organizing committee    honorary cochairs    jonathan chan king mongkut’s university of technology thonburi      irwin king the chinese university of hong kong hong kong    general cochairs      leung city university of hong kong hong kong    james kwok the hong kong university of science   technology hong kong    program cochairs    haiqin yang ping an life china    kitsuchart pasupa king mongkuts institute of technology        local arrangements cochairs        king mongkut university of technology thonburi      finance cochairs    vajirasak vanijja king mongkuts university of technology thonburi      seiichi ozawa kobe university japan    special sessions cochairs    kaizhu huang xian jiaotong liverpool university china      chiwing wong the hong kong university of science   technology hong kong    tutorial cochairs    zenglin xu harbin institute of technology shenzhen china    jing li the hong kong polytech university hong kong      cochairs    xinyi le shanghai jiao tong university china    jinchang ren university of          publicity cochairs    zengguang hou institute of automation chinese   of sciences china    kachun wong city university of hong kong hong kong
t3_fteim1,Machine Learning: Relationship Between Linear Regression and Decision Making," 

Hey guys!

I’m trying to make a bot that can learn to play a game given only the controls, the score (which it is supposed to maximize) and graphics data (the rgb values in a JavaScript canvas at any given time).

At a very high level, I am thinking I should use some combination of genetic algorithm and multiple linear regression. What I’m thinking is that every so often (*x* number of milliseconds or whatever), the bot will be presented with all this data (the rgb values and the score), and based on the linear regression model, it will make some decision about what buttons to press at that time.

I know how multiple linear regression works, in that there is only one dependent variable and multiple independent variables.

My question is:  
How do I represent the decision the bot is supposed to make as a function of those variables? So far I’m thinking that the buttons it is supposed to press should also be considered as independent variables, and that the score is the dependent variable…but…what do I make of that? Because the bot is making a decision on what buttons to press based on the graphics data, so it’s almost like it should have multiple dependent variables…

Hopefully this question has demonstrated my ignorance. If you don’t understand the question, but you know anything about using machine learning for automation (or just in general), please ask for clarification. I am open to whatever insight you have as to how wrong or nonsensical my thought process is.

Thanks in advance for your response 🙂.",bigdata,machine learning relationship between linear regression     making    hey guys  i’m trying to make a bot that can learn to play a game given only the controls the score which it is   to maximize   graphics   the rgb values in a javascript canvas at any given time  at a very high level i am thinking i   use some combination of genetic algorithm   multiple linear regression what i’m thinking is that every so often x number of   or whatever the bot will be   with all this   the rgb values   the score     on the linear regression   it will make some   about what buttons to press at that time  i know how multiple linear regression works in that there is only one   variable   multiple   variables  my question is   how   i represent the   the bot is   to make as a function of those variables so far i’m thinking that the buttons it is   to press   also be   as   variables   that the score is the   variable…but…what   i make of that because the bot is making a   on what buttons to press   on the graphics   so it’s almost like it   have multiple   variables…  hopefully this question has   my ignorance if you  ’t   the question but you know anything about using machine learning for automation or just in general please ask for clarification i am open to whatever insight you have as to how wrong or nonsensical my thought process is  thanks in   for your response 🙂
t3_ft0lz4,Ververica develops a free Community Edition of its Flink-powered stream processing platform,,bigdata,ververica   a free community   of its   stream processing platform 
t3_ft7db5,Should I become a big data engineer?,Hello I was thinking of becoming a big data engineer when I leave school but I don’t know how to become one.,bigdata,  i become a big   engineer hello i was thinking of becoming a big   engineer when i leave school but i  ’t know how to become one
t3_fszmfz,HDP and CDP installation question,"Hi, i'm new to Big Data world, did Hadoop, Spark and some other tools installation, played with them they are cool indeed, and did research on job offerings to see what do employers look for emplyees in the field. I noticed Cloudera/Hortonworks experience popping up pretty often but this is where I get confused. 

I've seen the statement that both HDP and CDH are open-source, but to get the binary files downloaded from the Cloudera site I would need an enterprise account. On the Cloudera docs, however, I see the installation guides from yum repository to deploy both - hdp and cdh - completely for free. 

I bet that there are different terms of use for production and learning but there is literally no information mentioned as far as I know. Are they previous, unsupported verions? (hdp up to 3.1 and cdh up to 6.sth, and there is literally ENTERPRISE stated next to the version) I know this may be an awkward question but are these legal to use?I know there is a sandbox version but would rather install and play with the professional version.",bigdata,      installation question hi im new to big         spark   some other tools installation   with them they are cool       research on job offerings to see what   employers look for emplyees in the   i     experience popping up pretty often but this is where i get     ive seen the statement that both       are opensource but to get the binary files   from the   site i     an enterprise account on the     however i see the installation   from yum repository to   both         completely for free   i bet that there are   terms of use for     learning but there is literally no information   as far as i know are they previous   verions   up to        up to  sth   there is literally enterprise   next to the version i know this may be an   question but are these legal to usei know there is a   version but   rather install   play with the professional version
t3_ft1c1w,R vs Python – ¿Cuál es mejor?,"En nuestro día a día es muy común que los estudiantes pregunten que lenguaje es más práctico para determinadas tareas. Con campos de trabajo cada vez más competitivos, muchos han optado por aprender ambos para así maximizar sus opciones laborales, pero si estas comenzando y aun no tienes claro como funciona cada uno, entonces te invitamos a que leas este articulo, que hemos traducido especialmente para aquellos que están incursionando por primera vez en el mundo del data análisis y el Big data.

En el perfil de Juan Gabriel Gomila vas a conseguir los mejores cursos en R, como Curso completo de Machine Learning: Data Science con RStudio, pero si estas mas interesado en aprender python, traemos para ti Curso completo de Machine Learning: Data Science en Python, en ambos podras aprender todos los elementos básicos e iras adentrándote en el mundo del Machine Learning. Ademas, con el curso de Machine Learning de la A a la Z: R y Python para data science, podras profundizar aun mas y obtener los mejores resultados, con una justa medida de teoría y practica para que te conviertas en un experto. y continúes avanzando en tu camino al éxito.

**R vs Python – ¿Cuál es mejor?**

Si está leyendo este artículo, me imagino que tú, como muchos otros científicos de datos, se está preguntando qué lenguaje de programación deberías aprender. Tanto si tienes experiencia en otras herramientas de codificación como si no, las características individuales de estas dos, incluyendo los vastos conjuntos de bibliotecas y paquetes pueden parecer inicialmente desalentadoras, pero no te preocupes, ¡estamos aquí para ayudarte!

Para sorpresa de nadie, tanto R como Python presentan sus respectivas ventajas para una gran cantidad de aplicaciones y son ampliamente utilizadas por los profesionales en su comunidad global. Este artículo le ayudará a decidir cuál de las dos tiene las herramientas adecuadas para ponerse en marcha.

Para empezar, probablemente es una buena idea revisar en qué exactamente quieres usar el lenguaje de programación en términos de tu ciencia de los datos. Por ejemplo, un científico de datos que trabaje predominantemente en la investigación genética puede encontrarse entre los que utilizan R (ya que es muy utilizado en la genética y popular entre los bioinformáticos), mientras que alguien que trabaje en modelos para el análisis de imágenes, por ejemplo, un empleado de Tesla que crea tecnología de auto-conducción de coches, podría encontrarse trabajando con personas que prefieren Python, debido a sus sofisticadas herramientas de manipulación de imágenes. En última instancia, sigue siendo su elección, y aunque normalmente una buena filosofía nunca sería hacer a ciegas aquello que los demás están haciendo, es necesario tomarse el tiempo de descubrir por qué estos profesionales prefieren ciertos idiomas. Es importante ser capaz de «hablar» el mismo idioma que tus futuros compañeros.

Si aún no lo has hecho, te aconsejo que eches un vistazo a las entradas en el blog de SuperDataScience relacionadas con R y Python en el lugar de trabajo, y Aprende todos los pros y contras de la programación de Python vs R para conocer las diferencias claves entre los dos, y sus usos en el campo.

**¿Quién usa R y cuál es su objetivo?**

R fue creado inicialmente como una plataforma para la computación estadística, albergando todas las pruebas clásicas, análisis de series de tiempo, agrupación, y más. Tiene una gran comunidad de mineros de datos, lo que significa una gran cantidad de paquetes accesibles, tanto para los desarrolladores de R como para los usuarios. En cuanto a los gráficos, hay multitud de paquetes y capas para trazar y analizar gráficos, como ggplot2. Es importante destacar que R ha surgido en el nuevo estilo de escena de la inteligencia artificial proporcionando herramientas para las redes neuronales, el machine learning y la inferencia bayesiana y es compatible con paquetes para Deep Learning como MXNet y TensorFlow.  Parecería que R tiene un sólido seguimiento no sólo de los científicos de datos, sino también en gran medida de los estadísticos y campos asociados que requieren manipulación de datos (por ejemplo, los de la medicina, las finanzas y las ciencias sociales). Para nosotros, los científicos de datos, es importante encontrar un programa ampliamente utilizado; queremos ser capaces de hablar con tantas disciplinas como sea posible dentro de un idioma, haciendo que nuestros hallazgos sean fácilmente traducibles.

**¿Quién usa Python y cuál es su objetivo?**

Al otro lado de la cancha, Python es una excelente herramienta para programadores y desarrolladores en general. Ya sea desarrollando algoritmos para simular biomoléculas o entregando software anti-spam, te encontrarás en casa usando su interfaz y su conjunto de funciones. Lanzado en 1989, es citado como uno de los lenguajes de programación orientados a objetivos de uso general más importantes. Python tiene una creciente popularidad entre los nuevos programadores (entre ellos los científicos de datos), lo que por supuesto significa una rica comunidad de usuarios y solucionadores de problemas.

Del mismo modo, en el tema candente de la inteligencia artificial, Python es también la opción más popular; tiene herramientas para el Machine Learning, redes neuronales y Tensorflow. Además, abarcando algunos objetivos más generales, sus usuarios se benefician de bibliotecas como NumPy para el análisis estadístico, pandas para la preparación de datos, y seaborn para la generación de gráficos.

**R contra Python: Limitaciones**

A la parte más interesante: ¿cómo se relacionan cada uno de ellos? Descubrir las limitaciones a tiempo es posiblemente uno de los consejos más importantes. Hablando desde la experiencia, pasando de usar Matlab donde hay una enormidad de apoyo en línea (y generalmente alguna persona maravillosa que ha escrito un código exacto para tus necesidades), a labVIEW donde había poca o ninguna presencia en línea, conozco muy bien la sensación de pánico y de ser incapaz de resolver ese error y de frustrarse por no haber considerado estas posibles limitaciones.

Algunas de las principales consideraciones para una aplicación de la ciencia de los datos son:

La velocidad de procesamiento (¿utilizará grandes cantidades de datos?)

Comunidad en línea (realmente es invaluable y me ha salvado muchas veces)

Una curva de aprendizaje empinada (¿cuánto tiempo y paciencia tienes para especializarte/ya has aprendido programación antes y estás mejor equipado para aprender un nuevo idioma?)

Interfaz fácil de usar (¿estás familiarizado con la programación o prefieres algo fácil de visualizar y bonito?)

Ampliamente hablado (¿Ha considerado futuras conexiones entre los campos y sus lenguajes?)

Veamos cómo le va a cada uno en estos temas…

**Velocidad de procesamiento:**

R se considera lento. Requiere que sus objetos se almacenen en una memoria física, lo que significa que no es una gran opción cuando se trata de aprovechar los grandes datos. Dicho esto, los procesadores más rápidos están reduciendo esta limitación, y hay varios paquetes por ahí enfocados a abordar esto. Python, sin embargo, es más adecuado para grandes conjuntos de datos y su capacidad para cargar grandes archivos más rápido.

**Comunidad en línea:**

Como mencioné, tanto R como Python tienen una red de soporte ampliamente respaldada a la que puedes llegar, siendo esta una fuente de ayuda invaluable para esos bugs que parece que no puedes solucionar fácilmente.

**Una curva de aprendizaje empinada:**

Esto puede o no considerarse como una limitación de R, pero su empinada curva de aprendizaje se debe a su gran poder para los estadísticos. Siendo desarrollado por expertos en el campo, R es una herramienta increíble, pero se paga el precio de esto con su inversión inicial de tiempo. Por otro lado, Python es muy atractivo para los nuevos programadores por su facilidad de uso y su relativa accesibilidad.

Ambos programas requerirán que te familiarices con la terminología, lo cual puede parecer inicialmente desalentador y confuso (como la diferencia entre un «paquete» y una «biblioteca»), con la configuración de Python teniendo la ventaja sobre R en términos de la experiencia de uso fácil, de nuevo un vínculo con R que está siendo desarrollado por los estadísticos y basado en gran medida en su maduro predecesor, S. Aunque, Python será implacablemente estricto con los usuarios en cuanto a la sintaxis y se negará a funcionar si no se han encontrado fallos fáciles de detectar (aunque éstos mejoran la experiencia del usuario a largo plazo ya que nos hace mejores y más pulcros escritores de código). R tiene el encantador atributo, en relación con sus numerosos usuarios académicos, de proporcionar al usuario mucho más control sobre el diseño de sus gráficos, permitiendo diversas exportaciones de pantallas y formatos.

Es importante señalar que ambos se basan en el intérprete y se ha descubierto, en relación con otros lenguajes (como C++), que esto facilita mucho la detección de errores.

**Interfaz de fácil manejo:**

Rstudio es ampliamente considerada la plataforma favorita para la interfaz en R y una vez que empieces a familiarizarte con ella, entenderás por qué es así. Está clasificado como un entorno de desarrollo integrado (IDE) y comprende una consola para la ejecución directa de código con todas las funciones para trazar, soportar gráficos interactivos, depuración y gestión del espacio de trabajo, ver Características del IDE de RStudio para una guía más detallada. 

Python alberga numerosas IDEs para la elección. El beneficio de esto es que proporciona una buena oportunidad para que usted elija uno que le resulte familiar en base a sus antecedentes. Por ejemplo, viniendo de una formación en informática, Spyder es un claro favorito. Mientras que los principiantes en el campo encuentran PyCharm accesible e intuitivo.

**Ampliamente utilizado:**

Hemos tocado este tema y me gustaría subrayar que esto es subjetivo al campo elegido. Si te inclinas por los campos de la academia, las finanzas, la salud, la R sería probablemente mucho más hablado y querrás aprovecharlo. Mientras que, aquellos que estén interesados en el desarrollo de software, automatización o robótica, pueden encontrarse inmersos en la comunidad Python.

**R vs Python: Ventajas**

**R:**

Una excelente elección si quieres manipular los datos. Cuenta con más de 10.000 paquetes de datos que se encuentran en su CRAN.

Puede crear hermosos gráficos de calidad de publicación muy fácilmente; R permite a los usuarios alterar la estética de los gráficos y personalizarlos con una mínima codificación, una gran ventaja sobre sus competidores.

Tal vez su herramienta más poderosa es su modelado estadístico, creando herramientas estadísticas para los científicos de datos y siendo los precursores en este campo, preferidos por los programadores experimentados.

Los usuarios se benefician de su interfaz con la gran plataforma de Github para descubrir y compartir un mejor software.

**Python:**

Es muy fácil e intuitivo de aprender para los principiantes (a diferencia de R, Python fue desarrollado por programadores, y su facilidad de uso lo convierte en el favorito de las universidades en general).

Es atractivo para una amplia gama de usuarios, creando una comunidad cada vez mayor en más disciplinas y una mayor comunicación entre los lenguajes de código abierto.

La sintaxis estricta te obligará a convertirte en un mejor codificador, escribiendo un código más condensado y legible.

Python es más rápido en el manejo de grandes conjuntos de datos y puede cargar los archivos con facilidad, haciéndolo más apropiado para aquellos que manejan grandes datos.

Con todo esto en mente, la elección de un idioma para empezar depende en gran medida de lo que se quiere de él. Si eres el tipo de científico de datos que se especializa en análisis estadístico o trabajas en investigación, puedes encontrar que la R funciona mejor para ti. Sin embargo, si eres alguien que se ve a sí mismo ramificando a través de múltiples disciplinas, podrías hacer uso de la generalidad de Python y su diversa red. También puedes estar de acuerdo en que te beneficiaría eventualmente aprender ambas (al menos lo suficiente como para poder leer la sintaxis del otro) a medida que conozcas a cada uno por sus respectivas fortalezas. Esto sin duda te abrirá más puertas en términos de oportunidades de empleos y, lo que es más importante, te dará esa claridad para decidir qué camino profesional quieres tomar. Pero no te agobies; ¡aprender el segundo idioma será más fácil que el primero! Sin duda, también te entusiasmará abrir una nueva comunidad para sumergirte en tu crecimiento como científico de datos.",bigdata,r vs python – ¿cuál es mejor en nuestro   a   es muy común que los   pregunten que lenguaje es más práctico para   tareas con campos   trabajo   vez más competitivos muchos han   por   ambos para así maximizar sus opciones laborales pero si estas   y aun no tienes claro como funciona   uno entonces te invitamos a que leas este articulo que hemos   especialmente para aquellos que están   por primera vez en el       análisis y el big    en el perfil   juan gabriel gomila vas a conseguir los mejores cursos en r como curso completo   machine learning   science con   pero si estas mas   en   python traemos para ti curso completo   machine learning   science en python en ambos       los elementos básicos e iras   en el     machine learning   con el curso   machine learning   la a a la z r y python para   science     aun mas y obtener los mejores   con una justa     teoría y practica para que te conviertas en un experto y continúes   en tu camino al éxito  r vs python – ¿cuál es mejor  si está   este artículo me imagino que tú como muchos otros científicos     se está   qué lenguaje   programación     tanto si tienes experiencia en otras herramientas     como si no las características     estas     los vastos conjuntos   bibliotecas y paquetes   parecer inicialmente   pero no te preocupes ¡estamos aquí para    para sorpresa     tanto r como python presentan sus respectivas ventajas para una gran     aplicaciones y son ampliamente   por los profesionales en su   global este artículo le   a   cuál   las   tiene las herramientas   para ponerse en marcha  para empezar probablemente es una buena   revisar en qué exactamente quieres usar el lenguaje   programación en términos   tu ciencia   los   por ejemplo un científico     que trabaje   en la investigación genética   encontrarse entre los que utilizan r ya que es muy   en la genética y popular entre los bioinformáticos mientras que alguien que trabaje en   para el análisis   imágenes por ejemplo un     tesla que crea tecnología       coches   encontrarse   con personas que prefieren python   a sus   herramientas   manipulación   imágenes en última instancia sigue   su elección y aunque normalmente una buena filosofía nunca sería hacer a ciegas aquello que los   están   es necesario tomarse el tiempo     por qué estos profesionales prefieren ciertos   es importante ser capaz   «hablar» el mismo   que tus futuros compañeros  si aún no lo has hecho te aconsejo que eches un vistazo a las   en el blog       con r y python en el lugar   trabajo y     los pros y contras   la programación   python vs r para conocer las   claves entre los   y sus usos en el campo  ¿quién usa r y cuál es su objetivo  r fue   inicialmente como una plataforma para la computación       las pruebas clásicas análisis   series   tiempo agrupación y más tiene una gran     mineros     lo que significa una gran     paquetes accesibles tanto para los     r como para los usuarios en cuanto a los gráficos hay     paquetes y capas para trazar y analizar gráficos como ggplot  es importante   que r ha   en el nuevo estilo   escena   la inteligencia artificial   herramientas para las   neuronales el machine learning y la inferencia bayesiana y es compatible con paquetes para   learning como mxnet y tensorflow  parecería que r tiene un   seguimiento no sólo   los científicos     sino también en gran     los   y campos   que requieren manipulación     por ejemplo los   la   las finanzas y las ciencias sociales para nosotros los científicos     es importante encontrar un programa ampliamente   queremos ser capaces   hablar con tantas   como sea posible     un     que nuestros hallazgos sean fácilmente    ¿quién usa python y cuál es su objetivo  al otro     la cancha python es una excelente herramienta para   y   en general ya sea   algoritmos para simular biomoléculas o   software antispam te encontrarás en casa   su interfaz y su conjunto   funciones   en      es   como uno   los lenguajes   programación   a objetivos   uso general más importantes python tiene una creciente   entre los nuevos   entre ellos los científicos     lo que por supuesto significa una rica     usuarios y     problemas    mismo   en el tema     la inteligencia artificial python es también la opción más popular tiene herramientas para el machine learning   neuronales y tensorflow     algunos objetivos más generales sus usuarios se benefician   bibliotecas como numpy para el análisis     para la preparación     y seaborn para la generación   gráficos  r contra python limitaciones  a la parte más interesante ¿cómo se relacionan   uno   ellos   las limitaciones a tiempo es posiblemente uno   los consejos más importantes     la experiencia     usar matlab   hay una     apoyo en línea y generalmente alguna persona maravillosa que ha escrito un   exacto para tus   a labview   había poca o ninguna presencia en línea conozco muy bien la sensación   pánico y   ser incapaz   resolver ese error y   frustrarse por no haber   estas posibles limitaciones  algunas   las principales   para una aplicación   la ciencia   los   son  la     procesamiento ¿utilizará            en línea realmente es invaluable y me ha   muchas veces  una curva       ¿cuánto tiempo y paciencia tienes para especializarteya has   programación antes y estás mejor   para   un nuevo    interfaz fácil   usar ¿estás   con la programación o prefieres algo fácil   visualizar y bonito  ampliamente   ¿ha   futuras conexiones entre los campos y sus lenguajes  veamos cómo le va a   uno en estos temas…      procesamiento  r se   lento requiere que sus objetos se almacenen en una memoria física lo que significa que no es una gran opción   se trata   aprovechar los       esto los   más   están   esta limitación y hay varios paquetes por ahí   a   esto python sin embargo es más   para   conjuntos     y su   para cargar   archivos más      en línea  como mencioné tanto r como python tienen una     soporte ampliamente   a la que   llegar   esta una fuente     invaluable para esos bugs que parece que no   solucionar fácilmente  una curva        esto   o no   como una limitación   r pero su   curva     se   a su gran   para los       por expertos en el campo r es una herramienta increíble pero se paga el precio   esto con su inversión inicial   tiempo por otro   python es muy atractivo para los nuevos   por su     uso y su relativa    ambos programas requerirán que te familiarices con la terminología lo cual   parecer inicialmente   y confuso como la   entre un «paquete» y una «biblioteca» con la configuración   python   la ventaja sobre r en términos   la experiencia   uso fácil   nuevo un vínculo con r que está     por los   y   en gran   en su     s aunque python será implacablemente estricto con los usuarios en cuanto a la sintaxis y se negará a funcionar si no se han   fallos fáciles     aunque éstos mejoran la experiencia   usuario a largo plazo ya que nos hace mejores y más pulcros escritores     r tiene el   atributo en relación con sus numerosos usuarios     proporcionar al usuario mucho más control sobre el     sus gráficos     exportaciones   pantallas y formatos  es importante señalar que ambos se basan en el intérprete y se ha   en relación con otros lenguajes como c que esto facilita mucho la     errores  interfaz   fácil manejo    es ampliamente   la plataforma favorita para la interfaz en r y una vez que empieces a familiarizarte con ella   por qué es así está   como un entorno         y   una consola para la ejecución       con   las funciones para trazar soportar gráficos interactivos   y gestión   espacio   trabajo ver características         para una guía más     python alberga numerosas   para la elección el beneficio   esto es que proporciona una buena   para que   elija uno que le resulte familiar en base a sus   por ejemplo     una formación en informática   es un claro favorito mientras que los principiantes en el campo encuentran pycharm accesible e intuitivo  ampliamente    hemos   este tema y me gustaría subrayar que esto es subjetivo al campo   si te inclinas por los campos   la   las finanzas la   la r sería probablemente mucho más   y querrás aprovecharlo mientras que aquellos que estén   en el     software automatización o robótica   encontrarse inmersos en la   python  r vs python ventajas  r  una excelente elección si quieres manipular los   cuenta con más         paquetes     que se encuentran en su cran    crear hermosos gráficos       publicación muy fácilmente r permite a los usuarios alterar la estética   los gráficos y personalizarlos con una mínima   una gran ventaja sobre sus    tal vez su herramienta más   es su       herramientas   para los científicos     y   los precursores en este campo   por los      los usuarios se benefician   su interfaz con la gran plataforma   github para   y compartir un mejor software  python  es muy fácil e intuitivo     para los principiantes a     r python fue   por   y su     uso lo convierte en el favorito   las   en general  es atractivo para una amplia gama   usuarios   una     vez mayor en más   y una mayor comunicación entre los lenguajes     abierto  la sintaxis estricta te obligará a convertirte en un mejor     un   más   y legible  python es más   en el manejo     conjuntos     y   cargar los archivos con     más   para aquellos que manejan      con   esto en mente la elección   un   para empezar   en gran     lo que se quiere   él si eres el tipo   científico     que se especializa en análisis   o trabajas en investigación   encontrar que la r funciona mejor para ti sin embargo si eres alguien que se ve a sí mismo   a través   múltiples     hacer uso   la     python y su     también   estar     en que te beneficiaría eventualmente   ambas al menos lo suficiente como para   leer la sintaxis   otro a   que conozcas a   uno por sus respectivas fortalezas esto sin   te abrirá más puertas en términos       empleos y lo que es más importante te   esa   para   qué camino profesional quieres tomar pero no te agobies ¡  el     será más fácil que el primero sin   también te entusiasmará abrir una nueva   para sumergirte en tu crecimiento como científico    
t3_fss1ie,"MongoDB Security, Monitoring &amp; Backup (Mongodump)",,bigdata,  security monitoring  backup   
t3_fsy53z,Hadoop is failing and big data is still BS!,,bigdata,  is failing   big   is still bs 
t3_fsbhap,(Help For Newbie) Approx cost of BIGDATA hardware + software," Hello. 

 I am interested in the cost of BIGDATA servers, which will be able to process nearly 40TB of csv DATA.   
 I know that it is hard to say without having other technical details, but i need approx. cost of this hardware and software, because I have no experience in Bigdata at all.   


 Can you advice me servers + software and tell me approx price of it including support.   Thank you in advance.",bigdata,help for newbie approx cost of      software  hello    i am   in the cost of   servers which will be able to process nearly   tb of csv       i know that it is   to say without having other technical   but i   approx cost of this     software because i have no experience in   at all       can you   me servers  software   tell me approx price of it   support   thank you in  
t3_fscp5u,VIDEO: How AI and data science can help fight COVID-19,,bigdata,  how ai     science can help fight   
t3_fsbmfw,An interview with Tyler Colby about his experiences working as a data professional in the non-profit sector and the challenges that are unique to that domain,,bigdata,an interview with tyler colby about his experiences working as a   professional in the nonprofit sector   the challenges that are unique to that   
t3_frxm3l,Robust Apache Airflow Deployment,,bigdata,robust apache airflow   
t3_frqqcb,How to Transition into a Data Science Role,"Hello Everyone!  My name is Ken. Over the last 5 years, I transitioned from being a management consultant to a data scientist (and now director of data science at my company). When I was making this move, I felt that there were limited resources out there to help me with this specific career change. 

Over the past year, I started a YouTube channel for those who were also looking to get into data science. I talk about my experience learning the skills, building a portfolio, and I even threw in a few projects I did in my free time. 

I hope that my content is informative and can help others avoid some of the mistakes that I made on the way. If this is something that may interest you, feel free to check it out here:  [https://www.youtube.com/c/kenjee1](https://www.youtube.com/c/kenjee1) 

Thanks!

\-Ken",bigdata,how to transition into a   science role hello everyone  my name is ken over the last   years i   from being a management consultant to a   scientist   now   of   science at my company when i was making this move i felt that there were   resources out there to help me with this specific career change   over the past year i   a youtube channel for those who were also looking to get into   science i talk about my experience learning the skills   a portfolio   i even threw in a few projects i   in my free time   i hope that my content is informative   can help others   some of the mistakes that i   on the way if this is something that may interest you feel free to check it out here     thanks  ken
t3_fruh8b,Using Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration,,bigdata,using flink as   engine for     warehousing   hive integration 
t3_frxucz,Indicators of big data proficiency?,What would be *measurable indicators* that employees have a **basic understanding** of big data? What would be indicators of a **deeper expertise**?,bigdata,  of big   proficiency what   be measurable   that employees have a basic   of big   what   be   of a   expertise
t3_frxn13,Easy way to manage your Airflow setup,,bigdata,easy way to manage your airflow setup 
t3_frmcw4,How do companies handle data that is generated at 1billion records per day?,"I am very new to the big data field and very curious about how big tech companies handle data, for example, a social networking site generates 1billion records in a day -  how do they store it? How do they process it? What technologies/tools are used?  How do they see patterns for targeted marketing?",bigdata,how   companies     that is   at  billion   per   i am very new to the big       very curious about how big tech companies     for example a social networking site generates  billion   in a     how   they store it how   they process it what technologiestools are    how   they see patterns for   marketing
t3_frs6fe,What are Open Source Time Series Databases?,,bigdata,what are open source time series   
t3_frnx6b,Is it possible to execute queries to PrestoDB fusing VBA?,"And if so, how?

Thank you",bigdata,is it possible to execute queries to   fusing vba   if so how  thank you
t3_frp38u,25 Best Online Python Courses,,bigdata,   best online python courses 
t3_frmg48,Big Data Engineer In 2020: Becoming One,"Big data engineers, the professionals that work behind the scenes in every AI project, the backbone of every successful brand in the current times of intense competition in the global markets. They collect, develop, test, and maintain data all throughout the project lifecycle. Similar to what real estate contractors during the construction of a building that is taking care of the supply of essential raw material, a big data engineer makes sure that a continuous flow of filtered data gets provided to the analysts and scientists in the said domain.

big data engineer, career in data science, Skills in Data Analytics, Data Engineering, data science certificate, Senior Big Data Engineer

[http://teehog.com/big-data-engineer-in-2020-becoming-one](http://teehog.com/big-data-engineer-in-2020-becoming-one)",bigdata,big   engineer in      becoming one big   engineers the professionals that work   the scenes in every ai project the backbone of every successful   in the current times of intense competition in the global markets they collect   test   maintain   all throughout the project lifecycle similar to what real estate contractors   the construction of a   that is taking care of the supply of essential raw material a big   engineer makes sure that a continuous flow of     gets   to the analysts   scientists in the      big   engineer career in   science skills in   analytics   engineering   science certificate senior big   engineer  
t3_fr6juf,Coronavirus Could Infect Privacy And Civil Liberties Forever,,bigdata,coronavirus   infect privacy   civil liberties forever 
t3_frlfm3,Amazing Career Paths for a Big Data Professional to Pursue,"By the end of 2020, nearly 90 percent of the enterprise analytics and business professionals stated data and analytics will become the key to digital transformation.

According to IBM, the big data jobs in the U.S. will tend to increase to 2,720,000 by the end of 2020. So, if you’re thinking of getting into the big data industry, then, bang on! You’re on the right track.

Big Data Professional, Big data career, big data industry, Data engineer, Data science certification, Big data analytics

[https://www.losboquerones.com/amazing-career-paths-for-a-big-data-professional-to-pursue](https://www.losboquerones.com/amazing-career-paths-for-a-big-data-professional-to-pursue)",bigdata,amazing career paths for a big   professional to pursue by the   of      nearly    percent of the enterprise analytics   business professionals       analytics will become the key to   transformation    to ibm the big   jobs in the us will   to increase to         by the   of      so if you’re thinking of getting into the big     then bang on you’re on the right track  big   professional big   career big       engineer   science certification big   analytics  
t3_frj7h1,Advantages of Big Data | Disadvantages of Big Data,,bigdata,  of big      of big   
t3_frakil,The benefits of integrating Apache Kafka with Istio,,bigdata,the benefits of integrating apache kafka with istio 
t3_fr4d46,How countries are doing better than others for Covid-19.,,bigdata,how countries are   better than others for   
t3_fr4tsp,Best Machine Learning Courses to Learn,,bigdata,best machine learning courses to learn 
t3_fr3u0q,Data Journalist Mona Chalabi Isn’t Sure About Certainty,,bigdata,  journalist mona chalabi isn’t sure about certainty 
t3_fqmkpb,Is it interesting how to reach a high salary in your career in the shortest time possible?,"I am thinking of writing a blog / YouTube videos on the subject of maximising your IT career to learn everything required to reach a senior / high salary role in the shortest time span.

I have done this myself, where in 5 years have gone from complete noob to senior dev, on a high salary (~150K €)

I see so many people much more skilled than me which are on much lower salaries, and worse jobs with less freedom etc

I would like to share with other IT folk and people getting into this industry how to fast track your first few years to get to the top fastest.  

Some topics would be:

- Researching the market: Knowing what skills are in demand.  
- Your first 1.5 years - Type of company to learn the most. 
- Start to teach others (before you are ready) - git/blog/stackoverflow
- 1 company for 5 years vs 5 companies for 1 year each 
- Being in demand - &amp; staying up to date
- Job App Process - CV, Interviews, Negotiation 
- Permentant vs Contract work

This is not just about salary, but getting to a point where you are in demand for jobs, so you can work less, more freedom, work remotely, be your own boss etc

Would any of you be interested in any of this type of content?

Any feedback?  Or other areas to focus on?",bigdata,is it interesting how to reach a high salary in your career in the shortest time possible i am thinking of writing a blog  youtube   on the subject of maximising your it career to learn everything   to reach a senior  high salary role in the shortest time span  i have   this myself where in   years have gone from complete noob to senior   on a high salary    k €  i see so many people much more   than me which are on much lower salaries   worse jobs with less   etc  i   like to share with other it folk   people getting into this   how to fast track your first few years to get to the top fastest    some topics   be   researching the market knowing what skills are in      your first    years  type of company to learn the most   start to teach others before you are    gitblogstackoverflow    company for   years vs   companies for   year each   being in     staying up to    job app process  cv interviews negotiation   permentant vs contract work  this is not just about salary but getting to a point where you are in   for jobs so you can work less more   work remotely be your own boss etc    any of you be   in any of this type of content  any    or other areas to focus on
t3_fqmo70,Deep dive how to build a Flink Cluster on Kubernetes,"Sharing a post [https://medium.com/@diogodssantos/deploy-flink-jobs-on-kubernetes-df83bb4b5c76](https://medium.com/@diogodssantos/deploy-flink-jobs-on-kubernetes-df83bb4b5c76) to overpass some headaches when you start to use Apache Flink.

A brief introduction on the components used to orchestrate the cluster as well a simple example of an application running on the cluster.",bigdata,    how to   a flink cluster on kubernetes sharing a post  to overpass some   when you start to use apache flink  a brief   on the components   to orchestrate the cluster as well a simple example of an application running on the cluster
t3_fq2vm2,List of top tutorials to learn Big data,Sharing list of top [big data tutorials &amp; certification courses](https://blog.coursesity.com/best-big-data-certifications%20?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=redditPost&amp;utm_term=blog-bigdata) online. It will be useful for people wants to deep dive into bigdata during this free time.,bigdata,list of top tutorials to learn big   sharing list of top   online it will be useful for people wants to     into     this free time
t3_fpyfx0,NLP AI on Social Media Content RSS," 

Project Name: Christopher A

What kind of Dataset is available to train (Social media NLP problems) ?  
It should be able to determine whether the testing posts are ""depressing or encouraging""

Machine Learning Pipeline:

1. Handle unstructured data from (Social Media Post as RSS)
2. Train the ""Christopher A"" to understand where the post is speaking about (depressing or encouraging)
3. Training using (Gradient Booster OR RandomForest)
4. Test the outcome using accuracy, precision and recall.  

   1. The algorithm should be sufficiently smart enough to extract from any social media website.

It would be beneficial if someone can help me out in this project. The toolkit I am using it Anaconda Package.

The main help is the need for an algorithm to clean the dataset in STEP 1.

Any idea where should I use Supervised, Unsupervised approach for this problem.",bigdata,nlp ai on social   content rss    project name christopher a  what   of   is available to train social   nlp problems    it   be able to   whether the testing posts are   or encouraging  machine learning pipeline          from social   post as rss   train the christopher a to   where the post is speaking about   or encouraging   training using   booster or     test the outcome using accuracy precision   recall         the algorithm   be sufficiently smart enough to extract from any social   website  it   be beneficial if someone can help me out in this project the toolkit i am using it   package  the main help is the   for an algorithm to clean the   in step    any   where   i use     approach for this problem
t3_fq14f3,Are the servers behind a load balancer replicas or partitions ?,,bigdata,are the servers   a   balancer replicas or partitions  
t3_fps3ut,Small Data vs. Big Data : Back to the basics,,bigdata,small   vs big    back to the basics 
t3_fphefr,A story about the data scientists frustrations and how to build effective data teams,,bigdata,a story about the   scientists frustrations   how to   effective   teams 
t3_fppm5b,Does database replication require some routing for client to read or write a replica?,,bigdata,    replication require some routing for client to   or write a replica 
t3_fpuogj,Big Data Career in 2020: The Biggest Pros Associated,"The big data market is expected to surge to a whopping $103 billion by 2027, twice its market size in 2018. The software industry will be the biggest incorporator of data analytics by 2027, with a forecasted share of 45%.

Given the above-mentioned data, seeking a [big data career](https://www.dasca.org/data-science-certifications/careers-in-big-data) could prove to be the best decision you took in 2020. A job role in the data analytics domain will bring you wealth and respect, both at the same time. Big data soon, will control every part of our lives, ranging from what products we buy, to where we go on our holidays.

Big Data Career, big data professional, big data analyst, big data certification, big data professional, big data industry

[https://downtownmiami.com/community/technology/big-data-career-in-2020-the-biggest-pros-associated.html](https://downtownmiami.com/community/technology/big-data-career-in-2020-the-biggest-pros-associated.html)",bigdata,big   career in      the biggest pros   the big   market is   to surge to a whopping     billion by      twice its market size in      the software   will be the biggest incorporator of   analytics by      with a   share of     given the     seeking a     prove to be the best   you took in      a job role in the   analytics   will bring you wealth   respect both at the same time big   soon will control every part of our lives ranging from what   we buy to where we go on our    big   career big   professional big   analyst big   certification big   professional big      
t3_fp92ib,Getting into Big Data - Not sure which technologies are relevant,"Hey guys, I'm currently in my last bachelor semester in Computer Science, studying in Germany.

Recently I've written a seminar on *Self-Adaptive* A*rchitecture in Stream Processing*, which *spark*ed my interest in stream processing and big data.  
Now I've started doing some more work, including my bachelor thesis, in the field of stream processing and I was wondering which skills I should acquire before finishing my master's if I want to work as a **big data engineer/architect or stream processing engineer**, as I currently have no feel for what technologies are still relevant today.  


So far I have the following:

## Frameworks

* General: Kafka, Spark, Hadoop (is it deprecated?)
* Stream Processing: Spark Streaming, Kafka Streams, Flink, Apache Storm

## Programming Languages

* JAVA
* Python
* Scala (Similar to Java, more focus on functional programming)

## Container-Technologies and Frameworks

* Kubernetes
* Docker
* Openshift?

## Cloud-Technologies

* AWS
* Azure
* Google Cloud

## Other

* Distributed Computing
* Patterns for Cloud Applications/Distributed Applications",bigdata,getting into big    not sure which technologies are relevant hey guys im currently in my last bachelor semester in computer science   in germany  recently ive written a seminar on   architecture in stream processing which   my interest in stream processing   big     now ive     some more work   my bachelor thesis in the   of stream processing   i was   which skills i   acquire before finishing my masters if i want to work as a big   engineerarchitect or stream processing engineer as i currently have no feel for what technologies are still relevant       so far i have the following   frameworks   general kafka spark   is it    stream processing spark streaming kafka streams flink apache storm   programming languages   java  python  scala similar to java more focus on functional programming   containertechnologies   frameworks   kubernetes     openshift       aws  azure  google     other     computing  patterns for     applications
t3_fpehgp,25 Best Data Science Courses 2020,,bigdata,   best   science courses      
t3_fp9bsq,Deploying your streaming applications on Kubernetes is a no-brainer!,,bigdata,  your streaming applications on kubernetes is a nobrainer 
t3_fopxg1,"Data Teams Going ""Remote"" - Challenges, Learnings &amp; Observations","Folks, how are you and your data teams impacted in the current situation? Has the ""remote"" transition been easy? While my team is working hard with IT/admin to resolve their access issues + tool/tech setup, I was wondering if you had any useful tips, challenges you faced or learnings you'd like to share? Especially the impact on intangibles like collaboration, productivity/agility, comms...",bigdata,  teams going remote  challenges learnings  observations folks how are you   your   teams   in the current situation has the remote transition been easy while my team is working   with   to resolve their access issues  tooltech setup i was   if you   any useful tips challenges you   or learnings   like to share especially the impact on intangibles like collaboration   comms
t3_fonl1z,Data Veracity: a New Key to Big Data,,bigdata,  veracity a new key to big   
t3_fooz1k,White House Announces New Partnership to Unleash Supercomputing Resources to Fight COVID-19,,bigdata,white house announces new partnership to unleash supercomputing resources to fight   
t3_foimvi,Looking for some open-source datasets related to Covid-19,"Looking for datasets which show 

•	# of COVID19 by hospital location. Nightly feed

•	# of respirators per hospital location. Weekly feed",bigdata,looking for some opensource     to   looking for   which show   •	 of   by hospital location nightly    •	 of respirators per hospital location weekly  
t3_fo3huc,Big data or data engineering for NLP?,,bigdata,big   or   engineering for nlp 
t3_fnz3h0,Reactive Interface for MapR Database,,bigdata,reactive interface for mapr   
t3_fnmh0g,Integrate GitHub Actions with Apache Kafka via Lenses.io and service-account tokens,,bigdata,integrate github actions with apache kafka via lensesio   serviceaccount tokens 
t3_fnvjv4,"Where shall I install HDFS filesystem: on my existing hard drive, partition, filesystem or ...?","In *Hadoop The Definitive Guide*, when installing Hadoop to run in pseudodistributed mode:

&gt; **Formatting the HDFS filesystem**
&gt; 
&gt; Before HDFS can be used for the first time, **the filesystem** must be
&gt; formatted. This is done by running the following command:
&gt; 
&gt;     % hdfs namenode -format

I am trying  the above on my local Ubuntu laptop. But I was worrying running the above command will empty all the data on my existing ext4 filesystems.

What is ""the filesystem""?

Which part of my hard drive, partition, or file system does the above command apply to? 

Shall I create a new partition to install a HDFS filesytem on? Or can I use a file or directory on my existing ext4 filesystem to install a HDFS filesystem on?

Thanks.",bigdata,where shall i install   filesystem on my existing     partition filesystem or  in   the     when installing   to run in       formatting the   filesystem    before   can be   for the first time the filesystem must be    this is   by running the following               format  i am trying  the above on my local ubuntu laptop but i was worrying running the above   will empty all the   on my existing ext  filesystems  what is the filesystem  which part of my     partition or file system   the above   apply to   shall i create a new partition to install a   filesytem on or can i use a file or   on my existing ext  filesystem to install a   filesystem on  thanks
t3_fnrsvr,"Is blockchain useful for data engineering, big data and data science?",,bigdata,is blockchain useful for   engineering big       science 
t3_fni66j,How can we improve technology using big data,"In these quarantine days i am always thinking, how can we improve technology or make a new one using big data or collecting data from user. For example, they made a ai camera using google photos.",bigdata,how can we improve technology using big   in these quarantine   i am always thinking how can we improve technology or make a new one using big   or collecting   from user for example they   a ai camera using google photos
t3_fnnkj9,Building a Data warehouse with Hive at Helpshift — Part 1,,bigdata,  a   warehouse with hive at helpshift — part   
t3_fnkoaz,An interview with the project lead for Linode's recently released object storage service about the challenges involved in building a provider grade S3 compatible service.,,bigdata,an interview with the project   for   recently   object storage service about the challenges   in   a     s  compatible service 
t3_fnjg95,Building Apache Spark with Play Framework,,bigdata,  apache spark with play framework 
t3_fne826,Cloud Computing vs Big Data,,bigdata,  computing vs big   
t3_fn4ca4,How to boost big data performance by organizing Columnar data wisely,"Columnar file formats have become the primary storage choice for big data systems, but when I Googled related topics this weekend, I just found that most articles were talking about the simple query benchmark and storage footprint comparisons between a particular columnar format vs. row formats. Sorting is also a critical feature of columnar formats, but its benefit and effective practice have not been emphasized or explained in detail so far. IMHO, using columnar formats without proper sorting is like to take only half of the advantage of the underlying file format. I’d like to share my insights about this topic.

&amp;#x200B;

[https://medium.com/@eric.sun\_39815/are-we-taking-only-half-of-the-advantage-of-columnar-file-format-f1bae4927532](https://medium.com/@eric.sun_39815/are-we-taking-only-half-of-the-advantage-of-columnar-file-format-f1bae4927532)",bigdata,how to boost big   performance by organizing columnar   wisely columnar file formats have become the primary storage choice for big   systems but when i     topics this   i just   that most articles were talking about the simple query benchmark   storage footprint comparisons between a particular columnar format vs row formats sorting is also a critical feature of columnar formats but its benefit   effective practice have not been   or   in   so far imho using columnar formats without proper sorting is like to take only half of the   of the   file format i’  like to share my insights about this topic  x   b  
t3_fnfadv,"Netflix, Walmart, Uber and More: Big wins with Big Data","China contained coronavirus with the help of Big Data. Big Data and business analytics ensures big wins for organizations like Netflix, Walmart &amp; Uber.

China Coronavius, Big Data, Big Data and Business Analytics, Big Data Professionals, Career in Big Bata, Big Data Analytics, Analytics Remodeling, Chengdu Location

[https://www.dasca.org/world-of-big-data/article/netflix-walmart-uber-and-more-big-wins-with-big-data](https://www.dasca.org/world-of-big-data/article/netflix-walmart-uber-and-more-big-wins-with-big-data)",bigdata,netflix walmart uber   more big wins with big   china   coronavirus with the help of big   big     business analytics ensures big wins for organizations like netflix walmart  uber  china coronavius big   big     business analytics big   professionals career in big bata big   analytics analytics     location  
t3_fmy5vf,Differences between cluster managers for HPT and for big data?,,bigdata,  between cluster managers for hpt   for big   
t3_fmt6pj,Cloudera quickstart VMs missing?,"Hi All

I was trying to get cloudera quickstart VM but it seems they are not available anymore. Would anyone have the  VM stored anywhere and can share? I don't want the HDP sandbox vm. TIA",bigdata,  quickstart vms missing hi all  i was trying to get   quickstart vm but it seems they are not available anymore   anyone have the  vm   anywhere   can share i   want the     vm tia
t3_fmhubm,Quarantined in Spain,"Hi everyone! The coronavirus situation over here is pretty wild, so I'll  be forced to postpone the my Big Data masters from end of April to  October. I would hate to spend the next six months doing absolutely  nothing, but I'm also breaking my head trying to find something related  to data science (education aside), and not just any random job. For  context, I have a BS in Economics, so studied a lot of econometrics but  zero programming. I'd appreciate your advice if you have any idea, and  hope you're safe wherever you are!

Edit: Education aside because I already found resources I'm excited about in that field :)",bigdata,  in spain hi everyone the coronavirus situation over here is pretty   so ill  be   to postpone the my big   masters from   of april to  october i   hate to   the next six months   absolutely  nothing but im also breaking my   trying to   something    to   science       not just any   job for  context i have a bs in economics so   a lot of econometrics but  zero programming   appreciate your   if you have any      hope youre safe wherever you are        because i     resources im   about in that   
t3_fmgu3o,Resources or Courses to recommend a 40 Yr old who never programmed before,"I have been working in Scala / Big Data for the last 5 years and now my brother who never programmed before has been laid off in his job. 

He is very smart but never did anything in CS before. As I have a preference to Scala / Big Data I thought about recommending the same for him to start out.

What do you think about this?   Are there any very beginner tutorials or courses for Scala (like with java)?    Are big data concepts OK to grasp for a total beginner?

I myself did not do CS in college so believe all of this can be self taught. I understand it can be difficult but I want to recommend a hard but fast results type path.   What can you think of or recommend?",bigdata,resources or courses to   a    yr   who never   before i have been working in scala  big   for the last   years   now my brother who never   before has been   off in his job   he is very smart but never   anything in cs before as i have a preference to scala  big   i thought about   the same for him to start out  what   you think about this   are there any very beginner tutorials or courses for scala like with java    are big   concepts ok to grasp for a total beginner  i myself   not   cs in college so believe all of this can be self taught i   it can be   but i want to   a   but fast results type path   what can you think of or  
t3_fmdrzy,Are cluster managers on AWS clusters to be installed by users or have they already been installed by AWS?,"I am reading Spark online documentations. It says how to use cluster managers (Standalone, YARN, Mesos, Kubernetes). 

Then I am trying to understand a bit about how AWS provides clusters to run Spark applications.
In particular, whether  cluster managers on their clusters are installed by users or by AWS, and if by AWS, what cluster managers.

- Spark online documentation mentions `spark-ec2` for using Spark on AWS EC2 clusters.  Do users have to install cluster managers by themselves? Or have  clusters  on EC2 already provided/installed cluster managers? Are the cluster managers  EC2's own proprietary ones, or some third party cluster managers such as Meso, Kubernetes?

- If I am correct EMR also provides a cluster to users. Has EMR already installed cluster manager  on its clusters, or do users have to install a cluster manager on their own?

Thanks.",bigdata,are cluster managers on aws clusters to be   by users or have they   been   by aws i am   spark online   it says how to use cluster managers   yarn mesos kubernetes   then i am trying to   a bit about how aws   clusters to run spark applications in particular whether  cluster managers on their clusters are   by users or by aws   if by aws what cluster managers   spark online   mentions sparkec  for using spark on aws ec  clusters    users have to install cluster managers by themselves or have  clusters  on ec      cluster managers are the cluster managers  ec s own proprietary ones or some   party cluster managers such as meso kubernetes   if i am correct emr also   a cluster to users has emr     cluster manager  on its clusters or   users have to install a cluster manager on their own  thanks
t3_fmd1qm,Context aware recommender systems,,bigdata,context aware   systems 
t3_fm8lwa,Coursera UC Davis Computational Social Science Specialization,"&amp;#x200B;

https://reddit.com/link/fm8lwa/video/1frx9eev3yn41/player

Hello everyone!

I took this course I think this group would be interested in, called Computational Social Science on Coursera. The course is offered by the University of California and features professors from all 10 UC campuses. The specialization consists of five courses: ""[Computational Social Science Methods](https://www.coursera.org/learn/computational-social-science-methods?specialization=computational-social-science-ucdavis)"", ""[Big Data + A.I. + Ethics](https://www.coursera.org/learn/big-data-ai-ethics?specialization=computational-social-science-ucdavis)"", ""[Social Network Analysis](https://www.coursera.org/learn/social-network-analysis?specialization=computational-social-science-ucdavis)"", ""[Computer Simulations](https://www.coursera.org/learn/computer-simulations?specialization=computational-social-science-ucdavis)"", and a ""[Capstone Project](https://www.coursera.org/learn/css-capstone)"". The project involves analyzing your own social network and creating computer simulations of artificial societies. If you would like to further your knowledge of big data and machine learning, I would definitely recommend you check out this course!

https://www.coursera.org/specializations/computational-social-science-ucdavis",bigdata,coursera uc   computational social science specialization x   b    hello everyone  i took this course i think this group   be   in   computational social science on coursera the course is   by the university of california   features professors from all    uc campuses the specialization consists of five courses           a   the project involves analyzing your own social network   creating computer simulations of artificial societies if you   like to further your   of big     machine learning i       you check out this course  
t3_fm27e7,"Benchmarking Time Series workloads on Kudu, ClickHouse, InfluxDB and VictoriaMetrics",,bigdata,benchmarking time series   on   clickhouse     victoriametrics 
t3_flyw54,Why is Big data a need for the development of your business?,,bigdata,why is big   a   for the   of your business 
t3_flty9b,Did AI and big data aid in China rapid recovery from Covid-19,"AI, facial recognition, and big data are being used more and more frequently in countries all over the world. There are a lot of us that are wondering if this kind of invasion of privacy is worth it. Well, China may have just proved it is. While the rest of the world is starting to feel the true weight of the Covid-19 pandemic, China is making a hasty recovery that is being attributed to using these technologies to enforce stringent quarantine and travel bans. 

Here is how they did it: [https://4king.com/covid-19-facial-recognition-and-the-future-of-privacy/](https://4king.com/covid-19-facial-recognition-and-the-future-of-privacy/) 

Do you think the compromise on privacy is worth the potential gain on public health and safety? ",bigdata,  ai   big     in china   recovery from   ai facial recognition   big   are being   more   more frequently in countries all over the   there are a lot of us that are   if this   of invasion of privacy is worth it well china may have just   it is while the rest of the   is starting to feel the true weight of the     china is making a hasty recovery that is being   to using these technologies to enforce stringent quarantine   travel bans   here is how they   it      you think the compromise on privacy is worth the potential gain on public health   safety 
t3_floos7,Big Data Vs Data Science,,bigdata,big   vs   science 
t3_flln13,Analysis of Exam Papers,There is this exam which asks questions relating to the current affairs (articles which are mentioned in  1-2 news papers) of the previous 1-2 years. Is there any way so that the important  topics for the upcoming exam can be predicted based on the analysis of previous year question papers and newspapers( like frequency of sole news etc) using Big Data Analysis?,bigdata,analysis of exam papers there is this exam which asks questions relating to the current affairs articles which are   in     news papers of the previous    years is there any way so that the important  topics for the upcoming exam can be     on the analysis of previous year question papers   newspapers like frequency of sole news etc using big   analysis
t3_fliubu,How can Data Science be Instrumental in Combating Corona Outbreak?,,bigdata,how can   science be instrumental in combating corona outbreak 
t3_fl4v3v,Good time to upskill and master Data Science from scratch,,bigdata,  time to upskill   master   science from scratch 
t3_flc7k6,How AI and data science can help fight COVID-19?,,bigdata,how ai     science can help fight   
t3_fl9e55,Academic Research into Big Data,"Hi everyone,

Firstly, sorry if this is inappropriate for this subreddit, please remove this post if it is not allowed.

I am a university researcher for IE University in Madrid and my research team is working on a paper that is investigating the use of Big Data in organizations. We need data science professionals to fill out a 25-question survey. All responses will be shown in an aggregated format with no identification of the company or individual involved. These responses will only be used for academic purposes.

If there are any professional data scientists working on big data projects in this subreddit, please can you spare 5 minutes to fill out our survey? Please, only people with relevant work experience should respond, this is not for students or people who work in other fields.

Thank-you in advance, the insights will be of great value in our work.

[https://docs.google.com/forms/d/e/1FAIpQLSe4OGSoxCDZzuBcGPQusaqsEiZoXZXLDW7A3kLLjVdVp7fYQA/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSe4OGSoxCDZzuBcGPQusaqsEiZoXZXLDW7A3kLLjVdVp7fYQA/viewform?usp=sf_link)",bigdata,  research into big   hi everyone  firstly sorry if this is inappropriate for this   please remove this post if it is not    i am a university researcher for ie university in     my research team is working on a paper that is investigating the use of big   in organizations we     science professionals to fill out a   question survey all responses will be shown in an   format with no   of the company or     these responses will only be   for   purposes  if there are any professional   scientists working on big   projects in this   please can you spare   minutes to fill out our survey please only people with relevant work experience     this is not for   or people who work in other    thankyou in   the insights will be of great value in our work  
t3_fkunn0,"What is anomaly detection, and why you need it?",,bigdata,what is anomaly     why you   it 
t3_fl4rnw,How is Big Data useful in the Fashion World?,"Companies use Big Data to protect brand integrity via pattern recognition. This not only happens by maintaining quality control but also by building resistance to counterfeiters looking to replicate the designs sold by the company.

Counterfeit and piracy of fashionwear have led fashion brands and tech startups to find an automated solution to the long-standing problem and minimize losses by using pattern recognition. From Rolex and Gucci merchandise to Armani and YSL products, these counterfeit products could even be found in the stores of global retail giants like Amazon, Alibaba and eBay.

However, as tech companies seek a solution to counterfeit and piracy, technology has made replicating products infinitely easier. Copied to every minute detail, the luxury and medical products are replicated and sold all over the world, especially in developing countries where it is difficult to monitor activity and exercise control.

&amp;#x200B;

Read more on the same [here](https://www.gsbitlabs.com/how-ai-and-big-data-are-changing-the-fashion-world/).",bigdata,how is big   useful in the fashion   companies use big   to protect   integrity via pattern recognition this not only happens by maintaining quality control but also by   resistance to counterfeiters looking to replicate the     by the company  counterfeit   piracy of fashionwear have   fashion     tech startups to   an   solution to the   problem   minimize losses by using pattern recognition from rolex   gucci   to armani   ysl   these counterfeit     even be   in the stores of global retail giants like amazon alibaba   ebay  however as tech companies seek a solution to counterfeit   piracy technology has   replicating   infinitely easier   to every minute   the luxury       are       all over the   especially in   countries where it is   to monitor activity   exercise control  x   b    more on the same  
t3_fl5nlm,How To Benefit from Big Data Analytics in the Oil and Gas Industry?,,bigdata,how to benefit from big   analytics in the oil   gas   
t3_fl1c66,help with my assignment,"Hey!!! I'm a bit confused on how to answer this question. ""Describe how applying big data technology to social media can be useful for: 1) a chain of fitness centers, 2) a large government agency, 3) a multinational fashion retail company, and 4) a global online university.

If somebody can give an example on how to answer this question of one of the parts. I would really appreciate it Thanks",bigdata,help with my assignment hey im a bit   on how to answer this question   how applying big   technology to social   can be useful for   a chain of fitness centers   a large government agency   a multinational fashion retail company     a global online university  if   can give an example on how to answer this question of one of the parts i   really appreciate it thanks
t3_fl0qmu,How Big Tech Knows You Better Than Your Mom?,,bigdata,how big tech knows you better than your mom 
t3_fkqhu8,Spark Dynamic Partition Inserts and AWS S3 — Part 2,"Hey,

Following [this post](https://www.reddit.com/r/bigdata/comments/fde3h9/apache_spark_dynamic_partition_insert/), u/RoiTeveth and I just published the [second post](https://medium.com/nmc-techblog/spark-dynamic-partition-inserts-and-aws-s3-part-2-9ba0c97ad2c0) in our series about ApacheSpark Dynamic Partition Inserts, based on our production experience at Nielsen.

In [part 1](https://medium.com/nmc-techblog/spark-dynamic-partition-inserts-part-1-5b66a145974f), we discussed what is Spark Partitioning, what is Dynamic Partition Inserts and how we leveraged it to build a Spark application that is both idempotent and efficiently utilizes the cluster resources.

In this just-released [part 2](https://medium.com/nmc-techblog/spark-dynamic-partition-inserts-and-aws-s3-part-2-9ba0c97ad2c0), we deep dive into **how Dynamic Partition Inserts works, the different S3 connectors used when running Spark on AWS EMR and Kubernetes (e.g EMRFS vs Hadoop S3A), why in some cases it can make your application run much slower, and how you can mitigate that.**",bigdata,spark   partition inserts   aws s  — part   hey  following   uroiteveth   i just   the   in our series about apachespark   partition inserts   on our   experience at nielsen  in   we   what is spark partitioning what is   partition inserts   how we   it to   a spark application that is both     efficiently utilizes the cluster resources  in this     we     into how   partition inserts works the   s  connectors   when running spark on aws emr   kubernetes eg emrfs vs   s a why in some cases it can make your application run much slower   how you can mitigate that
t3_fkmyxq,Kafka rolling upgrade made easy with Supertubes,,bigdata,kafka rolling     easy with supertubes 
t3_fkdkno,OmniSci Virtual Summit March 23 - 26 (FREE online data science summit),"Join **OmniSci for our 2020 NVIDIA GTC &amp; Gartner Data &amp; Analytics** on-demand sessions online during OmniSci's virtual summit. During this summit we will highlight new content and past speaking sessions from previous events, and showcase a virtual booth experience with demo videos, free content and swag to give away - that’s right, we are still giving away swag! Sign up today for updates leading up to OmniSci’s online summit and for a chance to win Bose Noise Cancelling Headphones 700.

**Register here:** [http://www2.omnisci.com/l/298412/2020-03-17/7x57w](http://www2.omnisci.com/l/298412/2020-03-17/7x57w)

**Featured Sessions:**

Accelerated Analytics Fir for Purpose: Scaling Out &amp; Up

Speaker: Todd Mostak, CEO &amp; Co-Founder, Omnisci

Session Overview: OmniSci has demonstrated the massive scaling possible using GPUs for computation and visualization. However, not every analytics problem or user persona requires massive scale; rather, our customers have expressed the desire to have proper-sized tools for the various problems they encounter across the enterprise. This talk will outline the vision for scaling the OmniSci platform from trillions of records in a giant data store to hundreds of millions of records on a laptop and every form factor in between. Whether you have a massive cluster of servers, a Data Science Workstation, a GPU-enabled laptop or even a CPU-only laptop, OmniSci can provide the same accelerated analytics

Using OmniSci for Interactive Exploratory Analysis on AWS

Speakers: Aaron Williams, VP of Global Community, Omnisci and Ram Dileepan, Solutions Architect, AWS

Session Overview: In this talk, we show how you can overcome the limitations of the existing analytics tools you use by using OmniSci on AWS Marketplace. OmniSci is an accelerated analytics platform that deploys quickly on AWS with NVIDIA GPU instances to query and visualize billions of rows of data delivering lower latency over other solutions. Using an open source analytics platform that deploys in Amazon’s cloud gives you the computational power and limitless capacity to scale the deployment. Learn about the range of different instances supported by OmniSci including P3 and G4 along with the variety of offerings we have with NVIDIA GPU support. Hear about use cases that help you correctly identify which offerings suit your needs the best and help you work on your data effortlessly with speed and scalability.

Flint Water Crisis: Data-Driven Solutions &amp; Transparency

Speaker: Dr. Jared Webb

Session Overview: In this session, data scientists from BlueConduit discuss how they combined their experience in machine learning, GPU-accelerated analytics, and infrastructure project management to improve the efficiency of the lead service line replacement program in Flint, Michigan.",bigdata,omnisci virtual summit march        free online   science summit join omnisci for our        gtc  gartner    analytics   sessions online   omniscis virtual summit   this summit we will highlight new content   past speaking sessions from previous events   showcase a virtual booth experience with     free content   swag to give away  that’s right we are still giving away swag sign up   for     up to omnisci’s online summit   for a chance to win bose noise cancelling        register here     sessions    analytics fir for purpose scaling out  up  speaker   mostak ceo    omnisci  session overview omnisci has   the massive scaling possible using gpus for computation   visualization however not every analytics problem or user persona requires massive scale rather our customers have   the   to have   tools for the various problems they encounter across the enterprise this talk will outline the vision for scaling the omnisci platform from trillions of   in a giant   store to   of millions of   on a laptop   every form factor in between whether you have a massive cluster of servers a   science workstation a   laptop or even a cpuonly laptop omnisci can   the same   analytics  using omnisci for interactive exploratory analysis on aws  speakers aaron williams vp of global community omnisci   ram   solutions architect aws  session overview in this talk we show how you can overcome the limitations of the existing analytics tools you use by using omnisci on aws marketplace omnisci is an   analytics platform that   quickly on aws with   gpu instances to query   visualize billions of rows of     lower latency over other solutions using an open source analytics platform that   in amazon’s   gives you the computational power   limitless capacity to scale the   learn about the range of   instances   by omnisci   p    g  along with the variety of offerings we have with   gpu support hear about use cases that help you correctly   which offerings suit your   the best   help you work on your   effortlessly with     scalability  flint water crisis   solutions  transparency  speaker     webb  session overview in this session   scientists from     how they   their experience in machine learning   analytics   infrastructure project management to improve the efficiency of the   service line replacement program in flint michigan
t3_fkiyqj,"A fiasco in the making? As the coronavirus pandemic takes hold, we are making decisions without reliable data",,bigdata,a fiasco in the making as the coronavirus   takes   we are making   without reliable   
t3_fkkk1e,4 Ways Big Data is Useful in Mobile Application Development,"Big Data is used in retail to improve customer experience and in finance to offer more personalized services to clients. Nowadays Big Data is almost everywhere. As the penetration of mobile devices and software applications is increasing, the [Big Data industry](http://www.digitalsoftw.com/2020/02/27/4-ways-big-data-is-useful-in-mobile-application-development/) is growing along with it.

It’s no surprise that the applications of Big Data to build mobile apps are picking up pace. As mobile applications are increasingly becoming part of people’s life, Big Data has made its way to improve mobile applications.

Big Data industry, Big Data professionals, Mobile Application Development, Big data analytics, big data analyst, applications of Big Data

[http://www.digitalsoftw.com/2020/02/27/4-ways-big-data-is-useful-in-mobile-application-development/](http://www.digitalsoftw.com/2020/02/27/4-ways-big-data-is-useful-in-mobile-application-development/)",bigdata,  ways big   is useful in mobile application   big   is   in retail to improve customer experience   in finance to offer more   services to clients   big   is almost everywhere as the penetration of mobile     software applications is increasing the   is growing along with it  it’s no surprise that the applications of big   to   mobile apps are picking up pace as mobile applications are increasingly becoming part of people’s life big   has   its way to improve mobile applications  big     big   professionals mobile application   big   analytics big   analyst applications of big    
t3_fkk4ki,YARN Hadoop – Big Data Path,,bigdata,yarn   – big   path 
t3_fkb8xv,An interview about the CouchDB document database and the work being done to rearchitect it to run on top of FoundationDB,,bigdata,an interview about the         the work being   to rearchitect it to run on top of   
t3_fk6klr,Data Analytics Provides New Insights on Email Marketing Metrics,,bigdata,  analytics   new insights on email marketing metrics 
t3_fkalyh,How big data is reshaping aging research and education,,bigdata,how big   is reshaping aging research     
t3_fjytsa,Detecting COVID-19 in x-ray images,,bigdata,    in xray images 
t3_fjnzm8,Big Data and Analytics in the COVID-19 Era,,bigdata,big     analytics in the   era 
t3_fjhkf4,How Machine Learning protects big data: ML in Cybersecurity - Parsers,,bigdata,how machine learning protects big   ml in cybersecurity  parsers 
t3_fjgboh,What is Hadoop MapReduce?,,bigdata,what is     
t3_fizqnz,What frameworks are people using to sequence ETL workflows,Airflows used to be the older way.,bigdata,what frameworks are people using to sequence etl workflows airflows   to be the   way
t3_fj65sy,FREE Data Science Tools,,bigdata,free   science tools 
t3_fishhu,Which analytics platform is best?,"I am looking at alteryx, dataiku, rapidminer and SAS. We are a 5k employee firm and I have a suspicion we might need licenses for different products for technical and business users.

Would appreciate if some one can highlight what they like or dislike about any of these products? And if they are primarily same when it comes to functionality, ear of use and integration with data sources / AutoML tools etc?",bigdata,which analytics platform is best i am looking at alteryx       sas we are a  k employee firm   i have a suspicion we might   licenses for     for technical   business users    appreciate if some one can highlight what they like or   about any of these     if they are primarily same when it comes to functionality ear of use   integration with   sources  automl tools etc
t3_fih6t0,The concept of workflow engines,,bigdata,the concept of workflow engines 
t3_fihi2o,How does spark-submit.sh work with different modes and different cluster managers?,,bigdata,how   sparksubmitsh work with         cluster managers 
t3_fihhuw,Do I need to install something else besides Spark release package?,,bigdata,  i   to install something else   spark release package 
t3_fi343t,Deadline extended for the FDA Open Data Adverse Event Anomalies Challenge,"The [Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge](https://go.usa.gov/xdMxv) submission period has been extended to May 18th. Thank you so much to those of you that have already submitted to the challenge. We are extremely excited to review your work! 

If you have any questions about the challenge, please feel free to post them in this thread and we will respond as quickly as possible.",bigdata,    for the   open     event anomalies challenge the   submission   has been   to may   th thank you so much to those of you that have     to the challenge we are extremely   to review your work   if you have any questions about the challenge please feel free to post them in this     we will   as quickly as possible
t3_fi4dcr,Help Needed- Image Labeling Bounty - Compensation Available,"&amp;#x200B;

https://preview.redd.it/vg8991nnhhm41.png?width=740&amp;format=png&amp;auto=webp&amp;s=a284ff71a43817ceacc90d0315e6b48251e79222

Greetings! We are offering the opportunity for AI, Machine Learning, and Data Science enthusiasts and professionals to join our image tagging bounty. It is completely free to join, and you can earn compensation (paid in cryptocurrency) for completing the bounty. No previous technical experience is required.  

For this challenge, we are asking the community to appropriately classify a simple image based on what you see within the image. Each image will only a few seconds to complete, and the entire bounty can be completed within a few hours.  You will simply draw a box around the object pictured in the image, and then select from a drop down menu what you see in the image. We would love it if you joined us!  

**Your time is valuable, and that's why we're giving 15,000 KAT to every participant who successfully completes the bounty.**  

**🤖Bounty Details**

👉Registration begins: 7:30 PM UTC+7, March 13th, 2020 

👉Registration closes: 11:59 PM UTC+7 March 27th, 2020 

👉Labeling Period Begins: 00:00 AM UTC+7, March 28th, 2020

👉Labeling Period Deadline: 11:59 PM UTC+7, April 12th, 2020  

&amp;#x200B;

✍️To sign up, visit the official bounty page by visiting  [https://app.kambria.io/bounty/5e68936c54c01e8af7952989](https://app.kambria.io/bounty/5e68936c54c01e8af7952989) 

If you would like to participate in this bounty, please register and sign up at your earliest convenience. **Spots are limited, so do not wait!**",bigdata,help   image labeling bounty  compensation available x   b    greetings we are offering the opportunity for ai machine learning     science enthusiasts   professionals to join our image tagging bounty it is completely free to join   you can earn compensation   in cryptocurrency for completing the bounty no previous technical experience is      for this challenge we are asking the community to appropriately classify a simple image   on what you see within the image each image will only a few   to complete   the entire bounty can be   within a few hours  you will simply   a box   the object   in the image   then select from a     menu what you see in the image we   love it if you   us    your time is valuable   thats why were giving       kat to every participant who successfully completes the bounty    🤖bounty    👉registration begins     pm utc  march   th        👉registration closes      pm utc  march   th        👉labeling   begins      am utc  march   th       👉labeling          pm utc  april   th         x   b  ✍️to sign up visit the official bounty page by visiting     if you   like to participate in this bounty please register   sign up at your earliest convenience spots are   so   not wait
t3_fi20py,bigdata big problems,"Is there a way to spot people's problem in big data ? if yes what kind of data should i look or check in to ?

&amp;#x200B;

for e.g. lets assume in 1000 people who are registered to database 100 of them did not how to tie shoe lace how can we come to conclusion they dont know how to tie their shoes .",bigdata,  big problems is there a way to spot peoples problem in big    if yes what   of     i look or check in to   x   b  for eg lets assume in      people who are   to       of them   not how to tie shoe lace how can we come to conclusion they   know how to tie their shoes 
t3_fhyrzt,What cluster managers are or will be popularly used with Spark ?,"Spark online documentation says Spark applications can run on clusters via a number of cluster managers: Spark's standalone, Yarn, Meso, Kubernetes, ...

- In industry, which one(s) are the popular currently and what will be in the future?
- I am trying to pick some to learn about. So I am also asking for learning purpose.

Thanks.",bigdata,what cluster managers are or will be popularly   with spark  spark online   says spark applications can run on clusters via a number of cluster managers sparks   yarn meso kubernetes    in   which ones are the popular currently   what will be in the future  i am trying to pick some to learn about so i am also asking for learning purpose  thanks
t3_fi5bsp,Companies: All Your Data Are Belong to Us,,bigdata,companies all your   are belong to us 
t3_fhxxi7,Big Data definition by characteristics and concepts,"We took a different approach to define Big Data.  
We  define it by the traditional 4V characteristics and then we point out  the concepts which are often applied in Big Data Solutions.   


What is your take?   


Are we missing noteworthy typical Big Data concepts?  
Which concepts do you consider most important?  


https://www.iunera.com/kraken/fabric/big-data/",bigdata,big     by characteristics   concepts we took a   approach to   big     we    it by the    v characteristics   then we point out  the concepts which are often   in big   solutions      what is your take      are we missing noteworthy typical big   concepts   which concepts   you   most important     
t3_fhoar9,Is ORM often used in data engineering?,"In web application development, ORM is often used though not necessary. For example, EF in ASP.NET web development, SQLAlchemy in Flask web development, Hibernate/JPA in Spring web development

In data engineering or big data, is ORM as important and often used?  I haven't seen so in the limited number of simple examples

- in Kafka, reading and writing data from and to data systems
- in Spark, reading and writing data from and to data systems

But I might miss something.

Thanks.",bigdata,is orm often   in   engineering in web application   orm is often   though not necessary for example ef in aspnet web   sqlalchemy in flask web   hibernatejpa in spring web    in   engineering or big   is orm as important   often    i havent seen so in the   number of simple examples   in kafka     writing   from   to   systems  in spark     writing   from   to   systems  but i might miss something  thanks
t3_fhl979,Is Spark a framework or a set of libraries?,"https://en.wikipedia.org/wiki/Apache_Spark says

&gt; Apache Spark is an open-source distributed general-purpose cluster-computing framework. 
Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. 

In the Spark application examples written in Scala and Java that I have seen, they have `main()` method as entry point. The examples use Spark in the way of libraries. So is Spark a framework or a set of libraries?

Thanks.",bigdata,is spark a framework or a set of libraries  says   apache spark is an opensource   generalpurpose clustercomputing framework  spark   an interface for programming entire clusters with implicit   parallelism   fault tolerance   in the spark application examples written in scala   java that i have seen they have main   as entry point the examples use spark in the way of libraries so is spark a framework or a set of libraries  thanks
t3_fhu430,What is Apache Cordova?,,bigdata,what is apache   
t3_fhe2ou,What Is Big Data and Where to Find It: Industries That Have Tamed Their Data,,bigdata,what is big     where to   it   that have   their   
t3_fhc5v5,The top 15 terms Every Big Data Professional Must know,"According to SAS, “Big data is a term that describes the large volume of data – both structured and unstructured – that inundates a business on a day-to-day basis.” The important part, however, is not so much the volume of data as what an organization does with it – it could be a great source of insights to guide better strategic decision-making. These possibilities have brought the big data industry into prominence, and attracted numerous people to the idea of taking up a job as a big data professional.

big data industry, big data professional, big data career, data science and big data analytics, career in big data, certified data scientist, big data terms

[http://www.thegoogleblog.com/education/jobs-career/the-top-15-terms-every-big-data-professional-must-know/](http://www.thegoogleblog.com/education/jobs-career/the-top-15-terms-every-big-data-professional-must-know/)",bigdata,the top    terms every big   professional must know   to sas “big   is a term that   the large volume of   – both       – that   a business on a   basis” the important part however is not so much the volume of   as what an organization   with it – it   be a great source of insights to   better strategic   these possibilities have brought the big     into prominence     numerous people to the   of taking up a job as a big   professional  big     big   professional big   career   science   big   analytics career in big       scientist big   terms  
t3_fh6pm6,What is the relation between a Spark application's processes and a cluster manager's processes?,,bigdata,what is the relation between a spark applications processes   a cluster managers processes 
t3_fhayc7,In Depth Difference Between Big Data And Cloud Computing,,bigdata,in     between big       computing 
t3_fgy72w,Big data – big losses: biggest Big Data leaks in 3 years - Parsers,,bigdata,big   – big losses biggest big   leaks in   years  parsers 
t3_fhcem2,Top-Notch Reasons Why choose DASCA for Data Science Certifications,"Whether you’re looking to start your career in data science or are well along in your career, a credible certification in 2020 will help navigate your career in data science.

To address the skill-gap, the [Data Science Council of America (DASCA)](https://www.dasca.org/), an online program disruptor in data science projects to reskill tech professionals today. Today, DASCA is hitting its stride by offering structured and detailed data science certification programs. Based on ‘eLearning Industry’s statistics’ it is said that there will be a 28% rise in organizations that will look up to online learning programs and corporate training.

DASCA Data Science Certifications, data science industry, data science professional, data science career, data science and big data analytics, Certified Data Scientists

[https://www.basisschooldeark.com/education/top-notch-reasons-why-choose-dasca-for-data-science-certifications.html](https://www.basisschooldeark.com/education/top-notch-reasons-why-choose-dasca-for-data-science-certifications.html)",bigdata,topnotch reasons why choose   for   science certifications whether you’re looking to start your career in   science or are well along in your career a   certification in      will help navigate your career in   science  to   the skillgap the   an online program   in   science projects to reskill tech professionals       is hitting its   by offering         science certification programs   on ‘elearning  ’s statistics’ it is   that there will be a    rise in organizations that will look up to online learning programs   corporate training      science certifications   science     science professional   science career   science   big   analytics     scientists  
t3_fguvxk,Coursera Pi Day Promo - Data Science programs goong at $3.14,,bigdata,coursera pi   promo    science programs goong at     
t3_fgyxoz,How to explain any black-box machine learning model?,"With the given debacles in AI implementation and a lot of data scientists simply uploading data in and throwing out results without evaluating the ""why"" and ""how"" behind these algorithms, it's necessary to debug your model to remove biases and learn where the model fails to perform and why. 

Here is a recent article that appeared in Towards Data Science and will people get started with the notion of explainability.  
Let me know what you all think about this new field of xAI!

[https://towardsdatascience.com/how-to-build-machine-learning-algorithms-that-we-can-all-trust-cb00f1c055e1?source=activity---post\_recommended](https://towardsdatascience.com/how-to-build-machine-learning-algorithms-that-we-can-all-trust-cb00f1c055e1?source=activity---post_recommended)",bigdata,how to explain any blackbox machine learning   with the given   in ai implementation   a lot of   scientists simply     in   throwing out results without evaluating the why   how   these algorithms its necessary to   your   to remove biases   learn where the   fails to perform   why   here is a recent article that   in     science   will people get   with the notion of explainability   let me know what you all think about this new   of xai  
t3_fguct5,"Will Redis, Confluent and MongoDB follow same fate as Hadoop (Hortonworks / Cloudera)?","I personally feel AWS killed Hadoop - they just took the open source code and made managed Hadoop available as a cloud service. Soon enough, Hadoop guys were left for dry and are now losing customers consistently.

Now, I think you can make the case that data lakes are less relevant than data pipelines etc (esp with Data Science and ML on the rise). BUT the problem is still the same - these guys are struggling to beat back AWS strip-sourcing. 

What do you think? Am I too pessimistic?",bigdata,will   confluent     follow same fate as   hortonworks    i personally feel aws      they just took the open source           available as a   service soon enough   guys were left for     are now losing customers consistently  now i think you can make the case that   lakes are less relevant than   pipelines etc esp with   science   ml on the rise but the problem is still the same  these guys are struggling to beat back aws stripsourcing   what   you think am i too pessimistic
t3_fgr841,AutoAI: Synchronize ModelOps and DevOps to drive digital transformation,,bigdata,autoai synchronize       to     transformation 
t3_fgvceo,Top 20 Data Science Platforms &amp; Their Most Common Uses,"A platform helps to create a logical workflow, gives version controls, and facilitates integrations. It scales and creates better models in lesser time. In this DASCA infographic, a compiled list of commonly used data science platforms by data scientists across the globe are briefed with its main functionalities.

Have a quick rundown of some of the best data science platforms that are extensively used to turn their data into a valuable resource and create business value.

Data Science Platforms, Data Science Professionals, Data science career, data science and big data analytics, data science industry, data science certificate programs

[https://www.yahooposts.com/top-20-data-science-platforms-their-most-common-uses/](https://www.yahooposts.com/top-20-data-science-platforms-their-most-common-uses/)",bigdata,top      science platforms  their most common uses a platform helps to create a logical workflow gives version controls   facilitates integrations it scales   creates better   in lesser time in this   infographic a   list of commonly     science platforms by   scientists across the globe are   with its main functionalities  have a quick   of some of the best   science platforms that are extensively   to turn their   into a valuable resource   create business value    science platforms   science professionals   science career   science   big   analytics   science     science certificate programs  
t3_fgvlth,How Hadoop Certifications Help Bag a Data Science Job in 2020,"The role of Hadoop software framework in data science jobs is critical with every organization posting data science openings seeking candidate’s close familiarity with Hadoop. As per IDC (International Data Corporation), the global big data industry size in 2020 would skyrocket to a whopping $203 billion!

As per Indeed, a Hadoop Administrator in the current times is compensated with a median annual salary of $123,000.

And, if the words of Alice Hill - MD at Dice (a popular tech job posting portal) are to be believed, openings for Hadoop jobs have surged up by 64% in 2019 compared to the year before that. She also mentioned that Hadoop has emerged as a leader in the Big Data job postings.

Hadoop Certifications, data science certifications, Data Science Job in 2020, big data industry, big data professionals, data science industry, data science professional, data science and big data analytics, hadoop data scientist certification

&amp;#x200B;

[https://www.heatbud.com/post/technology-how-hadoop-certifications-help-bag-a-data-science-job-in-2020](https://www.heatbud.com/post/technology-how-hadoop-certifications-help-bag-a-data-science-job-in-2020)",bigdata,how   certifications help bag a   science job in      the role of   software framework in   science jobs is critical with every organization posting   science openings seeking  ’s close familiarity with   as per   international   corporation the global big     size in        skyrocket to a whopping     billion  as per   a     in the current times is   with a   annual salary of           if the   of alice hill    at   a popular tech job posting portal are to be   openings for   jobs have   up by    in        to the year before that she also   that   has   as a   in the big   job postings    certifications   science certifications   science job in      big     big   professionals   science     science professional   science   big   analytics     scientist certification  x   b  
t3_fgqneb,Alteryx Modelling Capabilities,"When we got Alteryx a few years ago, we thought that bespoke packages in R and Python were the way to go for really high performing models (standard packages on Alteryx were just alright).

But now I am thinking that the Python Jupyter tool can potentially change that. Would love to hear thoughts from you guys (and girls).",bigdata,alteryx   capabilities when we got alteryx a few years ago we thought that bespoke packages in r   python were the way to go for really high performing     packages on alteryx were just alright  but now i am thinking that the python jupyter tool can potentially change that   love to hear thoughts from you guys   girls
t3_fgvrff,What is Big Data | Big Data Tutorial | Big Data Examples,,bigdata,what is big    big   tutorial  big   examples 
t3_fgcvsg,"Do Spark, Kafka and Elastic Search work together?","Kafka is used as a messaging system and a storage system.
Elastic search is a storage system. 
Spark is a computing engine.

Do the three  often work together? Or just two of them often work together?

In what way do the three or two of them work together?
Do they need something else to work together?

Thanks.",bigdata,  spark kafka   elastic search work together kafka is   as a messaging system   a storage system elastic search is a storage system  spark is a computing engine    the three  often work together or just two of them often work together  in what way   the three or two of them work together   they   something else to work together  thanks
t3_fga9fd,Why is Big data a need for the development of your business? - TechPerks,,bigdata,why is big   a   for the   of your business  techperks 
t3_ffwnay,An interview about how a data hub architecture can reduce the overhead of managing data governance and compliance across an organization,,bigdata,an interview about how a   hub architecture can   the   of managing   governance   compliance across an organization 
t3_fg6q9f,Reference implementation for a new NoSQL query language paradigm.,,bigdata,reference implementation for a new nosql query language   
t3_fg471r,Red Hat Data Analytics Infrastructure Solution - Web Secure Media,,bigdata,  hat   analytics infrastructure solution  web secure   
t3_fg3ls8,Big data analysis: how Big Data helps to develop a metropolis,,bigdata,big   analysis how big   helps to   a metropolis 
t3_ffreh4,Kafka disaster recovery on Kubernetes using MirrorMaker2,,bigdata,kafka   recovery on kubernetes using mirrormaker  
t3_ffuf44,Big data analysis: how Big Data helps to develop a metropolis - Parsers,,bigdata,big   analysis how big   helps to   a metropolis  parsers 
t3_ffpeex,What is Data Analyst?,,bigdata,what is   analyst 
t3_ffa3g5,How Big Data Helped Netflix Series House of Cards Become a Blockbuster?,,bigdata,how big     netflix series house of   become a blockbuster 
t3_ffafcj,Why we need general AI and why we're not there yet,,bigdata,why we   general ai   why were not there yet 
t3_feyerw,Certifications: Cloudera CCA175,"Has anyone taken the CCA Spark and Hadoop Developer Certification (CCA175) in 2020?
Required skills are not really clear: the english page talks about Spark only, pages in different languages (IT, DE, FR) talk about other tools like Sqoop and HDFS CLI.

What should I expect during the exam?",bigdata,certifications   cca    has anyone taken the cca spark       certification cca    in        skills are not really clear the english page talks about spark only pages in   languages it   fr talk about other tools like sqoop     cli  what   i expect   the exam
t3_feq9jb,Need help finding data,"Hi I don’t know if this is the right place to post this, but I really need help finding data for a school project. I am trying to find a correlation between high school drop out rates and minor (below age of 18) employment rates. I have been looking for two days now and can’t seem to find any data on the information I’m looking for. Any help will be appreciated. Thank you in advance for your help!",bigdata,  help     hi i  ’t know if this is the right place to post this but i really   help     for a school project i am trying to   a correlation between high school   out rates   minor below age of    employment rates i have been looking for two   now   can’t seem to   any   on the information i’m looking for any help will be   thank you in   for your help
t3_fe11g6,How Netflix uses Druid for Real-time Insights to Ensure a High-Quality Experience,[https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06](https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06),bigdata,how netflix uses   for realtime insights to ensure a highquality experience 
t3_febxmz,What awaits Big Data in 2020: results of the past decade and future prospects - Parsers,,bigdata,what awaits big   in      results of the past     future prospects  parsers 
t3_fea64x,How to Load Parquet file into Snowflake table,,bigdata,how to   parquet file into snowflake table 
t3_fec2cq,Big Data Benefits In HR Department,,bigdata,big   benefits in hr   
t3_fe87a0,Apache Spark vs Hadoop,,bigdata,apache spark vs   
t3_fe4tlq,Planet Analytics Feeds shortens big data projects,,bigdata,planet analytics   shortens big   projects 
t3_fe31a5,airflow with kubernetes executor,"does anyone have experience running airflow with kubernetes executor?

I am not able to use this option

pod\_template\_file =",bigdata,airflow with kubernetes executor   anyone have experience running airflow with kubernetes executor  i am not able to use this option    
t3_fe0mij,eDiscovery Importance,[https://podcasts.apple.com/us/podcast/ediscovery/id1456416350?i=1000467549437](https://podcasts.apple.com/us/podcast/ediscovery/id1456416350?i=1000467549437),bigdata,  importance 
t3_fdukk9,Big data – big losses: biggest Big Data leaks in 3 years - Parsers,,bigdata,big   – big losses biggest big   leaks in   years  parsers 
t3_fdx92l,[Question] Is there a way to generate avro schema file 'avsc' from a Hive table?,"Can we generate a avro schema file from Hive table using 

Hive , sqoop, spark or any of the hadoop/bigdata technologies?

Thanks",bigdata,  is there a way to generate avro schema file avsc from a hive table can we generate a avro schema file from hive table using   hive  sqoop spark or any of the   technologies  thanks
t3_fdrfpc,Government agency looking to manage silo’d data better across the enterprise. Considering Palantir but was wondering if anyone here had experience or thoughts?,,bigdata,government agency looking to manage silo’    better across the enterprise   palantir but was   if anyone here   experience or thoughts 
t3_fdtyr9,Multi-dimensional Time Series Analysis with OLAP | iunera,,bigdata,  time series analysis with olap  iunera 
t3_fdtva5,Big Data Information Architecture,,bigdata,big   information architecture 
t3_fdt2uf,Closing the gender data gap with Data2X,,bigdata,closing the     gap with   
t3_fdtxi0,Multi-dimensional Time Series Analysis with OLAP,,bigdata,  time series analysis with olap 
t3_fdic70,SQL server for 50 TB+ of data?,Which db should I use (currently MS SQL server) to store and query 50tb+ of data. Front end app queries tables from SQL. But queries will become slow as data size gets bigger. Looked into sharding the sql server but is this the best way for my use case?,bigdata,sql server for    tb of   which     i use currently ms sql server to store   query   tb of   front   app queries tables from sql but queries will become slow as   size gets bigger   into   the sql server but is this the best way for my use case
t3_fdquhb,Singing about big data can make its ideas more accessible?,,bigdata,singing about big   can make its   more accessible 
t3_fdiecj,Reminder for the FDA Open Data Adverse Event Anomalies Challenge,"As a friendly reminder for those of you participating in the precisionFDA [Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge](https://go.usa.gov/xdfSq), you have nine days left before the submission period closes on March 13th. 

For ease of access, openFDA data files can now be downloaded automatically by parsing this [JSON file](https://urldefense.com/v3/__https:/api.fda.gov/download.json__;!!May37g!facF94rAOSPNu5u2M7UP-XjxFwPz2c7A-9wH07FtXmp2ae0IJ76lG3orRR48ONY$). Please also note that providing code is no longer required for a valid final submission. Selected contributors will be invited to participate in a panel at the [Modernizing FDA’s Data Strategy public meeting](https://urldefense.com/v3/__https:/www.fda.gov/news-events/fda-meetings-conferences-and-workshops/modernizing-fdas-data-strategy-03272020-03272020__;!!May37g!facF94rAOSPNu5u2M7UP-XjxFwPz2c7A-9wH07FtXmp2ae0IJ76lG3orHC5iLpA$). We are also pleased to announce that the Journal of the American Medical Informatics Association (JAMIA) supports the submission of a paper describing the challenge and the insights that emerge from it. 

If you are interested in learning more about how FDA is modernizing its data strategy, please attend the [Modernizing FDA’s Data Strategy public meeting](https://urldefense.com/v3/__https:/www.fda.gov/news-events/fda-meetings-conferences-and-workshops/modernizing-fdas-data-strategy-03272020-03272020__;!!May37g!facF94rAOSPNu5u2M7UP-XjxFwPz2c7A-9wH07FtXmp2ae0IJ76lG3orHC5iLpA$) on March 27th. The meeting will also be streamed virtually for those who don’t live in the DC area!

We are very excited to see what Anomalies you find!",bigdata,  for the   open     event anomalies challenge as a     for those of you participating in the     you have nine   left before the submission   closes on march   th   for ease of access     files can now be   automatically by parsing this   please also note that     is no longer   for a   final submission   contributors will be   to participate in a panel at the   we are also   to announce that the journal of the american   informatics association jamia supports the submission of a paper   the challenge   the insights that emerge from it   if you are   in learning more about how   is   its   strategy please   the   on march   th the meeting will also be   virtually for those who  ’t live in the   area  we are very   to see what anomalies you  
t3_fdbm6m,Understanding the Customer Journey Through Data,,bigdata,  the customer journey through   
t3_fde3h9,Apache Spark Dynamic Partition Insert,"Hey,

Our Big Data group in Nielsen uses Spark to process 10’s of TBs of raw data from Kafka and AWS S3. Currently, all of our Sark applications run on top of AWS EMR, and we launch 1000’s of nodes per day.

My colleague and I wrote a post (the first part in a series) about Spark Dynamic Partition Inserts.

We think you'll find it interesting:

[https://medium.com/nmc-techblog/spark-dynamic-partition-inserts-part-1-5b66a145974f](https://medium.com/nmc-techblog/spark-dynamic-partition-inserts-part-1-5b66a145974f)",bigdata,apache spark   partition insert hey  our big   group in nielsen uses spark to process   ’s of tbs of raw   from kafka   aws s  currently all of our sark applications run on top of aws emr   we launch     ’s of   per    my colleague   i wrote a post the first part in a series about spark   partition inserts  we think youll   it interesting  
t3_fdfwnc,Breaking up the Airflow DAG monorepo,,bigdata,breaking up the airflow   monorepo 
t3_fdf1x9,BDF Assistance!!!,"Hey Guys, I'm needing some help!

 

Step One: Business Case Evaluation.

Go to www.data.gov You will find many datasets organized by category. First, filter the datasets by ‘CSV’ under format on the left. You can continue to apply filters until you find a dataset of interest to you.

* You will want to find a dataset with at least 500 rows/records and 5 fields (including at least a couple fields in which you can perform calculations). You will also want a ‘time’ element within the dataset.
* Provide a link to the page where the dataset can be downloaded and describe why this dataset is of interest to you.
* Formulate 4 questions that could be asked about this dataset – a descriptive, diagnostic, predictive, and prescriptive (in your book). Explain why each question ‘qualifies’ for the given category.
* Finally, describe a business or industry to which this dataset would provide value.

Step Two: Data Identification.

Once you have chosen a dataset you need to complete the following tasks and answer the questions below:

* How large is the dataset (how many rows/instances)? How many fields are there?
* Describe how the data is presumed accurate and how you think the person/machine that created the dataset ensures its accuracy.
* Identify two fields/columns within the dataset you would consider most important. Is it possible these fields/columns exist in other datasets? If so, identify an additional dataset (doesn’t have to be within data.gov) that you feel would likely include these fields/columns.
* Finally, decide on and create a key performance indicator for the dataset. The KPI determines whether ‘corrective action’ is needed based on the data, and we will use this KPI in the next assignment.",bigdata,  assistance hey guys im   some help     step one business case evaluation  go to  you will   many     by category first filter the   by ‘csv’   format on the left you can continue to apply filters until you   a   of interest to you   you will want to   a   with at least               at least a couple   in which you can perform calculations you will also want a ‘time’ element within the      a link to the page where the   can be       why this   is of interest to you  formulate   questions that   be   about this   – a         prescriptive in your book explain why each question ‘qualifies’ for the given category  finally   a business or   to which this       value  step two      once you have chosen a   you   to complete the following tasks   answer the questions below   how large is the   how many rowsinstances how many   are there    how the   is   accurate   how you think the personmachine that   the   ensures its accuracy    two   within the   you     most important is it possible these   exist in other   if so   an      ’t have to be within   that you feel   likely   these    finally   on   create a key performance   for the   the kpi   whether ‘corrective action’ is     on the     we will use this kpi in the next assignment
t3_fdg40i,Getting into Big Data Career: An Overview,,bigdata,getting into big   career an overview 
t3_fd94w5,Career advice?,"I have a (tech) lead position for big data in a mid-sized company (\~1000 people). I jumped in there from a role of big data analyst / developer / engineer. So far so good.

&amp;#x200B;

There are several problems, however. I'm listing them below, looking forward to opinions.

1. I have many gaps in my knowledge as I spent most of my dev days working out problems, never having time to sit down and investigate technology in-depth. I always worked in startups and this was a result.

2. I started with dev pretty late - at 30 - and I'm now 10 years in the industry. I should've been senior until now and - while my position is reflecting that -  I don't feel confident I could hit the same position in an open market.

3. I'm a bit bored / stressed-out in my current company. IT here is considered as a cost and I don't have much opportunities to develop myself.

&amp;#x200B;

What would you do?

&amp;#x200B;

Thanks!",bigdata,career   i have a tech   position for big   in a   company      people i   in there from a role of big   analyst     engineer so far so    x   b  there are several problems however im listing them below looking   to opinions    i have many gaps in my   as i spent most of my     working out problems never having time to sit     investigate technology   i always   in startups   this was a result    i   with   pretty late  at       im now    years in the   i   been senior until now    while my position is reflecting that   i   feel   i   hit the same position in an open market    im a bit      in my current company it here is   as a cost   i   have much opportunities to   myself  x   b  what   you    x   b  thanks
t3_fd7qk8,Difference Between Big Data and Data Analytics,,bigdata,  between big       analytics 
t3_fcrge1,What are the options for a free hadoop stack in 2020 (and years to come),"Hi everyone, I'm a newly-transformed data engineer and searching for hadoop installation recently.

I've heard that you can just install CDH or HDP and call it a day. But after digging around I've noticed the following facts (please correct me if wrong):

- Cloudera Express was the only free edition of CDH, and now [Cloudera has discontinued it for CDH 6.3.3 and above][cdh]. That means CDH as a binary distributable is not accessible for free anymore (except for trial use of course).
- As Hortonworks merged with Cloudera, HDP 3.1.5 and above is not freely accessible anymore. [The documentation basically says][hdp] that you have to be a Cloudera customer to access the binaries.

At this point, I'm a little bit reluctant to install CDH or HDP as knowing that I'm not going to getting any upgrades (except by paying).

So what are other options to install hadoop?

Surely I can install upstream components individually by hand. But that's really painful and not good for management in general.

[cdh]: https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/install_software_cm_wizard.html#id_abd_dtm_25
[hdp]: https://docs.cloudera.com/HDPDocuments/Ambari-2.7.5.0/bk_ambari-installation/content/access_ambari_paywall.html",bigdata,what are the options for a free   stack in        years to come hi everyone im a     engineer   searching for   installation recently  ive   that you can just install   or     call it a   but after     ive   the following facts please correct me if wrong     express was the only free   of     now    that means   as a binary   is not accessible for free anymore except for trial use of course  as hortonworks   with           above is not freely accessible anymore    that you have to be a   customer to access the binaries  at this point im a little bit reluctant to install   or   as knowing that im not going to getting any   except by paying  so what are other options to install    surely i can install upstream components   by   but thats really painful   not   for management in general       
t3_fcz8eg,Where to start in big data?,"I am a student of web development and big data has caught my attention. I want to start learning big data, where should I start?",bigdata,where to start in big   i am a   of web     big   has caught my attention i want to start learning big   where   i start
t3_fcyrrp,VMware Cloud on AWS Vs. Cloud Volumes ONTAP,,bigdata,vmware   on aws vs   volumes ontap 
t3_fcvvf7,Structuring robust data pipelines,"Hey all,

I just finished writing an article about a data pipeline my team \[platform\] owns. I talk about:

\- Operational stability

\- Data/Event structure

\- How to introduce abstractions (and the benefits of doing so)

\- etc...

If you're into that kind of stuff, then I think you might find the article interesting!

[https://medium.com/@talham7391/structuring-a-robust-data-pipeline-24ff67783782](https://medium.com/@talham7391/structuring-a-robust-data-pipeline-24ff67783782)",bigdata,structuring robust   pipelines hey all  i just   writing an article about a   pipeline my team   owns i talk about   operational stability     structure   how to   abstractions   the benefits of   so   etc  if youre into that   of stuff then i think you might   the article interesting  
t3_fct4hg,25 Best Data Science Courses 2020,,bigdata,   best   science courses      
t3_fcz979,Need big data insights.,"My company is currently recruiting Big Data experts to understand some of their day to day pain points and where we can help.

If you feel like you have something to add, we'd love to talk to you!

[https://forms.gle/ZYxmBGw3qWpK77URA](https://forms.gle/ZYxmBGw3qWpK77URA)

Thanks",bigdata,  big   insights my company is currently recruiting big   experts to   some of their   to   pain points   where we can help  if you feel like you have something to     love to talk to you    thanks
t3_fcr6l2,Feeling so disappointed to Cassandra,"Nowadays, many people or companies recommend to use Cassandra due to its high tolerance and reliable.

My work is handling more than 2M data and they are stored in Cassandra with 4 nodes.

I feel so annoying and disappointed to Cassandra because of the following points.

1. Reading data is super slow even you have indexing with primary key. (Selecting one data with pk costs me a few seconds)
2. If get the whole data in spark frequently or update frequently, Cassandra will be dead.
3. So many limitations on CQL.
4. If you are using python, the only one choice of driver is DataStax one. You cannot set the host ports of Cassandra servers if you use the ORM model.

Now, I'm moving the dataset to mongoDB. I think they are much useful than Cassandra. At least, I can search one single item fast. 

Does anyone tell me what the adv. in production of Cassandra is?",bigdata,feeling so   to     many people or companies   to use     to its high tolerance   reliable  my work is   more than  m     they are   in   with      i feel so annoying     to   because of the following points        is super slow even you have   with primary key selecting one   with pk costs me a few     if get the whole   in spark frequently or   frequently   will be     so many limitations on cql   if you are using python the only one choice of   is   one you cannot set the host ports of   servers if you use the orm    now im moving the   to   i think they are much useful than   at least i can search one single item fast     anyone tell me what the   in   of   is
t3_fckxzl,Big Data Was A Mistake – Wisecrack Edition,,bigdata,big   was a mistake – wisecrack   
t3_fcnoyi,An interview about the ksqlDB platform and the unified experience that it provides for building stream processing applications on top of Kafka with SQL.,,bigdata,an interview about the   platform   the   experience that it   for   stream processing applications on top of kafka with sql 
t3_fccrly,Design Thinking on Big Data Architecture for AI/ML platforms,,bigdata,  thinking on big   architecture for aiml platforms 
t3_fckvfc,AI Can Help Find Illegal Opioid Sellers Online,,bigdata,ai can help   illegal   sellers online 
t3_fcp9u4,Big Data Online Training,,bigdata,big   online training 
t3_fc9qv9,Why most computation over big data will become online,,bigdata,why most computation over big   will become online 
t3_fc72x5,"Do alteryx, dataiku, rapidminer and databricks fundamentally do the same thing?","I have only seen Alteryx being used to bring data from different sources, then standardized and transformed for a result (metrics etc). Do the likes of Databricks, Dataiku, Rapidminer also do the same or do they add newer functionality that Alteryx doesnt have?

P.S. my experience with Alteryx is very limited",bigdata,  alteryx             the same thing i have only seen alteryx being   to bring   from   sources then       for a result metrics etc   the likes of       also   the same or   they   newer functionality that alteryx   have  ps my experience with alteryx is very  
t3_fcb5bc,Big Data Training Institutes in kharadi,,bigdata,big   training institutes in   
t3_fc6m8d,Designing Real-time Reports from S3 Data,"At my company, we recently built out a near real-time data pipeline (\~1 minute end to end latency from ingest to reporting dashboard). We evaluated a few different approaches for creating analytical reports from data stored in S3 for this process.

I wrote up some of our learnings into a blog post which I think might be helpful for anyone else that is building out a similar pipeline. The main AWS services we tested were:

* Athena
* ETL via Glue + Redshift
* ELT via Lambda, Firehose, &amp; Redshift

You can check out the full post here: [https://syvarth.com/post/real-time-reports-from-s3-data](https://syvarth.com/post/real-time-reports-from-s3-data)

I would love to hear any feedback or other ideas people have on how to approach this problem!",bigdata,  realtime reports from s    at my company we recently built out a near realtime   pipeline   minute   to   latency from ingest to reporting   we   a few   approaches for creating analytical reports from     in s  for this process  i wrote up some of our learnings into a blog post which i think might be helpful for anyone else that is   out a similar pipeline the main aws services we   were   athena  etl via glue     elt via   firehose     you can check out the full post here   i   love to hear any   or other   people have on how to approach this problem
t3_fcanxx,The drivetrain approach to create data product,,bigdata,the   approach to create     
t3_fc46sj,Does AutoML really help democratize predictive analytics to non-technical people?,"I know that predictive analytics has always been the purview of data scientists but I am now increasingly hearing that AutoML will open up AI / ML to the masses. You simply just drop data in and it does all of the cleaning, processing, ML model selection.

Can anyone really comment if this is true? (I don't have experience in this field so would really appreciate any insight)",bigdata,  automl really help     analytics to nontechnical people i know that   analytics has always been the purview of   scientists but i am now increasingly hearing that automl will open up ai  ml to the masses you simply just     in   it   all of the cleaning processing ml   selection  can anyone really comment if this is true i   have experience in this   so   really appreciate any insight
t3_fc5buv,DMBD'2020：Final Call for Papers (March 19),"Title: DMBD'2020：Final Call for Papers (March 19)

&amp;#x200B;

DMBD'2020: Final Call for Papers (March 19)

&amp;#x200B;

Name: The Fifth International Conference of Data Mining and Big Data (DMBD'2020)

Theme: SERVING LIFE WITH Data Science

URL: [http://dmbd2020.ic-si.org/](http://dmbd2020.ic-si.org/)

Dates: July 14-19, 2020

Location: Singidunum University, Belgrade, Serbia

&amp;#x200B;

Important Date: March 19, 2020: Final Deadline for Paper Submission.

&amp;#x200B;

Submission Details: Prospective authors are invited to contribute their original and high-quality papers to DMBD'2020 through the online submission page at [https://www.easychair.org/conferences/?conf=dmbd2020](https://www.easychair.org/conferences/?conf=dmbd2020).

 

DMBD’2020 serves as an international forum for researchers and practitioners to exchange latest advantages in theories, algorithms, models, and applications of data mining and big data as well as artificial intelligence techniques. Data mining refers to the activity of going through big data sets to look for relevant or pertinent information. Big data contains huge amount of data and information. DMBD’2020 is the fifth event after Chiang Mai event (DMBD'2019), Shanghai event (DMBD'2018), Fukuoka event (DMBD'2017) and Bali event (DMBD'2016) where more than hundreds of delegates from all over the world to attend and share their latest achievements, innovative ideas, marvelous designs and excel implementations. 

&amp;#x200B;

Prospective authors are invited to contribute high-quality papers (8-12 pages) to DMBD’2020 through Online Submission System. Papers presented at DMBD'2020 will be published in Springer (indexed by EI, ISTP, DBLP, SCOPUS, Web of Knowledge ISI Thomson, etc.), some high-quality papers will be selected for SCI-indexed International Journals.

&amp;#x200B;

Sponsored and Co-sponsored by Internatonal Association of Swarm and Evolutionary Intelligences, Singidunum University, Peking University and Southern University of Science and Technology, etc.

&amp;#x200B;

The DMBD’2020 will be held in Singidunum University at Belgrade, Serbia, which is the capital and the largest city of Serbia. Belgrade is a vibrant city, surprising in its diversity and rich in its history and culture. 

&amp;#x200B;

We look forward to welcoming you at Belgrade in 2020!

&amp;#x200B;

&amp;#x200B;

DMBD'2020 Secretariat

Email: [dmbd2020@ic-si.org](mailto:dmbd2020@ic-si.org)

WWW: [http://dmbd2020.ic-si.org](http://dmbd2020.ic-si.org)

&amp;#x200B;

\---Please contact with [dmbd2020@ic-si.org](mailto:dmbd2020@ic-si.org) to unsubscribe from us if you do not wish to receive further mail---",bigdata, ：final call for papers march    title  ：final call for papers march     x   b    final call for papers march     x   b  name the fifth international conference of   mining   big      theme serving life with   science  url     july            location   university   serbia  x   b  important   march         final   for paper submission  x   b  submission   prospective authors are   to contribute their original   highquality papers to   through the online submission page at       ’     serves as an international forum for researchers   practitioners to exchange latest   in theories algorithms     applications of   mining   big   as well as artificial intelligence techniques   mining refers to the activity of going through big   sets to look for relevant or pertinent information big   contains huge amount of     information  ’     is the fifth event after chiang mai event   shanghai event   fukuoka event     bali event   where more than   of   from all over the   to     share their latest achievements innovative   marvelous     excel implementations   x   b  prospective authors are   to contribute highquality papers     pages to  ’     through online submission system papers   at   will be   in springer   by ei istp   scopus web of   isi thomson etc some highquality papers will be   for   international journals  x   b        by internatonal association of swarm   evolutionary intelligences   university peking university   southern university of science   technology etc  x   b  the  ’     will be   in   university at   serbia which is the capital   the largest city of serbia   is a vibrant city surprising in its     rich in its history   culture   x   b  we look   to welcoming you at   in       x   b  x   b    secretariat  email     www   x   b  please contact with    to unsubscribe from us if you   not wish to receive further mail
t3_fc647z,Are you fighting the 5 biggest risks of big data?,,bigdata,are you fighting the   biggest risks of big   
t3_fbsg82,Is not implementing AMQP 1.0 a disadvantage of RabbitMQ?,,bigdata,is not implementing amqp    a   of rabbitmq 
t3_fax92g,Why predictive maintenance is the next big thing in manufacturing,,bigdata,why   maintenance is the next big thing in manufacturing 
t3_fb49ma,An analytics reference architecture for the Internet of Things (IoT),[https://imply.io/post/analytics-reference-architecture-for-iot](https://imply.io/post/analytics-reference-architecture-for-iot),bigdata,an analytics reference architecture for the internet of things iot 
t3_faz1qe,How to Organize a BigData System,"Hi Everyone,

I am working on a somewhat typical big data system where we consume from a number of data sources, transform, filter and munge the data and finally store and emit a stream of widgets to our downstream consumers. Right now we are having some internal debates on the team on how to organize our data and logic. I am reaching out to the community for feedback and to get some different perspectives on the matter. 

The way our system is laid out is we have a consuming application for each data source. Each consumer application knows how to read, parse and munge a widget for it's specific data source and sends a ""raw widget"" to our Central Widget Processing Engine (CWPE). Per-usual we have some data source specific logic, we also have logic shared between two or more data sources and we have some business logic that can be data source specific as well (e.g. Ignore all widgets of category X from this data source because we know it is bad data).

All of that logic has been spread across all of our applications, with some business logic being applied in the data source specific applications and some of the data source specific logic being handled by the Central Widget Processing Engine (CWPE). The team is considering ways to clean this all up and are debating two different world views:

One camp is starting from the principal that all the data source specific logic should live in the consumer applications, even if it means two apps share the same logic sometimes. In other words the Central Widget Processing Engine (CWPE) should only contain logic that is common to all widgets.

A natural result of this camp's perspective is that each data source will have it's own ""raw widget"" format and it's own storage (e.g. hive table) for that data source's raw widgets. The consumer applications will emit a narrow and well defined widget to the CWPE.

The other camp is starting from the principal that the Central Widget Processing Engine (CWPE) should contain all the logic, even if it only applies to single data source, and the consumer applications should just parse the data and pass a ""raw widget"" down to the CWPE. This means that a ""raw widget"" is a wide object with fields that are not used by all the data sources and all of our raw widgets will be stored in one central location with various attributes populated based on the data source. 

I am hoping the community here can give me some feedback on these two options and principals. If you have any real world stories you can share about any of the perspectives I would love to hear them as well.

Thank you!",bigdata,how to organize a   system hi everyone  i am working on a somewhat typical big   system where we consume from a number of   sources transform filter   munge the     finally store   emit a stream of   to our   consumers right now we are having some internal   on the team on how to organize our     logic i am reaching out to the community for     to get some   perspectives on the matter   the way our system is   out is we have a consuming application for each   source each consumer application knows how to   parse   munge a   for its specific   source     a raw   to our central   processing engine cwpe perusual we have some   source specific logic we also have logic   between two or more   sources   we have some business logic that can be   source specific as well eg ignore all   of category x from this   source because we know it is      all of that logic has been   across all of our applications with some business logic being   in the   source specific applications   some of the   source specific logic being   by the central   processing engine cwpe the team is   ways to clean this all up   are   two     views  one camp is starting from the principal that all the   source specific logic   live in the consumer applications even if it means two apps share the same logic sometimes in other   the central   processing engine cwpe   only contain logic that is common to all    a natural result of this camps perspective is that each   source will have its own raw   format   its own storage eg hive table for that   sources raw   the consumer applications will emit a narrow   well     to the cwpe  the other camp is starting from the principal that the central   processing engine cwpe   contain all the logic even if it only applies to single   source   the consumer applications   just parse the     pass a raw     to the cwpe this means that a raw   is a   object with   that are not   by all the   sources   all of our raw   will be   in one central location with various attributes     on the   source   i am hoping the community here can give me some   on these two options   principals if you have any real   stories you can share about any of the perspectives i   love to hear them as well  thank you
t3_farrlu,Going from Data Silos to Data Breaking Silos: Mission Impossible?,"One of the biggest challenges facing data teams or those interested in making data-driven decisions is the lack of easy access to data aka data silos. You would think that this problem would have been sorted by now, right? Well, turns out it’s not. 

Just like Tom Cruise keeps on coming back to sort out the baddies of the world in the wildly popular MI movie franchise, data access problems keep on cropping up as well. 📷

What are the causes of data silos—the baddies in the world of data...

That’s like asking what causes the sun to shine or the birds to chirp. No, really! The truth is that no matter which way you slice and dice the facts, data silos occur due to one of the two following reasons, or usually a combination:

People…  🧑‍🤝‍🧑

Technology… associated with software, vendors and platforms 👨‍💻

[https://humansofdata.atlan.com/2020/02/going-from-data-silos-to-data-breaking-silos/](https://humansofdata.atlan.com/2020/02/going-from-data-silos-to-data-breaking-silos/)",bigdata,going from   silos to   breaking silos mission impossible one of the biggest challenges facing   teams or those   in making     is the lack of easy access to   aka   silos you   think that this problem   have been   by now right well turns out it’s not   just like tom cruise keeps on coming back to sort out the   of the   in the   popular mi movie franchise   access problems keep on cropping up as well 📷  what are the causes of   silos—the   in the   of    that’s like asking what causes the sun to shine or the   to chirp no really the truth is that no matter which way you slice     the facts   silos occur   to one of the two following reasons or usually a combination  people…  🧑‍🤝‍🧑  technology…   with software     platforms 👨‍💻  
t3_farkwa,How to Use Big Data For Improving Driver Safety - Acuvate,,bigdata,how to use big   for improving   safety  acuvate 
t3_faf7cr,How to Turn Your Weaknesses Into Strength Statements In a Job Interview,"Fact is, you can get the job offer even if you’re not a perfect match  to what the job description is asking for.  You can even beat others  that are more qualified than you. It happens all the time.

But you need to know your weaknesses, how to talk about them in an interview, and turn them into **strength statements**.

Truthfully, I hate the word weakness. But for the purposes of this  article, when I say weakness I mean any lack of skill, any lack of  experience or gap in employment you might have.  Generally, you need to  know how to address any objection they may have towards your competency  for the position.

We all have weaknesses. The question is, what is the best way to talk about them and turn them into **strength statements** when they come up in an interview?

First, back up a bit and think about what it means to already have an interview.

1. Your resume made it through the ATS filters
2. A person read your resume and liked it enough to consider you a candidate and call you

That’s awesome! Your job now is to sell them on your strengths and  ease their fears about your weaknesses. By reading your resume, they may  already see some holes in your qualifications. That’s OK! They’re  always going to have some doubt about your competency unless you are  very well networked.

So how do you turn fear or doubt into trust in an interview?

## 1) Prepare your Strength Statements before your Interview

Interviews aren’t just about answering the questions you get asked.  More importantly, you need to come prepared with **Strength Statements** that can be stated alone or in almost every question you are asked.

**Strength Statements** are concise ways to communicate  your top strengths, value-adds, qualifications, and benefits you will be  bringing to the company. 

Having a solid understanding of your top qualifications will let you  easily redirect back to them if they bring up any objection toward you. 

I’ll talk more about the **Address and Redirect Method** below with word-for-word examples, which is how you shift the focus off your weaknesses and onto your Strength Statements.

But I’m not talking about any random strengths you have. You could  have skills and experience in several areas that don’t apply directly to  the job. Therefore, the first step is:

### Step 1) Determine Your Top 3 Strengths

Your top 3 strengths will be your best qualifications that apply  directly to what the job is asking for. They can be direct or  transferable skills/experience (and they should be on your resume as  well).

You probably have more than 3, but in an interview you don’t always  have the ability to say everything you want to. Therefore, it’s critical  to know what to talk about first. Think:

*What are the 3 most important things I MUST say in the interview to prove I am the best person for the job?*

But beyond simply stating your top 3 qualifications (your strengths),  consider relating them to how they will benefit the company. (your  benefits)

A strength could be your skills, experience, network, etc. For example:

* knowledge of project management
* parallel programming or 
* managing a team. 
* already knowing your teammates or the hiring manager

Those are strengths.

But you need to take it one step further and tell them how your  strengths will BENEFIT THEM. And that’s how you can make the connection  in their minds between what you have and why you are a good fit for the  job.

Ultimately they are trying to determine if you can do the job and if  you can do it well. So just talking about your strengths doesn’t fully  convey you know the problems the job is supposed to solve.

Therefore, the best **Strength Statements** mention how your qualifications will *benefit* the company in some way. Example benefits could be:

* Saving money or time
* Increased customer satisfaction
* Increased efficiency or outputs

### Step 2) Create Your Top 3 Strength Statements 

Once you’ve determined your top 3 qualifications and benefits, construct **strength statements** and write them down word-for-word.

There are several ways to communicate a strength statement in an interview. Here are a few examples:

* You can state them clearly – Strength, then Benefit

*I have a lot of experience with OpenMP and if hired I can  increase the efficiency of your existing code sets and therefore lower  computing costs.*

* You can wrap them in a behavioral-based story. 

*I once was working on this very old Fortran code and you wouldn’t believe how slow it was running until I…*

* You can lead with the Benefit and then follow with the Strength. 

*I have a lot of experience with knowing how to lower computing costs by increasing the efficiency of existing OpenMP code sets.*

You should also prepare to elaborate on your strength statements if  asked to. Therefore, make sure you have examples or stories ready to  support them.

Writing these down and practicing them out loud is one of the most  important things you can do to prepare for your interview. Why? Because  increased stress levels in your interview can botch your speech,  articulation, and memory.

But having an arsenal of prepared **strength statements**  will let you articulately convey your value and make you appear more  competent to the hiring manager in the most stressful moments.

### Example Strength Statements – Technical Support Engineer

Let’s say you are applying for a Technical Support Engineer position. Your Strength Statements might be:

* “I’m comfortable providing technical support under pressure, especially with new problems.”
* “I can handle a large support case load and still maximize customer satisfaction.”
* “I  have experience with the technology and working relationships with the  team which will allow me to start taking cases faster.”

## 2) Know your Weaknesses before your Interview

Again, a weakness is generally any lack of qualification you may have for the *specific* job you are applying to. A weakness could be:

* A lack of a hard or soft skill
* A lack of years of experience in the job *or* the industry
* A gap in employment
* A lack of education
* A lack of quality references

They will vary with each job you apply to.

You should generally already have a solid understanding of what your  weaknesses are compared to the job description. But if you didn’t study  the job description before submitting your resume, now is the time  before your interview.

1. Highlight all the hard and soft skills written in the job description. 
2. Determine which ones you don’t have or are weak at

Although the job description isn’t always the best way to determine  what’s required of the job or what qualifications you’re expected to  have. To get a better understanding of that, reach out to people at the  company on LinkedIn and ask them questions to get a deeper understanding  of the company culture, the team, and the job.

It’s better to be prepared with what’s coming. So think about what you may get asked about from your unique background.

## 3) The Address and Redirect Method

Now what do you do if you get asked about a weakness in your job  interview?  Whether you bring it up first or you wait for them to, the  strategy is the same. That is:

You **Address** it quickly and then **Redirect** the conversation back to your strength statements.

Honestly, why dwell on the negative? What the hiring manager really  wants you to do is erase their fears and convince him/her that you can  solve the problems the job entails.  Therefore, the more time you can  talk about connecting the dots between your qualifications and the  responsibilities/problems inherent in the position, the better.

And by having a solid understanding of your strengths and knowledge  of your weaknesses, you will be better able to deflect and redirect back  to what they NEED to hear before the interview is over.

The following are four examples of how to use the Address and Redirect Method with the following scenarios:

* Lack of a Skill
* Lack of Industry Experience
* Low GPA
* A Gap in Work Experience

### Example: Lack of a Skill 

Let’s say you’re applying to a Software Engineering job. You have a  lot of skill with Java but not as much with C++, which is what the job  asks for.  So what do you say if you get asked in the interview about  your knowledge of C++? 

**(Address)** *I have a little experience with C++…*

**(Redirect)** *but a few  years ago when I needed to really learn Java, I read books, I took  online courses, and I got help when needed from my coworkers. I was then  ready to write production code in just 2 month and I know that I can do  the same with C++ in this position.*

**Key Point:** If you don’t have a skill they want, then  talk about HOW you’re going to acquire that skill or gain that  experience based on HOW you gained or accomplished some previous skill  or experience in the past.

By default, always redirect back to the value and benefits you’re going to bring to the position.

### Example: Lack of Industry Experience

Let’s say you’re applying to a Sales position in the Computer Data  Storage business. You will be targeting the Financial Market, which you  have no experience in. Although, you do have a lot of experience in the  Oil and Gas Market. How would you respond to a question about your  experience in the Financial market?

**(Address)**  *…*

**(Redirect)** *The  Financial Market is interesting because &lt;KNOWLEDGE 1&gt; and &lt;  KNOWLEDGE 2&gt;. I’ve been talking to successful sales executives in the  Financial Market and what I’ve learned is that &lt;SOLUTION TO COMMON  PROBLEM&gt;. I have a lot of experience in the Oil and Gas market where  in my first couple years at my previous company, I was able to close $10  million in revenue by &lt;RELATE TO SAME SOLUTION TO COMMON  PROBLEM&gt;. I’m confident that I can achieve the same revenue targets  at COMPANY NAME. In fact, I think I can do better.*

**Key Point:** Depending on how the question comes up,  you don’t need to blatantly say you DON’T have a qualification. Instead,  you can jump right into the Redirect and talk about what you know. You  might not have the experience on your resume, but you should have done  your homework and be able to talk intelligently about the qualifications  you lack.

### Example: Low GPA

Let’s say you’re applying for your second job out of college or grad  school and your grades weren’t that great. You only have a year of job  experience and you left out your GPA on your resume. You are asked in an  interview what your GPA was. How would you respond?

**(Address)** *My GPA in school could have been better,*

**(Redirect)** *but since  then I’ve consistently added a lot of value in my past roles. For  example, at my last job I increased customer conversions by 50% by  rewriting the website’s sales copy.*

**Key Point:** You don’t always have the chance to  mention transferable skills/experience when redirecting away from a  weakness like in the last two examples. In this example, you need to go  right into one of your strength statements to ease the hiring manager’s  underlying fear that your low GPA means you won’t be able to do the job.

### Example: A Gap in Work Experience

Let’s say you have a sizable gap in your work history. For whatever  reason, you needed to take a long leave of absence.  How would you  address this in an interview?

**(Address)** *I had to take a leave of absence for a personal issue and now it is resolved.*

**(Redirect)** *…*

**Key Point:** You don’t need to lie, but you don’t need  to go into the details of a work-history gap either. You have no  obligation to tell them what it was for, so don’t disqualify yourself by  doing so. If they ask you for details, then you can just say:

*“I’d prefer to leave it at that. It was a personal issue and now it is resolved.*

Don’t redirect to anything. Just let them move on and ask you their  next question. By handling this situation in this way, you are calmly  redirecting the conversation back to questions where you can communicate  your competencies.

## Conclusion

Successfully addressing questions that expose your weaknesses starts  with a solid understanding of your strengths. When you know how to  connect the dots between your qualifications and the  responsibilities/problems inherent in the position, you can better  deflect or redirect any objection they may have against you.

The more you can redirect the conversation back to what you want them  to hear with powerful strength statements, the easier it will be to  ease any fears they may have towards your ability to do the job well.

Use the Address and Redirect Method and these four examples to craft  your own Strength Statements and steer the interview ship away from  their fears and objections. And when you do, you can better prove that  you are the hire they’ve been looking for. 

&amp;#x200B;

Read More @ HPC Careers | [hpccareers.com](https://hpccareers.com) | Career Strategies for Ambitious HPC/Big Data Professionals

Original Article: [https://hpccareers.com/how-to-turn-your-weaknesses-into-strength-statements-in-a-job-interview/](https://hpccareers.com/how-to-turn-your-weaknesses-into-strength-statements-in-a-job-interview/)",bigdata,how to turn your weaknesses into strength statements in a job interview fact is you can get the job offer even if you’re not a perfect match  to what the job   is asking for  you can even beat others  that are more   than you it happens all the time  but you   to know your weaknesses how to talk about them in an interview   turn them into strength statements  truthfully i hate the   weakness but for the purposes of this  article when i say weakness i mean any lack of skill any lack of  experience or gap in employment you might have  generally you   to  know how to   any objection they may have   your competency  for the position  we all have weaknesses the question is what is the best way to talk about them   turn them into strength statements when they come up in an interview  first back up a bit   think about what it means to   have an interview    your resume   it through the ats filters   a person   your resume     it enough to   you a     call you  that’s awesome your job now is to sell them on your strengths    ease their fears about your weaknesses by   your resume they may    see some holes in your qualifications that’s ok they’re  always going to have some   about your competency unless you are  very well    so how   you turn fear or   into trust in an interview     prepare your strength statements before your interview  interviews aren’t just about answering the questions you get    more importantly you   to come   with strength statements that can be   alone or in almost every question you are    strength statements are concise ways to communicate  your top strengths   qualifications   benefits you will be  bringing to the company   having a     of your top qualifications will let you  easily   back to them if they bring up any objection   you   i’ll talk more about the         below with   examples which is how you shift the focus off your weaknesses   onto your strength statements  but i’m not talking about any   strengths you have you    have skills   experience in several areas that  ’t apply   to  the job therefore the first step is   step     your top   strengths  your top   strengths will be your best qualifications that apply    to what the job is asking for they can be   or  transferable skillsexperience   they   be on your resume as  well  you probably have more than   but in an interview you  ’t always  have the ability to say everything you want to therefore it’s critical  to know what to talk about first think  what are the   most important things i must say in the interview to prove i am the best person for the job  but   simply stating your top   qualifications your strengths    relating them to how they will benefit the company your  benefits  a strength   be your skills experience network etc for example     of project management  parallel programming or   managing a team     knowing your teammates or the hiring manager  those are strengths  but you   to take it one step further   tell them how your  strengths will benefit them   that’s how you can make the connection  in their   between what you have   why you are a   fit for the  job  ultimately they are trying to   if you can   the job   if  you can   it well so just talking about your strengths  ’t fully  convey you know the problems the job is   to solve  therefore the best strength statements mention how your qualifications will benefit the company in some way example benefits   be   saving money or time    customer satisfaction    efficiency or outputs   step   create your top   strength statements   once you’ve   your top   qualifications   benefits construct strength statements   write them      there are several ways to communicate a strength statement in an interview here are a few examples   you can state them clearly – strength then benefit  i have a lot of experience with openmp   if   i can  increase the efficiency of your existing   sets   therefore lower  computing costs   you can wrap them in a   story   i once was working on this very   fortran     you  ’t believe how slow it was running until i…   you can   with the benefit   then follow with the strength   i have a lot of experience with knowing how to lower computing costs by increasing the efficiency of existing openmp   sets  you   also prepare to elaborate on your strength statements if    to therefore make sure you have examples or stories   to  support them  writing these     practicing them out   is one of the most  important things you can   to prepare for your interview why because    stress levels in your interview can botch your speech  articulation   memory  but having an arsenal of   strength statements  will let you articulately convey your value   make you appear more  competent to the hiring manager in the most stressful moments   example strength statements – technical support engineer  let’s say you are applying for a technical support engineer position your strength statements might be   “i’m comfortable   technical support   pressure especially with new problems”  “i can   a large support case     still maximize customer satisfaction”  “i  have experience with the technology   working relationships with the  team which will allow me to start taking cases faster”     know your weaknesses before your interview  again a weakness is generally any lack of qualification you may have for the specific job you are applying to a weakness   be   a lack of a   or soft skill  a lack of years of experience in the job or the    a gap in employment  a lack of    a lack of quality references  they will vary with each job you apply to  you   generally   have a     of what your  weaknesses are   to the job   but if you  ’t    the job   before submitting your resume now is the time  before your interview    highlight all the     soft skills written in the job        which ones you  ’t have or are weak at  although the job   isn’t always the best way to    what’s   of the job or what qualifications you’re   to  have to get a better   of that reach out to people at the  company on     ask them questions to get a      of the company culture the team   the job  it’s better to be   with what’s coming so think about what you may get   about from your unique       the          now what   you   if you get   about a weakness in your job  interview  whether you bring it up first or you wait for them to the  strategy is the same that is  you   it quickly   then   the conversation back to your strength statements  honestly why   on the negative what the hiring manager really  wants you to   is erase their fears   convince himher that you can  solve the problems the job entails  therefore the more time you can  talk about connecting the   between your qualifications   the  responsibilitiesproblems inherent in the position the better    by having a     of your strengths      of your weaknesses you will be better able to       back  to what they   to hear before the interview is over  the following are four examples of how to use the         with the following scenarios   lack of a skill  lack of   experience  low gpa  a gap in work experience   example lack of a skill   let’s say you’re applying to a software engineering job you have a  lot of skill with java but not as much with c which is what the job  asks for  so what   you say if you get   in the interview about  your   of c     i have a little experience with c…    but a few  years ago when i   to really learn java i   books i took  online courses   i got help when   from my coworkers i was then    to write     in just   month   i know that i can    the same with c in this position  key point if you  ’t have a skill they want then  talk about how you’re going to acquire that skill or gain that  experience   on how you   or   some previous skill  or experience in the past  by   always   back to the value   benefits you’re going to bring to the position   example lack of   experience  let’s say you’re applying to a sales position in the computer    storage business you will be targeting the financial market which you  have no experience in although you   have a lot of experience in the  oil   gas market how   you   to a question about your  experience in the financial market     …    the  financial market is interesting because             i’ve been talking to successful sales executives in the  financial market   what i’ve   is that solution to common  problem i have a lot of experience in the oil   gas market where  in my first couple years at my previous company i was able to close     million in revenue by relate to same solution to common  problem i’m   that i can achieve the same revenue targets  at company name in fact i think i can   better  key point   on how the question comes up  you  ’t   to blatantly say you  ’t have a qualification    you can jump right into the     talk about what you know you  might not have the experience on your resume but you   have    your homework   be able to talk intelligently about the qualifications  you lack   example low gpa  let’s say you’re applying for your   job out of college or    school   your   weren’t that great you only have a year of job  experience   you left out your gpa on your resume you are   in an  interview what your gpa was how   you      my gpa in school   have been better    but since  then i’ve consistently   a lot of value in my past roles for  example at my last job i   customer conversions by    by  rewriting the website’s sales copy  key point you  ’t always have the chance to  mention transferable skillsexperience when   away from a  weakness like in the last two examples in this example you   to go  right into one of your strength statements to ease the hiring manager’s    fear that your low gpa means you won’t be able to   the job   example a gap in work experience  let’s say you have a sizable gap in your work history for whatever  reason you   to take a long leave of absence  how   you    this in an interview    i   to take a leave of absence for a personal issue   now it is      …  key point you  ’t   to lie but you  ’t    to go into the   of a workhistory gap either you have no  obligation to tell them what it was for so  ’t   yourself by    so if they ask you for   then you can just say  “i’  prefer to leave it at that it was a personal issue   now it is     ’t   to anything just let them move on   ask you their  next question by   this situation in this way you are calmly    the conversation back to questions where you can communicate  your competencies   conclusion  successfully   questions that expose your weaknesses starts  with a     of your strengths when you know how to  connect the   between your qualifications   the  responsibilitiesproblems inherent in the position you can better    or   any objection they may have against you  the more you can   the conversation back to what you want them  to hear with powerful strength statements the easier it will be to  ease any fears they may have   your ability to   the job well  use the           these four examples to craft  your own strength statements   steer the interview ship away from  their fears   objections   when you   you can better prove that  you are the hire they’ve been looking for   x   b    more  hpc careers     career strategies for ambitious hpcbig   professionals  original article 
t3_faq8is,HDFS Features and Goals,,bigdata,  features   goals 
t3_fabcun,25 Best Online Python Courses to Learn,,bigdata,   best online python courses to learn 
t3_faa7h8,/Anyone learning from 365 Careers Data Science Bootcamp?,,bigdata,anyone learning from     careers   science bootcamp 
t3_fa9q1f,Big Data Course in Malaysia,,bigdata,big   course in malaysia 
t3_fa9jce,23 Useful Elasticsearch Example Queries,,bigdata,   useful elasticsearch example queries 
t3_fab1y0,How can I get a data science job without any prior experience?,,bigdata,how can i get a   science job without any prior experience 
t3_f9pb3u,Building a Serverless data processing platform at Nielsen,"Check out our blog post, describing how we built a data processing platform that handles 250 billion events per day, using Serverless architecture.

It discusses why we decided not to use Apache Spark for this specific use case, and what benefits we got from using AWS Lambda.

Read more about our Serverless real use-case here:  
[https://medium.com/nmc-techblog/going-serverless-a-real-use-case-1e40e80a2a9c](https://medium.com/nmc-techblog/going-serverless-a-real-use-case-1e40e80a2a9c)",bigdata,  a serverless   processing platform at nielsen check out our blog post   how we built a   processing platform that       billion events per   using serverless architecture  it   why we   not to use apache spark for this specific use case   what benefits we got from using aws      more about our serverless real usecase here   
t3_f9tt50,6 Steps to Ensuring Data Quality with Atlan - Atlan | Humans of Data,"I found this blog helpful to work on the data quality, 

Anyone who actively works on the data quality in their daily work? I'm writing a python code to solve the data quality problem universally, some ideas will definitely help :)",bigdata,  steps to ensuring   quality with atlan  atlan  humans of   i   this blog helpful to work on the   quality   anyone who actively works on the   quality in their   work im writing a python   to solve the   quality problem universally some   will   help 
t3_f9v3sx,Big data in Texas,https://texajob.com/?s=Big+data,bigdata,big   in texas 
t3_f9kfgo,Design Thinking Fundamentals,,bigdata,  thinking   
t3_f9q0as,How to use the BashOperator in Apache Airflow,,bigdata,how to use the bashoperator in apache airflow 
t3_f9l4bc,Open-source End-to-end ETL pipeline for GoodReads,,bigdata,opensource   etl pipeline for   
t3_f9808s,Py [package](https://github.com/kotartemiy/newscatcher) to collect news data from news websites,,bigdata,py   to collect news   from news websites 
t3_f9fzf8,Deadline extended for the FDA Open Data Adverse Event Anomalies Challenge,"In response to a flurry of new interest in the [Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge](https://go.usa.gov/xdyfq) we have decided to extend the submission period to March 13th. New guidance is available on the challenge site including updated evaluation criteria and anomaly examples. 

**Please also note that providing code is no longer required for a valid final submission. Selected contributors will be invited to participate in a panel at the** [**Modernizing FDA’s Data Strategy public meeting**](https://www.fda.gov/news-events/fda-meetings-conferences-and-workshops/modernizing-fdas-data-strategy-03272020-03272020)**. We are also pleased to announce that the Journal of the American Medical Informatics Association (JAMIA) supports the submission of a paper describing the challenge and the insights that emerge from it.**

If you are interested in learning more about how FDA is modernizing its data strategy, please attend the [Modernizing FDA’s Data Strategy public meeting](https://www.fda.gov/news-events/fda-meetings-conferences-and-workshops/modernizing-fdas-data-strategy-03272020-03272020) on March 27th. The meeting will also be streamed virtually for those who don’t live in the DC area!

If you have any questions about the challenge, please feel free to post them in this thread and we will respond as quickly as possible.",bigdata,    for the   open     event anomalies challenge in response to a flurry of new interest in the   we have   to   the submission   to march   th new   is available on the challenge site     evaluation criteria   anomaly examples   please also note that     is no longer   for a   final submission   contributors will be   to participate in a panel at the   we are also   to announce that the journal of the american   informatics association jamia supports the submission of a paper   the challenge   the insights that emerge from it  if you are   in learning more about how   is   its   strategy please   the   on march   th the meeting will also be   virtually for those who  ’t live in the   area  if you have any questions about the challenge please feel free to post them in this     we will   as quickly as possible
t3_f9fqm1,Education opinions needed pls,"I don't have a post-secondary education but I'd really like to study Big Data at York University in Toronto. They offer a pair of certificates, Big Data Analytics and Advanced Data Science and Predictive Analytics. These are 6 months of study each.

Other considerations aside, how reasonable is it to expect to land even an entry level job in big data, in the Toronto area, with only these certificates for an education?

Thanks for your time!",bigdata,  opinions   pls i   have a     but   really like to   big   at york university in toronto they offer a pair of certificates big   analytics       science     analytics these are   months of   each  other     how reasonable is it to expect to   even an entry level job in big   in the toronto area with only these certificates for an    thanks for your time
t3_f99y0d,A conversation about the conflicts that lead to shadow IT in data and analytics projects and how to work toward resolving those tensions.,,bigdata,a conversation about the conflicts that   to   it in     analytics projects   how to work   resolving those tensions 
t3_f99vn8,Apache Flink &amp; Apache Beam: How Beam Runs on Top of Flink,,bigdata,apache flink  apache beam how beam runs on top of flink 
t3_f9ay8x,Microsoft To Build First Cloud Data Center Region In Mexico,,bigdata,microsoft to   first     center region in mexico 
t3_f96h32,H2O.ai - Learn Machine Learning with Sparkling Water,,bigdata,h oai  learn machine learning with sparkling water 
t3_f96ela,Snowflake Tutorial with Spark Connector,,bigdata,snowflake tutorial with spark connector 
t3_f8vb9z,FREE DATA SCIENCE/AI WEBINAR,,bigdata,free   scienceai webinar 
t3_f8t9n8,Don't let Apache Kafka steal your weekends!,,bigdata,  let apache kafka steal your   
t3_f8otwi,Kafka disaster recovery on Kubernetes with CSI,,bigdata,kafka   recovery on kubernetes with csi 
t3_f8lhvo,"Big Data Testing: What is, Strategy, How to test Hadoop",,bigdata,big   testing what is strategy how to test   
t3_f8qqpw,Big data is revolutionising retail,,bigdata,big   is revolutionising retail 
t3_f89ls5,A BigData project for a newbie,"I am studying Big data as my course-work in college this sem and we have been asked to make a project in groups of 3. Since I have just started learning the theory and done some small programs in Hadoop (I am interested in shifting to pyspark as I like python) can you all recommend some projects?

The professor is more interested in the result rather than the novelty of the project so it would be better if I could show some stats and give some conclusion at the end. I have around a month for this project.",bigdata,a   project for a newbie i am   big   as my coursework in college this sem   we have been   to make a project in groups of   since i have just   learning the theory     some small programs in   i am   in shifting to pyspark as i like python can you all   some projects  the professor is more   in the result rather than the novelty of the project so it   be better if i   show some stats   give some conclusion at the   i have   a month for this project
t3_f86rqy,Learning path to get started with Bigdata,,bigdata,learning path to get   with   
t3_f7w7sx,How are event driven and message driven (reactive) different from each other?,"From *Architecting Data Intensive Applications* book

&gt; Communication	styles	define	mechanisms	that	can	be	followed	for	transmitting
&gt; data	from	one	point	to	another,	usually	across	the	network.	At	the	broad	level,
&gt; there	are	four	different	types	of	Communication	styles:
&gt; 
&gt; **Synchronous**: ...
&gt; 
&gt; **Asynchronous**: ...
&gt; 
&gt; **Event-Driven**:	This	is	an	asynchronous	message-driven	communication style	to	propagate	information.	An	event	source	publishes	events,	and	event
&gt; receivers	can	choose	to	listen	to	or	filter	out	specific	events,	and	make
&gt; proactive	decisions	in	near-realtime	about	how	to	react	to	the	events.	Events
&gt; are	propagated	in	near-realtime	throughout	a	highly	distributed
&gt; environment.	The	events-based	communication	architecture	promotes	low
&gt; latency	and	higher	decoupling	of	distributed	systems.	It	is	based	on	the
&gt; Publish/Subscribe	and	Push-based	messaging	pattern,	where	events	are
&gt; pushed	out	to	interested	listeners.	An	event	is	an	autonomous	message
&gt; containing	just	enough	information	to	represent	a	unit	of	work	and	to
&gt; provide	decision	support	for	notification	receivers.
&gt; 
&gt; 
&gt; **Reactive**:	Reactive	Systems	are	Responsive,	Resilient,	Elastic,	and **Message-Driven**.	They	rely	on	asynchronous	message-passing	to	establish	a
&gt; boundary	between	components	that	ensures	loose-coupling,	isolation,
&gt; location	transparency,	and	provides	the	means	to	delegate	errors	as
&gt; messages.	Employing	explicit	message-passing	enables	load	management,
&gt; elasticity,	and	flow-control	by	shaping	and	monitoring	the	message	queues
&gt; in	the	system,	and	applying	back-pressure	when	necessary.	Location-
&gt; transparent	messaging	as	a	means	of	communication	makes	it	possible	for the	management	of	failure	to	work	with	the	same	constructs	and	semantics
across	a	cluster	or	within	a	single	host.	Non-blocking	communication
allows	recipients	to	only	consume	resources	while	active,	leading	to	less
system	overhead.

The third communication style is ""event drive"", and the fourth is ""Reactive"" which is said to be ""message driven"". How are event driven and message driven different from each other?

Is event driven necessarily based on publisher-subscriber model, and message queues/brokers?

Is message driven or ""reactive"" necessarily based on directly message passing (e.g. web services, RPC, ...)?

Thanks.",bigdata,how are event     message   reactive   from each other from architecting   intensive applications book   communication	styles	 	mechanisms	that	can	be	 	for	transmitting   	from	one	point	to	another	usually	across	the	network	at	the	 	level  there	are	four	 	types	of	communication	styles    synchronous     asynchronous      	this	is	an	asynchronous	 	communication style	to	propagate	information	an	event	source	publishes	events	 	event  receivers	can	choose	to	listen	to	or	filter	out	specific	events	 	make  proactive	 	in	nearrealtime	about	how	to	react	to	the	events	events  are	 	in	nearrealtime	throughout	a	highly	   environment	the	 	communication	architecture	promotes	low  latency	 	higher	 	of	 	systems	it	is	 	on	the  publishsubscribe	 	 	messaging	pattern	where	events	are   	out	to	 	listeners	an	event	is	an	autonomous	message  containing	just	enough	information	to	represent	a	unit	of	work	 	to   	 	support	for	notification	receivers      reactive	reactive	systems	are	responsive	resilient	elastic	   	they	rely	on	asynchronous	messagepassing	to	establish	a   	between	components	that	ensures	loosecoupling	isolation  location	transparency	 	 	the	means	to	 	errors	as  messages	employing	explicit	messagepassing	enables	 	management  elasticity	 	flowcontrol	by	shaping	 	monitoring	the	message	queues  in	the	system	 	applying	backpressure	when	necessary	location  transparent	messaging	as	a	means	of	communication	makes	it	possible	for the	management	of	failure	to	work	with	the	same	constructs	 	semantics across	a	cluster	or	within	a	single	host	nonblocking	communication allows	recipients	to	only	consume	resources	while	active	 	to	less system	   the   communication style is event     the fourth is reactive which is   to be message   how are event     message     from each other  is event   necessarily   on publishersubscriber     message queuesbrokers  is message   or reactive necessarily   on   message passing eg web services rpc   thanks
t3_f7x60w,Pip Install D-Tale: Advanced Interactive Python Dashboard,,bigdata,pip install     interactive python   
t3_f7s191,"How can Big Data be used by amateurs to gather, measure and analyze environmental pollution data?",,bigdata,how can big   be   by amateurs to gather measure   analyze environmental pollution   
t3_f7q78u,Basic: Good data scraping tools for beginners?,"I’m a Marketing student. We’re doing a Big Data course in university and our course instructor inspired us to execute the project in reality. 

We want to collect data from social media platforms using data scraping tools. Any advice on what to use? And what to avoid?",bigdata,basic     scraping tools for beginners i’m a marketing   we’re   a big   course in university   our course instructor   us to execute the project in reality   we want to collect   from social   platforms using   scraping tools any   on what to use   what to  
t3_f7camr,Is SMACK the most popular data engineering tech stack?,"I have recently heard of SMACK (Spark, Meso, Akka, Cassandra, Kafka), tech stack in data engineering. 

Is SMACK the most popular tech stack right now and in the foreseeable future? Does a data engineer need or are better to be familiar with all the five components? 

Is there other competitive/promising alternative tech stack?

Thanks.",bigdata,is smack the most popular   engineering tech stack i have recently   of smack spark meso akka   kafka tech stack in   engineering   is smack the most popular tech stack right now   in the foreseeable future   a   engineer   or are better to be familiar with all the five components   is there other competitivepromising alternative tech stack  thanks
t3_f7hava,How to Handle Data Privacy Regulations,"[https://podcasts.apple.com/us/podcast/how-to-handle-massive-data-growth-data-privacy-regulations/id1456416350?i=1000466308739](https://podcasts.apple.com/us/podcast/how-to-handle-massive-data-growth-data-privacy-regulations/id1456416350?i=1000466308739)

&amp;#x200B;

A great listen on condensing files and how to prepare for further data regulations like GDPR and CCPA.",bigdata,how to     privacy regulations   x   b  a great listen on   files   how to prepare for further   regulations like     ccpa
t3_f7cyxp,Low code with easy sharing alternative to Jupyter Notebook,,bigdata,low   with easy sharing alternative to jupyter notebook 
t3_f7cmaj,How SK telecom democratizes streaming data with FLOW and Flink,,bigdata,how sk telecom   streaming   with flow   flink 
t3_f748bt,"What is Hadoop? Introduction, Architecture, Ecosystem, Components",,bigdata,what is     architecture ecosystem components 
t3_f6zw5p,"What does ""C"" in ACID mean for distributed transactions?","In a nondistributed database system, a transaction has ACID properties. ""C"" i.e. ""Consistency"" is said to be defined by applications (higher level), not by database systems (lower level), according to *Design Data Intensive Applications*:

&gt; In the context of ACID, consistency refers to an application-specific notion of the database being in a “good state.”

&gt; Atomicity,  isolation,  and  durability  are  properties  of  the  database,  whereas  consis‐
tency (in the ACID sense) is a property of the application. The application may rely
on the database’s atomicity and isolation properties in order to achieve consistency,
but it’s not up to the database alone. Thus, the letter C doesn’t really belong in ACID.


In a distributed database system, a distributed transaction also has ACID properties.
Is ""C"" also defined by applications, not by distributed database systems? 


Does consistency between replicas of the same data  belong to

-  ""C"" in ACID? (I guess no, because consistency between replicas is an issue at distributed data system level, not at application level)
-  ""A"" in ACID? (I guess yes, because, a transaction must be either none or all, not partially done. Inconsistency between replicas is partial work done.)
- something else in ACID?
- something else not in ACID?

[The following example](https://old.reddit.com/r/bigdata/comments/f6y9ea/why_do_we_need_to_keep_several_participant_data/fi7tody/) isn't consistency between replicas of the same data. Does it belong to ""C"" in ACID? (I guess yes, because the consistency in the example seems to be defined at application level, not database system level.)

&gt; Imagine you have a system containing 2 components, each with it's own datastore. One contains your users account data, the other contains billing related logic and data.
If account and billing data go out of sync, you might get into a situation where you are billing users with long deleted accounts because your billing service is not aware of the deletion status.
To solve this one approach would be to make sure both datastores commit account deletion information or none of them does, this could be achieved with a distributed transaction and prevent the mentioned issues.",bigdata,what   c in   mean for   transactions in a     system a transaction has   properties c ie consistency is   to be   by applications higher level not by   systems lower level   to     intensive applications   in the context of   consistency refers to an applicationspecific notion of the   being in a “  state”   atomicity  isolation        are  properties  of  the     whereas  consis‐ tency in the   sense is a property of the application the application may rely on the  ’s atomicity   isolation properties in   to achieve consistency but it’s not up to the   alone thus the letter c  ’t really belong in     in a     system a   transaction also has   properties is c also   by applications not by     systems      consistency between replicas of the same    belong to    c in   i guess no because consistency between replicas is an issue at     system level not at application level   a in   i guess yes because a transaction must be either none or all not partially   inconsistency between replicas is partial work    something else in    something else not in      isnt consistency between replicas of the same     it belong to c in   i guess yes because the consistency in the example seems to be   at application level not   system level   imagine you have a system containing   components each with its own   one contains your users account   the other contains billing   logic     if account   billing   go out of sync you might get into a situation where you are billing users with long   accounts because your billing service is not aware of the   status to solve this one approach   be to make sure both   commit account   information or none of them   this   be   with a   transaction   prevent the   issues
t3_f6yl9k,Most efficient way to export Hive to SQL for massive amounts of data?,"I am currently using reading Hive tables as Spark df's using PySpark and appending the df's to Microsoft SQL tables using a jdbc jar. The data is just a simple select \* on all the Hive tables. Basically, I am doing a one time historical load from all these Hive tables to SQL. It takes forever. I'm doing this for at least 2 dozen tables. A single table's load historical load can take anywhere from 12 hours to 30 hours. I can only have one connection with the user I am using for the SQL db meaning that I cannot write to multiple tables in parallel. For my Spark jobs, I am allocating around 700,000mb of processing power. Is there a better approach to loading all this data into these tables?",bigdata,most efficient way to export hive to sql for massive amounts of   i am currently using   hive tables as spark   using pyspark     the   to microsoft sql tables using a   jar the   is just a simple select  on all the hive tables basically i am   a one time historical   from all these hive tables to sql it takes forever im   this for at least     tables a single tables   historical   can take anywhere from    hours to    hours i can only have one connection with the user i am using for the sql   meaning that i cannot write to multiple tables in parallel for my spark jobs i am allocating         mb of processing power is there a better approach to   all this   into these tables
t3_f6sna0,List of Big Data Events - places where thousands of your peers gather to share exp and knowledge,,bigdata,list of big   events  places where   of your peers gather to share exp     
t3_f6znd5,MR3 Unleashes Hive on Kubernetes,,bigdata,mr  unleashes hive on kubernetes 
t3_f6y9ea,Why do we need to keep several participant data systems consistent with each other?,"*Design Data Intensive Applications* says


&gt; **Two quite different types of distributed transactions** are often conflated:
&gt; 
&gt; **Database-internal distributed transactions** Some distributed databases (i.e., databases that use replication and partitioning in
&gt; their standard configuration) support internal transactions among the
&gt; nodes of that database. For example, VoltDB and MySQL Cluster’s NDB
&gt; storage engine have such internal transaction support. In this case,
&gt; all the nodes participating in the transaction are running the same
&gt; database software.
&gt; 
&gt; **Heterogeneous distributed transactions** In a heterogeneous transaction, the participants are two or more different technologies: 
&gt; for  example,  two  databases  from  different  vendors,  or  even 
&gt; non-database systems such as message brokers. A distributed
&gt; transaction across these systems must ensure atomic commit, even
&gt; though the systems may be entirely different under the hood.

&gt; **X/Open  XA**  (short  for  eXtended  Architecture)  is  a  standard  for  implementing  two-phase commit across heterogeneous technologies

&gt; XA transactions solve the real and important problem of **keeping several participant data  systems  consistent  with  each  other**,  but  as  we  have  seen,  they  also  introduce major operational problems.

Why do we need to keep several participant data  systems  consistent  with  each  other, in either database-internal distributed transactions or heterogeneous distributed transactions?

Is it to keep the data in replica consistent with each other? Or is replication not involved in it? 

The quote above doesn't mention replication. Does it mean that the distributed system just partition the data onto different component systems? Does partition require keeping participant data systems consistent with each other?

Thanks.",bigdata,why   we   to keep several participant   systems consistent with each other     intensive applications says    two quite   types of   transactions are often          transactions some     ie   that use replication   partitioning in  their   configuration support internal transactions among the    of that   for example     mysql cluster’s    storage engine have such internal transaction support in this case  all the   participating in the transaction are running the same    software    heterogeneous   transactions in a heterogeneous transaction the participants are two or more   technologies   for  example  two     from        or  even     systems such as message brokers a    transaction across these systems must ensure atomic commit even  though the systems may be entirely     the     xopen  xa  short  for     architecture  is  a     for  implementing  twophase commit across heterogeneous technologies   xa transactions solve the real   important problem of keeping several participant    systems  consistent  with  each  other  but  as  we  have  seen  they  also    major operational problems  why   we   to keep several participant    systems  consistent  with  each  other in either     transactions or heterogeneous   transactions  is it to keep the   in replica consistent with each other or is replication not   in it   the quote above   mention replication   it mean that the   system just partition the   onto   component systems   partition require keeping participant   systems consistent with each other  thanks
t3_f6vofu,What Is Big Data? Why Big Data Is Important?,,bigdata,what is big   why big   is important 
t3_f6imxp,Cribl LogStream up to 55x faster than Apache NiFi,,bigdata,cribl logstream up to   x faster than apache nifi 
t3_f6qfyz,Composing Cohort Queries in BigQuery Using HyperLogLog++,,bigdata,composing cohort queries in bigquery using hyperloglog 
t3_f6983m,25 Best Online Python Courses to Learn,,bigdata,   best online python courses to learn 
t3_f6dtot,Building and analyzing updatable dataset of posts for a subreddit - A pubic dataset of posts about moral dilemmas from r/AmItheAsshole,,bigdata,    analyzing     of posts for a    a pubic   of posts about moral   from ramitheasshole 
t3_f66eco,How in-line address data quality delivers business ready data for AI initiatives,,bigdata,how inline     quality   business     for ai initiatives 
t3_f5xbvd,An interview with Snowplow Analytics tech lead about how they manage data infrastructure for streaming events across multiple clouds,,bigdata,an interview with snowplow analytics tech   about how they manage   infrastructure for streaming events across multiple   
t3_f5xcog,PrecisionFDA FDA Open Data Challenge Update,"For those of you interested in participating in the precisionFDA [Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data](https://go.usa.gov/xdQRC) Challenge, you have 10-days days left to complete your submission. Selected participants will be invited to participate in a panel at the [Modernizing FDA’s Data Strategy public meeting](https://www.fda.gov/news-events/fda-meetings-conferences-and-workshops/modernizing-fdas-data-strategy-03272020-03272020). Final submissions are due on February 28th, and providing code is no longer required for a valid entry. Additionally, new guidance is available on the challenge site including updated evaluation criteria and anomaly examples. 

If you are interested in learning more about how FDA is modernizing its data strategy, please attend the Modernizing FDA’s Data Strategy public meeting on March 27th. The meeting will also be streamed virtually for those who don’t live in the DC area!

If you have any questions about the challenge, please feel free to post them in this thread and we will respond as quickly as possible.",bigdata,    open   challenge   for those of you   in participating in the     challenge you have     left to complete your submission   participants will be   to participate in a panel at the   final submissions are   on february   th       is no longer   for a   entry   new   is available on the challenge site     evaluation criteria   anomaly examples   if you are   in learning more about how   is   its   strategy please   the    ’s   strategy public meeting on march   th the meeting will also be   virtually for those who  ’t live in the   area  if you have any questions about the challenge please feel free to post them in this     we will   as quickly as possible
t3_f607k4,Which message queues/brokers is Spark often used with?,"Is Apache Spark often used with some message queues/brokers?

Which of the following is used with Spark the most:

- Kafka
- RabbitMQ
- ActiveMQ
- something else?

Thanks.",bigdata,which message queuesbrokers is spark often   with is apache spark often   with some message queuesbrokers  which of the following is   with spark the most   kafka  rabbitmq  activemq  something else  thanks
t3_f5y1ia,Introducing the Kafka Scylla Connector,,bigdata,  the kafka scylla connector 
t3_f5wocp,How Machine Learning and Artificial Intelligence Will Impact Global Industries in 2020?,,bigdata,how machine learning   artificial intelligence will impact global   in      
t3_f5wmhk,Podcast: Data science coaching,,bigdata,    science coaching 
t3_f5n46u,IBM Data Science and Artificial Intelligence Programs Free for 30 days,,bigdata,ibm   science   artificial intelligence programs free for      
t3_f5pgwq,Hi All! These are my 10 favorite AI articles and news I red in January 2020. Best of AI February 2020,,bigdata,hi all these are my    favorite ai articles   news i   in january      best of ai february      
t3_f5qh85,How does a consumer receive a message from a message broker?,,bigdata,how   a consumer receive a message from a message broker 
t3_f5pfeh,Application Log Intelligence &amp; Performance Insight at Salesforce using Flink,,bigdata,application log intelligence  performance insight at salesforce using flink 
t3_f5bggx,Why Your Business Should Be Archiving Everything," [https://podcasts.apple.com/us/podcast/importance-archiving-all-electronic-conversations-within/id1456416350?i=1000465632963](https://podcasts.apple.com/us/podcast/importance-archiving-all-electronic-conversations-within/id1456416350?i=1000465632963) 

 We plan to discuss weekly, Big Tech and Big Data.",bigdata,why your business   be archiving everything      we plan to   weekly big tech   big  
t3_f5a3zi,How to produce .dtsx file?,"Hello, everybody. I have a task to produce a .dtsx file while using an open-source data set AdventureWorks when having no knowledge or experience with ETL and minimal knowledge of SQL. Please, help me with getting started.",bigdata,how to     file hello   i have a task to   a   file while using an opensource   set   when having no   or experience with etl   minimal   of sql please help me with getting  
t3_f543rl,7 Benefits to Using Big Data for Small Businesses,,bigdata,  benefits to using big   for small businesses 
t3_f54sos,Big Data and Hadoop Online Training | Big Data Hadoop Training | Hyderabad,"[Rainbow Training Institute](https://www.rainbowtraininginstitute.com) provides the best [Big Data and Hadoop online training.](https://www.rainbowtraininginstitute.com/big-data-and-hadoop/big-data-and-hadoop-training-in-hyderabad) Enroll for big data Hadoop training in Hyderabad certification, delivered by Certified Big Data Hadoop Experts. Here we are offering big data Hadoop training across global.",bigdata,big       online training  big     training        the best   enroll for big     training in   certification   by   big     experts here we are offering big     training across global
t3_f4h6xk,"what differences and similarities are between ""dataflow engines"" and ""stream processing systems""?","what differences and similarities are between ""dataflow engines""   and ""stream processing systems""?

In book *Data Intensive Applications*, Ch 10 *Batch Processing* says

&gt; **Batch processing systems (offline systems)**
&gt; 
&gt; A batch processing system takes a large amount of input data, runs a
&gt; job to process  it,  and  produces  some  output  data.  Jobs  often 
&gt; take  a  while  (from  a  few minutes to several days), so there
&gt; normally isn’t a user waiting for the job to finish. Instead, batch
&gt; jobs are often scheduled to run periodically (for example, once a
&gt; day). The primary performance measure of a batch job is usually
&gt; throughput (the time it takes to crunch through an input dataset of a
&gt; certain size). We discuss batch processing in this chapter.
&gt; 
&gt; **MapReduce**
&gt; 
&gt; A MapReduce job can only start when all tasks in the preceding jobs (that generate its inputs) have completed. ...
&gt; 
&gt; **Dataflow Engine**
&gt; 
&gt; In order to fix these problems with MapReduce, several new execution engines for distributed batch computations were developed, the most well known of which are **Spark** [61, 62], Tez [63, 64], and Flink [65, 66]. There are various differences in the way they are designed, but they have one thing in common: they handle an entire workflow as one job, rather than breaking it up into independent subjobs. Since they explicitly model the flow of data through several processing stages, these systems  are  known  as  **dataflow  engines**.  Like  MapReduce,  they  work  by  repeatedly calling a user-defined function to process one record at a time on a single thread.  They parallelize work by partitioning inputs, and they copy the output of one function over the network to become the input to another function. ... We call these functions  **operators**.

&gt;   MapReduce is like writing the output of each command to a temporary file, whereas dataflow engines look much more like
Unix pipes. Flink especially is built around the idea of **pipelined execution**: that is, incrementally passing the output of an operator to other operators, and not waiting  for the input to be complete before starting to process it.

Ch 11 *Stream Processing* is about 



&gt; **Stream processing systems (near-real-time systems)**
&gt; 
&gt; Stream processing is somewhere between online and offline/batch
&gt; processing (so it is sometimes called near-real-time or nearline
&gt; processing). Like a batch processing  system,  a  stream  processor 
&gt; consumes  inputs  and  produces  outputs (rather than responding to
&gt; requests). However, a stream job operates on events shortly after they
&gt; happen, whereas a batch job operates on a fixed set of input data.
&gt; This difference allows stream processing systems to have lower latency
&gt; than the equivalent batch systems. As stream processing builds upon
&gt; batch processing, we discuss it in Chapter 11.

&gt; Many open source distributed stream processing frameworks are designed with analytics in mind: for example, Apache Storm, **Spark Streaming**, Flink, Concord, Samza, and Kafka Streams [74]. Hosted services include Google Cloud Dataflow and Azure Stream Analytics.

&gt; **Spark performs stream processing** on top of a batch processing engine by breaking the stream into microbatches, whereas Apache Flink performs batch processing on top  of  a  stream  processing  engine.

I was wondering what differences and similarities are between ""dataflow engines"" (introduced in Ch 10 Batch Processing systems), and ""stream processing systems"" (introduced in Ch 11)?

- Do they both use pipeline to connect operators?

- Do they differ in whether the input data is bounded or unbounded?

- Do they have other differences than whether the input data is bounded or unbounded?


In Spark, according to Ch 11, 

- Spark Streaming is for streaming processing systems. 

- is everything else in Spark, e.g. structured APIs (DataSets, DataFrames, SQL) and low level APIs (RDDs, and Distributed Variables), for dataflow engines in batch processing systems? or for non-dataflow engines  in batch processing systems, similar to MapReduce?


Thanks.",bigdata,what     similarities are between   engines   stream processing systems what     similarities are between   engines     stream processing systems  in book   intensive applications ch    batch processing says   batch processing systems offline systems    a batch processing system takes a large amount of input   runs a  job to process  it        some  output     jobs  often   take  a  while  from  a  few minutes to several   so there  normally isn’t a user waiting for the job to finish   batch  jobs are often   to run   for example once a    the primary performance measure of a batch job is usually  throughput the time it takes to crunch through an input   of a  certain size we   batch processing in this chapter         a   job can only start when all tasks in the   jobs that generate its inputs have         engine    in   to fix these problems with   several new execution engines for   batch computations were   the most well known of which are spark   tez     flink   there are various   in the way they are   but they have one thing in common they   an entire workflow as one job rather than breaking it up into   subjobs since they explicitly   the flow of   through several processing stages these systems  are  known  as     engines  like     they  work  by    calling a   function to process one   at a time on a single    they parallelize work by partitioning inputs   they copy the output of one function over the network to become the input to another function  we call these functions  operators       is like writing the output of each   to a temporary file whereas   engines look much more like unix pipes flink especially is built   the   of   execution that is incrementally passing the output of an operator to other operators   not waiting  for the input to be complete before starting to process it  ch    stream processing is about      stream processing systems nearrealtime systems    stream processing is somewhere between online   offlinebatch  processing so it is sometimes   nearrealtime or nearline  processing like a batch processing  system  a  stream  processor   consumes  inputs        outputs rather than   to  requests however a stream job operates on events shortly after they  happen whereas a batch job operates on a   set of input    this   allows stream processing systems to have lower latency  than the equivalent batch systems as stream processing   upon  batch processing we   it in chapter      many open source   stream processing frameworks are   with analytics in   for example apache storm spark streaming flink   samza   kafka streams     services   google       azure stream analytics   spark performs stream processing on top of a batch processing engine by breaking the stream into microbatches whereas apache flink performs batch processing on top  of  a  stream  processing  engine  i was   what     similarities are between   engines   in ch    batch processing systems   stream processing systems   in ch        they both use pipeline to connect operators     they   in whether the input   is   or       they have other   than whether the input   is   or     in spark   to ch       spark streaming is for streaming processing systems    is everything else in spark eg   apis     sql   low level apis       variables for   engines in batch processing systems or for   engines  in batch processing systems similar to     thanks
t3_f4mnnw,"MLOps with a Feature Store, Data versioning with Apache Hudi",,bigdata,mlops with a feature store   versioning with apache   
t3_f4fbos,Apache Druid + Superset for real time analytics?,"Hello everyone,

After having several nightmares with external vendors our team is transforming our work around open-source solutions. 

We have some event data coming from Kafka streams which is processed and stored in Hadoop clusters.  The data contains timestamps, metrics and status information. Currently we use hive access to create dashboards with proprietary commercial solutions which doesn't provide the fast and real time access we need.

We are planning to use Apache Druid and Superset to produce real-time analytics views for our end user.

Being a noob in this domain I wanted to ask others if there optins other options that are better than what we are planning to deploy.

Apache Kafka + Hive+ Apache Druid + Superset

Update: Besides general visualization. We would like to have some map visualization.",bigdata,apache    superset for real time analytics hello everyone  after having several nightmares with external   our team is transforming our work   opensource solutions   we have some event   coming from kafka streams which is       in   clusters  the   contains timestamps metrics   status information currently we use hive access to create   with proprietary commercial solutions which     the fast   real time access we    we are planning to use apache     superset to   realtime analytics views for our   user  being a noob in this   i   to ask others if there optins other options that are better than what we are planning to    apache kafka  hive apache    superset      general visualization we   like to have some map visualization
t3_f4rwrv,Is taking input from stdin a way and the only way to implement a streaming system?,"Is it correct that a stream processing system is defined as a system which takes unlimited input?

I can write a program that takes input from its stdin, and then in bash I can run the program with redirecting unlimited input to its stdin. Is taking input from stdin a way and the only way to implement a streaming system?",bigdata,is taking input from   a way   the only way to implement a streaming system is it correct that a stream processing system is   as a system which takes   input  i can write a program that takes input from its     then in bash i can run the program with     input to its   is taking input from   a way   the only way to implement a streaming system
t3_f4a2lf,How are jobs chained together in MapReduce?,"*Data Intensive Applications* book says

&gt;  it is very common for MapReduce jobs to be chained together into workflows,
such that the output of one job becomes the input to the next job. The Hadoop Map‐
Reduce framework does not have any particular support for workflows, so this chaining is done implicitly by directory name: the first job must be configured to write its
output to a designated directory in HDFS, and the second job must be configured to
read that same directory name as its input. From the MapReduce framework’s point
of view, they are two independent jobs.

In Apache Hadoop's MapReduce, how can we specify that two jobs are chained together, so that the first job's output is the second job's input and the second job starts as soon as the first job finishes creating its output?

Thanks.",bigdata,how are jobs   together in     intensive applications book says    it is very common for   jobs to be   together into workflows such that the output of one job becomes the input to the next job the   map‐   framework   not have any particular support for workflows so this chaining is   implicitly by   name the first job must be   to write its output to a     in     the   job must be   to   that same   name as its input from the   framework’s point of view they are two   jobs  in apache     how can we specify that two jobs are   together so that the first jobs output is the   jobs input   the   job starts as soon as the first job finishes creating its output  thanks
t3_f481fr,Benchmarking Ozone: Cloudera’s next generation Storage for CDP,,bigdata,benchmarking ozone  ’s next generation storage for   
t3_f3ycet,Common big data tools that I get asked about on interviews. Which should I learn and possible to self teach?,"I get asked alot about the following tools on interviews.  

&amp;#x200B;

(1) AWS Redshift\*

(2) Spark

(3) Hadoop

(4) Microsoft Azure

&amp;#x200B;

\*\* I have used sql workbench to query redshift environments daily at work. but no nothing about how redshift works,etc

&amp;#x200B;

Questions

(1) What ones are worth learning and possible to self teach?

(2) which would be the quickest to self teach?

(3)  any recs on how to self teach?",bigdata,common big   tools that i get   about on interviews which   i learn   possible to self teach i get   alot about the following tools on interviews    x   b    aws      spark         microsoft azure  x   b   i have   sql workbench to query   environments   at work but no nothing about how   worksetc  x   b  questions    what ones are worth learning   possible to self teach    which   be the quickest to self teach     any recs on how to self teach
t3_f3odmp,Supercharge Your Big Data Career Through DASCA’s Framework,"Though organizations are aware of the benefits big data may impose, yet they struggle to embed the right and relevant big data practice within the organization.

DASCA’s big data frameworks are an approach for all organizations looking to consider all the capabilities resulting in the success of a big data practice. All in all, from providing a rigorous framework to offering the best DASCA- Data Science Body of Knowledge (DSBoK™) every organization must possess.

data science knowledge framework, DASCA big data frameworks, Big data analyst, Big data engineer, DASCA’s knowledge framework, data science professionals, Big data analytics professionals 

[https://www.techiexpert.com/supercharge-your-big-data-career-through-dascas-framework/](https://www.techiexpert.com/supercharge-your-big-data-career-through-dascas-framework/)",bigdata,supercharge your big   career through  ’s framework though organizations are aware of the benefits big   may impose yet they struggle to   the right   relevant big   practice within the organization   ’s big   frameworks are an approach for all organizations looking to   all the capabilities resulting in the success of a big   practice all in all from   a rigorous framework to offering the best     science   of    ™ every organization must possess    science   framework   big   frameworks big   analyst big   engineer  ’s   framework   science professionals big   analytics professionals   
t3_f3nv0m,A Pivotal Year for Data Scientists- 2020,"Data scientist remains to be one of the best jobs in 2020.

According to McKinsey, the US will be facing a shortfall of 250,000 data scientists by 2024. Though the jobs market is in constant change, a data scientist job still hits the top list. With many organizations looking for professionals adept in data science skills, a data science career is an ideal choice.

best data science certification, data science professionals,  data science careers, Data Science and Big Data, Data science Industry, big data professionals 

[https://www.datasciencecentral.com/profiles/blogs/a-pivotal-year-for-data-scientists-2020](https://www.datasciencecentral.com/profiles/blogs/a-pivotal-year-for-data-scientists-2020)",bigdata,a pivotal year for   scientists        scientist remains to be one of the best jobs in         to mckinsey the us will be facing a shortfall of          scientists by      though the jobs market is in constant change a   scientist job still hits the top list with many organizations looking for professionals   in   science skills a   science career is an   choice  best   science certification   science professionals    science careers   science   big     science   big   professionals   
t3_f3nv3d,AI and Big data – What The Future Holds,,bigdata,ai   big   – what the future   
t3_f3f7kx,Announcing PrestoCon 2020: Advancing the Big Data Ecosystem with Presto,,bigdata,announcing prestocon        the big   ecosystem with presto 
t3_f39itt,"Is Spark widely used in ""Windows-stack"" companies?","Spark (Scala, Java, Python via PySpark, R via SparkR) is cross platform, thanks to JVM. 

I know some companies develop  their application software  under Linux.  It seems to me that Spark is their natural choice for data analytics.

I heard that many companies develop application software under Windows OS, or even using Windows technology stack (.NET, which has been recently made cross platform but seems not popular in Linux).  I like to know how widely   Spark is used in   ""Windows-user"" companies, and job opportunities of Spark in   ""Windows-user"" companies.

- What data analytics and machine learning programming platform do those ""Windows-user"" companies use for their data analytics and machine learning tasks?
-  Is Spark (Scala, Java, Python via PySpark, R via SparkR) popular in these companies?
-  Is something else more or equally popular than Spark in these companies? 
- If they use Spark, do they use Scala, Python or R version of Spark? 
- If they use Spark, do they use Spark in Windows natively, or via Windows's Linux subsystem or Linux virtual machine or cygwin?

Thanks.",bigdata,is spark     in   companies spark scala java python via pyspark r via sparkr is cross platform thanks to jvm   i know some companies    their application software    linux  it seems to me that spark is their natural choice for   analytics  i   that many companies   application software     os or even using   technology stack net which has been recently   cross platform but seems not popular in linux  i like to know how     spark is   in     companies   job opportunities of spark in     companies   what   analytics   machine learning programming platform   those   companies use for their   analytics   machine learning tasks   is spark scala java python via pyspark r via sparkr popular in these companies   is something else more or equally popular than spark in these companies   if they use spark   they use scala python or r version of spark   if they use spark   they use spark in   natively or via   linux subsystem or linux virtual machine or cygwin  thanks
t3_f3oove,Seeking Big Bucks? Have a Look at These Unheard IT Jobs in 2020,"Get yourself technology secure by the end of 2020.

Your career’s vulnerability solely depends on how technology adept you are in modern times. With ever more advancement of technology in our lives, our career is always at stake. Yet, they are exposing us to a new era of newer technologies to keep us future-ready.

The new decade is going to experience a whole new era of advanced workforce with job skills that were once not renowned. Thanks to the adoption of technologies like artificial intelligence (AI), wearable tech, Internet of Things (IoT), virtual reality, machine-to-machine (M2M), 5G, and augmented reality.

data science career, certified data science professionals, data scientists and big data engineers, DevOps Engineer, big data professionals

[https://recruitingblogs.com/profiles/blogs/seeking-big-bucks-have-a-look-at-these-unheard-it-jobs-in-2020](https://recruitingblogs.com/profiles/blogs/seeking-big-bucks-have-a-look-at-these-unheard-it-jobs-in-2020)",bigdata,seeking big bucks have a look at these   it jobs in      get yourself technology secure by the   of       your career’s vulnerability solely   on how technology   you are in   times with ever more   of technology in our lives our career is always at stake yet they are exposing us to a new era of newer technologies to keep us    the new   is going to experience a whole new era of   workforce with job skills that were once not   thanks to the   of technologies like artificial intelligence ai wearable tech internet of things iot virtual reality machinetomachine m m  g     reality    science career     science professionals   scientists   big   engineers   engineer big   professionals  
t3_f3hjpz,Discussing Datafloq With Mark van Rijmenam,,bigdata,    with mark van rijmenam 
t3_f38eml,How to connect to an ODBC dataset through mySQL?,"I am using macOS operating system and i have been trying to connect this odbc data source through mySQL for several hours now, but have had no success. I have seen all tutorials online, but they aren't much use as most of them are outdated or based on windows. 

I have the following information-

Server details, Port, Database name, User id, Password

Could anyone please guide through the procedure from the start, i have my sql workbench and iodbc administrator.

Thanks alot!",bigdata,how to connect to an     through mysql i am using macos operating system   i have been trying to connect this     source through mysql for several hours now but have   no success i have seen all tutorials online but they arent much use as most of them are   or   on     i have the following information  server   port   name user        anyone please   through the   from the start i have my sql workbench        thanks alot
t3_f2yins,"Course suggestion for big data (Spark, drill, presto) to get job as a DA","Looking for an online course (Udemy/Youtube) for Big data...basically want to start learning them.

Please suggest should I learn spark I am looking to switch career as a data scientist

There are tons of them.",bigdata,course suggestion for big   spark   presto to get job as a   looking for an online course   for big   want to start learning them  please suggest   i learn spark i am looking to switch career as a   scientist  there are tons of them
t3_f36e3p,Creative ways of collecting retail data,"Hi! I'm doing some work for a company that sells their products in retail stores. However, as far as I know, their retail partners will not provide the POS data for their products. 

I'm sure we can track the wholesale orders, but was wondering if any of you have creative ways of collecting other data?

👉 For example, I might not know the mileage of a car. But if I can know when they change their tires, and the tire rating, I might be able to extrapolate that mileage.

Thanks!",bigdata,creative ways of collecting retail   hi im   some work for a company that sells their   in retail stores however as far as i know their retail partners will not   the pos   for their     im sure we can track the wholesale   but was   if any of you have creative ways of collecting other    👉 for example i might not know the mileage of a car but if i can know when they change their tires   the tire rating i might be able to extrapolate that mileage  thanks
t3_f308b0,Big Data Architecture Survey,"I'm going to write a survey about Big Data architectures, but I don't know if that would be useful or how difficult it would be. I mean, is there a science behind architectural choices?

In traditional software development we have a lot of books, papers and protocols on software architecture, but it isn't true on Big Data architectures. I see a lot of articles and posts about the Lambda and Kappa architectures, but how exactly do big companies choose their Big Data architectures?

Where can I find references and articles about that? Where do you find that?

Thanks.",bigdata,big   architecture survey im going to write a survey about big   architectures but i   know if that   be useful or how   it   be i mean is there a science   architectural choices  in   software   we have a lot of books papers   protocols on software architecture but it isnt true on big   architectures i see a lot of articles   posts about the     kappa architectures but how exactly   big companies choose their big   architectures  where can i   references   articles about that where   you   that  thanks
t3_f31h35,Unable to build zookeeper source code from github,"Error:java: Annotation processing is not supported for module cycles. Please ensure that all modules from cycle [zookeeper-recipes-queue,zookeeper-recipes-lock,zookeeper-recipes-election,zookeeper-prometheus-metrics,zookeeper] are excluded from annotation processing",bigdata,unable to   zookeeper source   from github errorjava annotation processing is not   for   cycles please ensure that all   from cycle   are   from annotation processing
t3_f2v7rj,Free tools and resources for data science and blockchain,,bigdata,free tools   resources for   science   blockchain 
t3_f2gfcf,"Data mining could provide the basis for a kind of universal basic income ""“Everybody's generating data, and everybody can get compensated for it.""",,bigdata,  mining     the basis for a   of universal basic income “  generating       can get   for it 
t3_f2qxzq,2020 | The Knowledge Graph Conference - early bird pricing ends Friday Feb 15th,,bigdata,      the   graph conference  early   pricing     feb   th 
t3_f2lccr,How is DevOps different from Agile? DevOps Vs Agile,,bigdata,how is     from agile   vs agile 
t3_f2els3,Data Retention and Deletion in Apache Druid,"If you're using Apache Druid, at some point you have encountered or will encounter the available options for setting a TTL on the data you store.

Some of this stuff can be confusing, so we published this blog post to help you find your way around the available options, as well as **a rather unique, advanced option of setting a dimension-based TTL**:

[Data Retention and Deletion in Apache Druid](https://medium.com/nmc-techblog/data-retention-and-deletion-in-apache-druid-74ffd12398a8)",bigdata,  retention     in apache   if youre using apache   at some point you have   or will encounter the available options for setting a ttl on the   you store  some of this stuff can be confusing so we   this blog post to help you   your way   the available options as well as a rather unique   option of setting a   ttl   
t3_f1sx6j,An interview about the data vault method of data modeling and how it simplifies integrating the evolving data sources that you are dealing with in your enterprise data warehouse,,bigdata,an interview about the   vault   of       how it simplifies integrating the evolving   sources that you are   with in your enterprise   warehouse 
t3_f1tzen,"Storm, Flink, etc. for single node stream processing in c++ setting.","Hi,

I'm trying to learn about the many (stream) data processing engines / frameworks out there, to see if they can help me with my latest project.

Basically, I'm working on a Ubuntu 16 machine that receives data from an ADC via PCIe in chunks of 2\^15 samples at 10Hz. I need to write an application that gets the data, processes it and passes it on, online (i.e. stream processing). Now, to get the data and to send it on, the application has to rely on c++ libraries from my working environment.

How would I integrate these boundary conditions with a stream processing framework like Flink that only has Java (JVM) and Python APIs? Or am I going about this problem entirely wrong? I was hoping that a stream processing engine like flink would take most of the tedious work away and make it rather easy to prototype new processing algorithms, as these are expected to change frequently during development.

I'd appreciate any guidance on how to go about this..

\- Simon

&amp;#x200B;

&amp;#x200B;

Side question: How come all these frameworks / engines belong to Apache?",bigdata,storm flink etc for single   stream processing in c setting hi  im trying to learn about the many stream   processing engines  frameworks out there to see if they can help me with my latest project  basically im working on a ubuntu    machine that receives   from an   via pcie in chunks of     samples at   hz i   to write an application that gets the   processes it   passes it on online ie stream processing now to get the     to   it on the application has to rely on c libraries from my working environment  how   i integrate these     with a stream processing framework like flink that only has java jvm   python apis or am i going about this problem entirely wrong i was hoping that a stream processing engine like flink   take most of the   work away   make it rather easy to prototype new processing algorithms as these are   to change frequently        appreciate any   on how to go about this   simon  x   b  x   b    question how come all these frameworks  engines belong to apache
t3_f1ofj8,Apache Flink: A Guide for Unit Testing in Apache Flink,,bigdata,apache flink a   for unit testing in apache flink 
t3_f1op2v,Anyone using Debezium in production?,"Hi guys.

We are currently testing Debezium (closed, controlled environment), trying to figure it out, if it could be a good replacement/alternative for Oracle Golden Gate.

Our confidence level is still low, since we are in a early stage of testing.

I'm very curious if someone made the transation from OGG to another CDC (change data capture) tool.

Anyone with experience with Debezium or another Open Source CDC tool?",bigdata,anyone using   in   hi guys  we are currently testing       environment trying to figure it out if it   be a   replacementalternative for oracle   gate  our   level is still low since we are in a early stage of testing  im very curious if someone   the transation from ogg to another   change   capture tool  anyone with experience with   or another open source   tool
t3_f1o83o,Social media analytics: What's the buzz?,,bigdata,social   analytics whats the buzz 
t3_f1fr5m,Data and analytics in 2020,,bigdata,    analytics in      
t3_f1mj19,NoSQL vs SQL — Which Database Type is Better For Big Data Applications,,bigdata,nosql vs sql — which   type is better for big   applications 
t3_f186e0,Scaling Apache Airflow with the Kubernetes Executors,,bigdata,scaling apache airflow with the kubernetes executors 
t3_f0sglp,"Apache Airflow: Variables, Macros and Templating",,bigdata,apache airflow variables macros   templating 
t3_f0sfyl,Apache Airflow: Password Authentication and Filtering by Owners,,bigdata,apache airflow   authentication   filtering by owners 
t3_f0c5o5,What topic would be great for school big data project?,"i am newbie here and a korean (not north lol) highschool student and looking for a fresh topic as my big data project

the other night, i watched and inspired from a documentary dealing with big data, which was extracting meaningful pattern or correlation from tiny data, which i thought useless and insignificant.

so, as a student, i wanted to do a some big data analysis with data generated on my school

The thing i came up was gathering purchase information made at school canteen to extract meaningful pattern on that. (or finding out a correlation between weather and hot selling item for that day, or such like these things)

please recommend or give some idea for my project

i wish i come up some groundbreaking and exciting idea, but i couldn't..

thanks for reading it and sorry for my bad grammar",bigdata,what topic   be great for school big   project i am newbie here   a korean not north lol highschool     looking for a fresh topic as my big   project  the other night i       from a     with big   which was extracting meaningful pattern or correlation from tiny   which i thought useless   insignificant  so as a   i   to   a some big   analysis with     on my school  the thing i came up was gathering purchase information   at school canteen to extract meaningful pattern on that or   out a correlation between weather   hot selling item for that   or such like these things  please   or give some   for my project  i wish i come up some     exciting   but i    thanks for   it   sorry for my   grammar
t3_ezuhzp,"Hadoop: Decade Two, Day Zero*",,bigdata,    two   zero 
t3_f06b93,11 Best Big Data Analytics Tools in 2020,,bigdata,   best big   analytics tools in      
t3_ezsps7,Ververica Platform &amp; FinTech Studios: using real time data and Apache Flink to inform the finance industry of tomorrow,,bigdata,ververica platform  fintech   using real time     apache flink to inform the finance   of tomorrow 
t3_ezwa2a,How does big data support the hospitality and travel industry?," 

Big data collects the knowledge and incorporates it in machine  learning programs, statistical analysis, and other high-level analytics  applications as a mixture of standardized, unstructured, and  semi-structured data that is gathered by different organizations. Big  data has done miracles throughout the globe in various activities, and  the travel industry is no different.

The travel business has influenced by vast knowledge and additionally  used for daily executions. The travel industry is one amongst the  sectors that are heavily addicted to the collected information and,  because of immense expertise and analytics, has become a lot of  productive in recent years. Big Data does everything a tourist needs  from the travel company, offering tailored deals and recommendations on  sites to visit. It can, therefore, be considered the perfect partner  traveler requirements. It’s going to be back to say the big data  stress-free the flying experience, which is something every traveler  wants.

read more:[https://planetstoryline.com/how-does-big-data-support-the-hospitality-and-travel-industry/](https://planetstoryline.com/how-does-big-data-support-the-hospitality-and-travel-industry/)",bigdata,how   big   support the hospitality   travel      big   collects the     incorporates it in machine  learning programs statistical analysis   other highlevel analytics  applications as a mixture of            that is   by   organizations big    has   miracles throughout the globe in various activities    the travel   is no    the travel business has   by vast          for   executions the travel   is one amongst the  sectors that are heavily   to the   information    because of immense expertise   analytics has become a lot of    in recent years big     everything a tourist    from the travel company offering         on  sites to visit it can therefore be   the perfect partner  traveler requirements it’s going to be back to say the big    stressfree the flying experience which is something every traveler  wants    more
t3_ezoq5w,Skills Required For Big Data &amp; Hadoop Jobs - Join the Live Session,,bigdata,skills   for big      jobs  join the live session 
t3_ezo9vy,How to start about big data analytics? what correlational analysis methods are there?,"Even though there are many criticisms over the heavy use and over reliance on correlational analysis, I just am not even sure what methods they are talking about tho.

&amp;#x200B;

I mean, a ""classic"" method of find a correlation would be to use the covariance, but there are certain assumptions about the data that need to meet before you can apply covariance on those two random variables.

&amp;#x200B;

What if the some data are nonstationary? while others are? what if all the data are of different dimensions? so many questions regarding what possible method do big data analysts use.",bigdata,how to start about big   analytics what correlational analysis   are there even though there are many criticisms over the heavy use   over reliance on correlational analysis i just am not even sure what   they are talking about tho  x   b  i mean a classic   of   a correlation   be to use the covariance but there are certain assumptions about the   that   to meet before you can apply covariance on those two   variables  x   b  what if the some   are nonstationary while others are what if all the   are of     so many questions   what possible     big   analysts use
t3_ezf6fp,I'm thinking about getting my Master's in Big Data Analytics. Is it worth it?,"I received my BS in Business Administration last year and I have been exploring multiple options for my Master's Degree. 

My top 3 choices are Business Administration: information systems, Mathematics, or Big Data Analytics.

I currently have an Analyst position and I am looking to continue down that career path.

Any insight would be a huge help.",bigdata,im thinking about getting my masters in big   analytics is it worth it i   my bs in business   last year   i have been exploring multiple options for my masters     my top   choices are business   information systems mathematics or big   analytics  i currently have an analyst position   i am looking to continue   that career path  any insight   be a huge help
t3_ez7k1l,Impact of Data Visualization in Healthcare I Importance and tools,,bigdata,impact of   visualization in healthcare i importance   tools 
t3_ezaj17,Latest Data Storage Trends for 2019,,bigdata,latest   storage   for      
t3_ezaisv,"Spotfire vs. Tableau: Analytics, Support &amp; Pricing Compared",,bigdata,spotfire vs tableau analytics support  pricing   
t3_ez842w,What are the Best Practices for Data Lineage?,,bigdata,what are the best practices for   lineage 
t3_ez6rxq,"Tableau vs. Splunk: Ranking, Function, Pricing &amp; More Compared",,bigdata,tableau vs splunk ranking function pricing  more   
t3_ez6no1,5 Best Practices for Cloud Data Integration along with Challenges &amp; Benefits,,bigdata,  best practices for     integration along with challenges  benefits 
t3_ez5did,QlikView vs. Tableau,,bigdata,qlikview vs tableau 
t3_ez73ro,How can Business Benefit from Real-Time Data Analytics? | YourTechDiet,,bigdata,how can business benefit from realtime   analytics    
t3_eyrlia,An interview about the BrightHive platform for building data trusts and the complexities that are inherent in sharing data across organizations,,bigdata,an interview about the brighthive platform for     trusts   the complexities that are inherent in sharing   across organizations 
t3_eynett,Toyota Uses Big Data to Evade Accelerator-Break Confusion,,bigdata,toyota uses big   to   acceleratorbreak confusion 
t3_eyoyz5,Can I please get a course / training / master recommendation for slingshotting to a BD Architect career?,"We are in a kind of a special situation at our company. We want to train one of our employees to be a BD Arquitect. Although I am myself educated in IT, my ignorance in this field is huge, and I am specialized in other things.

Is there any training that we could get for this employee, and get him up to speed in \~6 months? Or on the contrary, do you think this would be a waste of time and money? We are not looking for certifications, but a good training program. Money is not an issue here, so if premium is best, we will go premium.",bigdata,can i please get a course  training  master   for slingshotting to a   architect career we are in a   of a special situation at our company we want to train one of our employees to be a   arquitect although i am myself   in it my ignorance in this   is huge   i am   in other things  is there any training that we   get for this employee   get him up to   in   months or on the contrary   you think this   be a waste of time   money we are not looking for certifications but a   training program money is not an issue here so if premium is best we will go premium
t3_eycuyz,Problems with mismatched schemas in AWS Glue ETL Job," I have a set of CSV files that I am collating and moving into RDS. The schema for the files has evolved over the years and there are differences between the oldest file and the most recent. I am able to generate an accurate schema that reflects the current schema through a Glue Crawler, but my ETL Glue Job isn't accomplishing what I need it to.

I've included a mock up of what I'm trying to accomplish and what is happening. The autogenerated ETL job that Glue provided isn't enough for what I need, but I have very little experience with this type of work and am not sure how to change it.

Any help is appreciated

https://preview.redd.it/b3hzvjcglre41.png?width=382&amp;format=png&amp;auto=webp&amp;s=b847c07d70c872efd92af1b7dc0067a604860c26",bigdata,problems with   schemas in aws glue etl job  i have a set of csv files that i am collating   moving into   the schema for the files has   over the years   there are   between the   file   the most recent i am able to generate an accurate schema that reflects the current schema through a glue crawler but my etl glue job isnt accomplishing what i   it to  ive   a mock up of what im trying to accomplish   what is happening the   etl job that glue   isnt enough for what i   but i have very little experience with this type of work   am not sure how to change it  any help is    
t3_ey2eh4,IBM Data Science and AI Programs on Coursera Free for 30 Days,,bigdata,ibm   science   ai programs on coursera free for      
t3_exovq6,Cloudera Certifications: The Eternal Suffering,"On the 30th of January, 2020 Cloudera announced that it has no launch date yet for its platform of Cloudera certification exams. Astonishingly, a company based on trust, reliability, security and that's supposed to handle even much more complex situations is not able to deliver something so simple and so cheap to fulfill as an online exam. In the worst case that their whole infrastructure and platform were burning, they had plenty of options to let their customers take exams. I don't want to dive in a cloud operator fight right here, but since I've used AWS I'll use EMR as an example: with few clicks and paying as low as a couple of dollars they could have set up a wonderful, reliable, scalable and fully customizable infrastructure. Given the fact that each of the exams cost a few hundred dollars, investing one or two bucks on the platform could have been a nice idea. This would have also solved another big issue: many who took their exams complained about their faulty, unreliable exam infrastructure, some of them have also requested and obtained a refund because of this. I'm trying to imagine what could have kept them away from such an easy solution: were they worried about losing credibility for using a cloud-locked solution for their certification platform while trying to sell software for on-premise datacenters? Certainly not, since their exam platform already lost all credibility when last October decided to delay all exams by 4 months with inadequate notice, announcing that with a tweet. Who do they think they are to announce that decision on Twitter, without even sending an email? Donald Trump? Another fact that let a lot of customers down was the lack of transparency on some of the exam content. Let's talk about CCA175 for example: while once it used to contain questions on realtime computation (Spark Streaming, Flume, ...) now the range of the content has narrowed down to 2 or 3 topics but raising the total number of questions. The exam became a race against the time to make the same, repetitive task, that is very far from the everyday enterprise use of Spark. There's no way for a company or any customer to address this because talking about the exam content is strictly prohibited and Cloudera may in its sole discretion ban anyone who disagrees with them, revoking all previous certifications. And this is just the tip of the iceberg. When Cloudera merged with Hortonworks something was clear: whatever it came out, it would have been the only option for on-premise Big Data. This is crucial for companies who work in fields where privacy or security are majors concerns, like healthcare or the aviation industry. Cloudera's sudden shift to cloud-based solutions was already raising eyebrows to the on-premise world, the new pricing is so messy and complicated that often not even their sales agents can make prices to customers. Their decision to discontinue the release of binaries of open-source products is the icing on the cake. If this is the only alternative we have to centralized cloud services... well, we're fucked.",bigdata,  certifications the eternal suffering on the   th of january          that it has no launch   yet for its platform of   certification exams astonishingly a company   on trust reliability security   thats   to   even much more complex situations is not able to   something so simple   so cheap to fulfill as an online exam in the worst case that their whole infrastructure   platform were burning they   plenty of options to let their customers take exams i   want to   in a   operator fight right here but since ive   aws ill use emr as an example with few clicks   paying as low as a couple of   they   have set up a   reliable scalable   fully customizable infrastructure given the fact that each of the exams cost a few     investing one or two bucks on the platform   have been a nice   this   have also   another big issue many who took their exams   about their faulty unreliable exam infrastructure some of them have also       a   because of this im trying to imagine what   have kept them away from such an easy solution were they   about losing   for using a   solution for their certification platform while trying to sell software for onpremise   certainly not since their exam platform   lost all   when last october   to   all exams by   months with   notice announcing that with a tweet who   they think they are to announce that   on twitter without even   an email   trump another fact that let a lot of customers   was the lack of transparency on some of the exam content lets talk about cca    for example while once it   to contain questions on realtime computation spark streaming flume  now the range of the content has     to   or   topics but raising the total number of questions the exam became a race against the time to make the same repetitive task that is very far from the   enterprise use of spark theres no way for a company or any customer to   this because talking about the exam content is strictly       may in its sole   ban anyone who   with them revoking all previous certifications   this is just the tip of the iceberg when     with hortonworks something was clear whatever it came out it   have been the only option for onpremise big   this is crucial for companies who work in   where privacy or security are majors concerns like healthcare or the aviation       shift to   solutions was   raising eyebrows to the onpremise   the new pricing is so messy     that often not even their sales agents can make prices to customers their   to   the release of binaries of opensource   is the icing on the cake if this is the only alternative we have to     services well were  
t3_expfpg,"FREE data science, AI and ML webinar!",,bigdata,free   science ai   ml webinar 
t3_ewt8hf,Independent Performance Benchmark: Apache Druid versus Presto and Apache Hive,[https://imply.io/post/performance-benchmark-druid-presto-hive](https://imply.io/post/performance-benchmark-druid-presto-hive),bigdata,  performance benchmark apache   versus presto   apache hive 
t3_ewuzj9,Replacing HDFS for Spark Cluster,"I have a few clusters running Cloudera CDH on prem. Cloud is unfortunately not an option. For “reasons” we will have to replace Hadoop with something else. Initial tests with Spark master/workers and glusterfs looks promising. Operations is simpler than CDH and performance looks good for our workloads. 

The problem is authentication/authorization. HDFS/Yarn with Kerberos was a major pain to set up but it makes integration with  MS AD straightforward. Spark jobs submitted to Yarn get the right permissions, file ownership just works. Not so with Spark standalone. Jobs will run as the Spark user. Gluster uses normal Unix file permissions and POSIX acls. 

I haven’t looked into Spark on k8s or Mesos. Is the auth story better there? K8s also smells high maintenance to me.",bigdata,replacing   for spark cluster i have a few clusters running     on prem   is unfortunately not an option for “reasons” we will have to replace   with something else initial tests with spark masterworkers   glusterfs looks promising operations is simpler than     performance looks   for our     the problem is authenticationauthorization   with kerberos was a major pain to set up but it makes integration with  ms     spark jobs   to yarn get the right permissions file ownership just works not so with spark   jobs will run as the spark user gluster uses normal unix file permissions   posix acls   i haven’t   into spark on k s or mesos is the auth story better there k s also smells high maintenance to me
t3_ewot6m,State Unlocked: Ways of Interacting with State in Apache Flink,,bigdata,state   ways of interacting with state in apache flink 
t3_ewqmv4,5 Best Practices for Cloud Data Integration along with Challenges &amp; Benefits,,bigdata,  best practices for     integration along with challenges  benefits 
t3_ewqlya,The Avast scandal shows us that users need to capture the value of their data themselves,,bigdata,the avast   shows us that users   to capture the value of their   themselves 
t3_ewnmz4,How can Business Benefit from Real-Time Data Analytics? | YourTechDiet,,bigdata,how can business benefit from realtime   analytics    
t3_ewnqow,Reinvent your business model using AI in 2020,,bigdata,reinvent your business   using ai in      
t3_ewjcds,These Skills Will Make You a Hotshot Big Data Analyst,,bigdata,these skills will make you a hotshot big   analyst 
t3_ewl3kq,How is DevOps useful for Big Data?,,bigdata,how is   useful for big   
t3_ewl25j,What are the 7 V's of Big Data?,,bigdata,what are the   vs of big   
t3_ew72r7,difference between data lake and data warehouse and data mart,,bigdata,  between   lake     warehouse     mart 
t3_ew7cim,8 Use Cases of Data Mining by Industry,,bigdata,  use cases of   mining by   
t3_evlkap,Hey r/bigdata. I downloaded a Reddit dataset and built a recommendation algorithm for subreddits. Here is what it recommends for users of r/bigdata:,"Come over to r/RedditRecommender and get personal or subreddit-based recommendations.

Enjoy these recommendations for r/bigdata readers!

r/bigdata : no. 1 score: 618.6216473791695

r/dataengineering : no. 2 score: 74.4637168141593

r/devops : no. 3 score: 70.07912051303406

r/googlecloud : no. 4 score: 68.73573859768551

r/aws : no. 5 score: 61.544763019309535

r/datascience : no. 6 score: 61.007450372518626

r/scala : no. 7 score: 42.01423043315442

r/Database : no. 8 score: 40.33748801534036

r/datasets : no. 9 score: 33.940762936498786

r/MachineLearning : no. 10 score: 33.108007082431634

r/analytics : no. 11 score: 31.51067282486581

r/kubernetes : no. 12 score: 30.253116011505274

r/PostgreSQL : no. 13 score: 28.63989108236896

r/BusinessIntelligence : no. 14 score: 26.088466308391897

r/LanguageTechnology : no. 15 score: 25.210930009587724

r/Python : no. 16 score: 24.782945301542778

r/statistics : no. 17 score: 24.52769217956192

r/java : no. 18 score: 24.52769217956192

r/learnpython : no. 19 score: 22.95645056781366

r/linuxadmin : no. 20 score: 22.25835464244776

r/AWSCertifications : no. 21 score: 21.918207866631935

r/SQL : no. 22 score: 21.02373615391022

r/learnmachinelearning : no. 23 score: 19.796259263616044

r/compsci : no. 24 score: 19.408888205443642

r/docker : no. 25 score: 19.109738372093023

r/MLQuestions : no. 26 score: 18.677913429522754

r/golang : no. 27 score: 17.519780128258514

r/apachekafka : no. 28 score: 17.183934649421378

r/Rlanguage : no. 29 score: 16.438655899973952

r/ProgrammingLanguages : no. 30 score: 16.438655899973952

r/SideProject : no. 31 score: 16.27228775865403

r/rust : no. 32 score: 16.174073504536366

r/node : no. 33 score: 15.386190754827384

r/linuxquestions : no. 34 score: 15.143799868018478

r/deeplearning : no. 35 score: 15.126558005752637

r/PowerBI : no. 36 score: 14.546041258499482

r/rstats : no. 37 score: 14.546041258499482

r/SQLServer : no. 38 score: 14.008435072142065

r/startup : no. 39 score: 14.008435072142065

r/InsightfulQuestions : no. 40 score: 14.008435072142065

r/javascript : no. 41 score: 13.522901376761132

r/vuejs : no. 42 score: 13.509151236219628

r/computervision : no. 43 score: 13.509151236219628

r/startups : no. 44 score: 13.388371141519295

r/AskComputerScience : no. 45 score: 11.823512880562062

r/foreignpolicy : no. 46 score: 11.455956432947584

r/M1Finance : no. 47 score: 11.455956432947584

r/Electroneum : no. 48 score: 11.455956432947584

r/Terraform : no. 49 score: 11.455956432947584

r/AWS_Certified_Experts : no. 50 score: 11.455956432947584

r/DataScienceJobs : no. 51 score: 11.455956432947584

r/coding : no. 52 score: 11.12917732122388

r/baduk : no. 53 score: 10.959103933315967

r/ansible : no. 54 score: 10.959103933315967

r/HoloLens : no. 55 score: 10.959103933315967

r/softwaredevelopment : no. 56 score: 10.959103933315967

r/cs50 : no. 57 score: 10.959103933315967

r/GraphicsProgramming : no. 58 score: 10.959103933315967

r/cogsci : no. 59 score: 10.959103933315967

r/openwrt : no. 60 score: 10.959103933315967

r/diysound : no. 61 score: 10.959103933315967

r/programming : no. 62 score: 10.652937204591492

r/vim : no. 63 score: 10.51186807695511

r/AskProgramming : no. 64 score: 10.51186807695511

r/crypto : no. 65 score: 10.503557608288604

r/desktops : no. 66 score: 10.503557608288604

r/canadacordcutters : no. 67 score: 10.503557608288604

r/htpc : no. 68 score: 10.503557608288604

r/macbookpro : no. 69 score: 10.228200972447327

r/SelfDrivingCars : no. 70 score: 10.228200972447327

r/mysql : no. 71 score: 10.08437200383509

r/chch : no. 72 score: 10.08437200383509

r/expats : no. 73 score: 10.08437200383509

r/indianews : no. 74 score: 9.704444102721821

r/raisingkids : no. 75 score: 9.697360838999654

r/visualization : no. 76 score: 9.697360838999654

r/HomeNetworking : no. 77 score: 9.563122243942354

r/cscareerquestions : no. 78 score: 9.413484486873509

r/commandline : no. 79 score: 9.338956714761377

r/berlinsocialclub : no. 80 score: 9.338956714761377

r/typescript : no. 81 score: 9.338956714761377

r/ruby : no. 82 score: 9.338956714761377

r/linux : no. 83 score: 9.241346488257582

r/trackers : no. 84 score: 9.231714452896432

r/Chattanooga : no. 85 score: 9.006100824146419

r/redhat : no. 86 score: 9.006100824146419

r/apachespark : no. 87 score: 9.0

r/webdev : no. 88 score: 8.956825342757385

r/askTO : no. 89 score: 8.80290138094574

r/chrome : no. 90 score: 8.80290138094574

r/marriedredpill : no. 91 score: 8.696155436130631

r/Motorrad : no. 92 score: 8.696155436130631

r/violinist : no. 93 score: 8.696155436130631

r/reactjs : no. 94 score: 8.603094540249472

r/psychology : no. 95 score: 8.5300677182596

r/learnjava : no. 96 score: 8.406833849535417

r/NYCbike : no. 97 score: 8.406833849535417

r/IKEA : no. 98 score: 8.406833849535417

r/django : no. 99 score: 8.406833849535417

r/awardtravel : no. 100 score: 8.406833849535417",bigdata,hey   i   a       built a   algorithm for   here is what it   for users of   come over to     get personal or      enjoy these   for         no   score                      no   score                     no   score                      no   score                   raws  no   score                       no   score                    rscala  no   score                      no   score                      no   score                    rmachinelearning  no    score                    ranalytics  no    score                   rkubernetes  no    score                    rpostgresql  no    score                   rbusinessintelligence  no    score                    rlanguagetechnology  no    score                    rpython  no    score                    rstatistics  no    score                   rjava  no    score                   rlearnpython  no    score                      no    score                   rawscertifications  no    score                    rsql  no    score                   rlearnmachinelearning  no    score                    rcompsci  no    score                       no    score                    rmlquestions  no    score                    rgolang  no    score                    rapachekafka  no    score                    rrlanguage  no    score                    rprogramminglanguages  no    score                       no    score                   rrust  no    score                       no    score                    rlinuxquestions  no    score                       no    score                    rpowerbi  no    score                    rrstats  no    score                    rsqlserver  no    score                    rstartup  no    score                    rinsightfulquestions  no    score                    rjavascript  no    score                    rvuejs  no    score                    rcomputervision  no    score                    rstartups  no    score                    raskcomputerscience  no    score                    rforeignpolicy  no    score                    rm finance  no    score                    relectroneum  no    score                    rterraform  no    score                       no    score                       no    score                       no    score                      no    score                    ransible  no    score                    rhololens  no    score                       no    score                    rcs    no    score                    rgraphicsprogramming  no    score                    rcogsci  no    score                    ropenwrt  no    score                       no    score                    rprogramming  no    score                    rvim  no    score                   raskprogramming  no    score                   rcrypto  no    score                       no    score                       no    score                    rhtpc  no    score                    rmacbookpro  no    score                       no    score                    rmysql  no    score                   rchch  no    score                   rexpats  no    score                      no    score                      no    score                   rvisualization  no    score                   rhomenetworking  no    score                   rcscareerquestions  no    score                      no    score                   rberlinsocialclub  no    score                   rtypescript  no    score                   rruby  no    score                   rlinux  no    score                   rtrackers  no    score                   rchattanooga  no    score                      no    score                   rapachespark  no    score        no    score                   raskto  no    score                  rchrome  no    score                     no    score                      no    score                   rviolinist  no    score                   rreactjs  no    score                   rpsychology  no    score                 rlearnjava  no    score                   rnycbike  no    score                   rikea  no    score                      no    score                      no     score                 
t3_evkv5o,The AI transformation playbook,,bigdata,the ai transformation playbook 
t3_evmlzk,Learn Complete Machine Learning from Scratch,,bigdata,learn complete machine learning from scratch 
t3_evkl8v,Most Important Big Data Adoption Trends in 2019,,bigdata,most important big       in      
t3_evjglr,Managing financial services model risk in an age of big data and AI,,bigdata,managing financial services   risk in an age of big     ai 
t3_eva0fe,Newsletter of MLOps and DataOps,,bigdata,newsletter of mlops     
t3_evan5a,Yet another Spark ETL framework,"Hey all, I am currently working on a Scala ETL framework based on Apache Spark and I am very happy that we just open-sourced it :)

The goal of this framework is to make ETL application developers' life easier. 

We are a newly created but fast-growing data team. The main profiles of our team are data scientists, data analysts, and data engineers. However as the team grows, we start noticing 3 major problems:

* Our developers have to frequently switch between different projects, and the switch is somehow quite time-consuming
* Debugging others’ code (sometimes our own code) is a nightmare 
* We may have to spend lots of time solving non-business-related issues

So we try to solve our problems with the framework, which:

* generalizes the structure of ETL projects
* helps developers to write modular data transformation code
* hide technical and non-business-related code from data scientists/analysts

Feel free to take a look at our repository!  [https://github.com/JCDecaux/setl](https://github.com/JCDecaux/setl) 

Feedback is welcome :)",bigdata,yet another spark etl framework hey all i am currently working on a scala etl framework   on apache spark   i am very happy that we just   it   the goal of this framework is to make etl application   life easier   we are a newly   but fastgrowing   team the main profiles of our team are   scientists   analysts     engineers however as the team grows we start noticing   major problems   our   have to frequently switch between   projects   the switch is somehow quite timeconsuming    others’   sometimes our own   is a nightmare   we may have to   lots of time solving   issues  so we try to solve our problems with the framework which   generalizes the structure of etl projects  helps   to write     transformation      technical       from   scientistsanalysts  feel free to take a look at our repository       is welcome 
t3_ev6jjq,Top 33 Influencers In Big Data &amp; Analytics In 2019-20,,bigdata,top    influencers in big    analytics in        
t3_ev7ayr,Streaming Machine Learning with Tiered Storage and Without a Data Lake,,bigdata,streaming machine learning with   storage   without a   lake 
t3_euwtm6,Why Vertica Customers Adopt Apache Druid for Real-Time Analytics,,bigdata,why vertica customers   apache   for realtime analytics 
t3_euthdo,Databricks database as DWH,"What are the pros and cons of using Databricks databases and tables as DWH?

Correct me if I'm wrong, it's the same as HDFS + Hive, yes?",bigdata,    as   what are the pros   cons of using       tables as    correct me if im wrong its the same as    hive yes
t3_ev1uia,Increasing analytics teams productivity - an introduction to Neebo | Neebo,"Get ready to improve the productivity of your #analytics teams with Neebo 👉Join us in a #freewebinar on Feb. 18, Tue., 10AM PT/ 1PM ET and learn how you can help your analysts find answers without the need for IT to continuously fine-tune the operations.",bigdata,increasing analytics teams    an   to neebo  neebo get   to improve the   of your analytics teams with neebo 👉join us in a freewebinar on feb    tue   am pt  pm et   learn how you can help your analysts   answers without the   for it to continuously finetune the operations
t3_eun7gg,GDPR Compliance &amp; Its Importance for an Organization,,bigdata,  compliance  its importance for an organization 
t3_euoiy5,An interview about how the Great Expectations framework helps you add meaningful tests and validation to your data pipeline to drive down technical debt,,bigdata,an interview about how the great expectations framework helps you   meaningful tests     to your   pipeline to     technical   
t3_eunast,An AI epidemiologist sent the first warning signs of the Wuhan virus,,bigdata,an ai   sent the first warning signs of the wuhan virus 
t3_eu73vm,Google just published 25 million free datasets,,bigdata,google just      million free   
t3_euk5v4,Multi Matrix Deep Learning with GPUs,,bigdata,multi matrix   learning with gpus 
t3_eujf67,How will Big Data impact Law and Economics,"I'm a law student, but in school I used to code (nothing hardcore, just enough python to make a simple game and enough PHP SQL HTML CSS to make a functional website). 

I'm doing a B.A.LLB. course, which means I study social sciences with law subjects. Right now, I'm the most interested in Economics. The university is about to publish a book ""Law and Economics"" and I'm writing a chapter on Big Data. 

I need some resources which could give me a deep enough insight as to how data mining operates and how artificial intelligence combined with big data can, and does, impact our economy and our laws.

If you guys could tell me where I can read more about these topics, please help.

Thank you.",bigdata,how will big   impact law   economics im a law   but in school i   to   nothing   just enough python to make a simple game   enough php sql html css to make a functional website   im   a ballb course which means i   social sciences with law subjects right now im the most   in economics the university is about to publish a book law   economics   im writing a chapter on big     i   some resources which   give me a   enough insight as to how   mining operates   how artificial intelligence   with big   can     impact our economy   our laws  if you guys   tell me where i can   more about these topics please help  thank you
t3_eujc4g,SAS Certified Big Data Professional Using SAS 9,,bigdata,sas   big   professional using sas   
t3_eu7qk1,Podcast: the core fields of data science,,bigdata,  the core   of   science 
t3_etsmxa,Big Data : All about Data Injection and Apache Sqoop,"Data Injection Overview :

Data lake : A data lake is centralised storage repository used to store large amounts of structured,semi-structured and unstructured data. It has can have different formats of data like log files, click stream, social media IoT devices.

Data Lake Vs Data Warehouse :

Data Lake Data Warehouse
It can be structured,semi-structured and unstructured Data organised into single schema; like tabular formats used in RDBMS
Any data that may or may not be curated (i.e. raw data) Highly curated data that serves as the central version of the truth
Low cost Expensive storage that has faster response times.
Data scientist and data developer Business analysts
Machine Learning, predictive analytical and data discovery Batch reporting,BI, and visualisation

Data Injection -

1. Big data Injection involves transferring of data, especially unstructured data,from where it originated,into a system where it can be stored and analyse such as Hadoop.
2. The injection process can be continuous or asynchronous, real time real time Or both, depending upon the characteristics of the source of origination.
3. In scenario where the source and destination do not have the same data format or protocol,data transformation or conversion is done to make data usable by the destination system.

Data Injection Tools -

Choosing data injection tool is important which in-turn based on the factor like data source , target and transformation.

Apache Sqoop , Apache Kafka, Apache Flume

Data injection tools provide user with a data injection framework that makes it easier to extract data from different types of sources and support the range of data transport protocols.

Data injection tools also eliminate the need for manually coding individual data pipelines for every data source and accelerate data processing by helping you deliver data efficiency to ETL .


Apache Sqoop -

Sqoop, an Apache Hadoop ecosystem project is command line interface application for transferring data between relational databases and Hadoop. It supports incremental loads of a single table or a free from SQL query. Imports can also be used to populate tables in HIVE or HBASE. Exports can be used to put data from Hadoop into a relational database.

Sqoop is Apache Hadoop Ecosystem project whose responsibility is to import or export operations across relational databases. The reason for using Sqoop are as follows :

1. SQL server are developed worldwide .
2. Nightly processing is done on SQL server.
3. Sqoop allows to move certain parts of data from traditional SQL database to Hadoop.
4. Sqoop transfer data effectively and swiftly.
5. Sqoop handles large data through ecosystem.
6. Sqoop brings process data from Hadoop to the application.

Sqoop is required when a database is imported from a Relational Database (RDB) to Hadoop or vice versa.

• While exporting database from RDB to Hadoop : User must consider the consistency of data, consumption or production system resource, and preparation of data for provisioning that data to downstream pipeline.
• While importing data from Hadoop to RDB : User must keep in mind that directing accessing data residing in external system within a MapReduce framework, complicates applications. It exposes the production system to excessive load originating from cluster node.

Benefits of Sqoop -

1. Transfer data from Hadoop to RDB and vice versa.
2. Transforms data in Hadoop with MapReduce or HIVE without extra coding.
3. Import data from RDB to the Hadoop Distributed File System (HDFS)
4. Exports data back to RDB.",bigdata,big    all about   injection   apache sqoop   injection overview     lake  a   lake is   storage repository   to store large amounts of         it has can have   formats of   like log files click stream social   iot      lake vs   warehouse     lake   warehouse it can be           into single schema like tabular formats   in   any   that may or may not be   ie raw   highly     that serves as the central version of the truth low cost expensive storage that has faster response times   scientist       business analysts machine learning   analytical       batch reportingbi   visualisation    injection     big   injection involves transferring of   especially     where it   a system where it can be     analyse such as     the injection process can be continuous or asynchronous real time real time or both   upon the characteristics of the source of origination   in scenario where the source       not have the same   format or   transformation or conversion is   to make   usable by the   system    injection tools   choosing   injection tool is important which inturn   on the factor like   source  target   transformation  apache sqoop  apache kafka apache flume    injection tools   user with a   injection framework that makes it easier to extract   from   types of sources   support the range of   transport protocols    injection tools also eliminate the   for manually       pipelines for every   source   accelerate   processing by helping you     efficiency to etl    apache sqoop   sqoop an apache   ecosystem project is   line interface application for transferring   between relational       it supports incremental   of a single table or a free from sql query imports can also be   to populate tables in hive or hbase exports can be   to put   from   into a relational    sqoop is apache   ecosystem project whose responsibility is to import or export operations across relational   the reason for using sqoop are as follows     sql server are        nightly processing is   on sql server   sqoop allows to move certain parts of   from   sql   to     sqoop transfer   effectively   swiftly   sqoop   large   through ecosystem   sqoop brings process   from   to the application  sqoop is   when a   is   from a relational     to   or vice versa  • while exporting   from   to    user must   the consistency of   consumption or   system resource   preparation of   for provisioning that   to   pipeline • while importing   from   to    user must keep in   that   accessing     in external system within a   framework complicates applications it exposes the   system to excessive   originating from cluster    benefits of sqoop     transfer   from   to     vice versa   transforms   in   with   or hive without extra     import   from   to the     file system     exports   back to  
t3_etqgcs,"Big Data Learning Path for all Engineers, Developers and Data Scientists out there",,bigdata,big   learning path for all engineers       scientists out there 
t3_etbo09,New precisionFDA data science challenge: Help advance postmarket surveillance techniques for FDA regulated products,"Hi!

The precisionFDA Gaining New Insights by Detecting Adverse Event Anomalies Using FDA Open Data Challenge is now live!

The purpose of this challenge is to advance techniques for the surveillance and detection of adverse events associated with FDA products. Participants are encouraged to use machine learning and artificial intelligence algorithms to automate the detection of anomalies in adverse event data. For more information and to get started, visit [https://go.usa.gov/xdge3](https://go.usa.gov/xdge3)!

Also, if you live in the DMV area and are interested in learning more about how FDA is modernizing its data strategy, attend the “Modernizing FDA’s Data Strategy” public meeting on March 27th.",bigdata,new     science challenge help   postmarket surveillance techniques for       hi  the   gaining new insights by     event anomalies using   open   challenge is now live  the purpose of this challenge is to   techniques for the surveillance     of   events   with     participants are   to use machine learning   artificial intelligence algorithms to automate the   of anomalies in   event   for more information   to get   visit   also if you live in the   area   are   in learning more about how   is   its   strategy   the “   ’s   strategy” public meeting on march   th
t3_etgair,disabling incoming connections to hbase,Is it possible to block incoming connections to the hbase cluster? I want to do data export and import in BigTable with the ability to read data from an existing hbase cluster.,bigdata,  incoming connections to hbase is it possible to block incoming connections to the hbase cluster i want to     export   import in bigtable with the ability to     from an existing hbase cluster
t3_estwtf,ETL Spark Examples,"Hey everyone. I’m doing an ETL pipeline in Pyspark for my organization. Anyone have good resources to recommend? I read the ETL toolkit but that isn’t big data specific. All the examples I find online or on github are very small and seem to be written by people who spent 10 minutes on big data. I haven’t found any examples of production level robust pipelines that interact with traditional databases.

Thanks",bigdata,etl spark examples hey everyone i’m   an etl pipeline in pyspark for my organization anyone have   resources to   i   the etl toolkit but that isn’t big   specific all the examples i   online or on github are very small   seem to be written by people who spent    minutes on big   i haven’t   any examples of   level robust pipelines that interact with      thanks
t3_esvspf,How Data Management should be like driving a car from A to B point.,,bigdata,how   management   be like   a car from a to b point 
t3_esron5,[2013] You May Not Need Big Data After All,,bigdata,  you may not   big   after all 
t3_esx6bm,Infinite Storage in Confluent Platform,,bigdata,infinite storage in confluent platform 
t3_esswyy,The Decision Maker's Handbook to Data Science,,bigdata,the   makers   to   science 
t3_esovb3,Spark SQL Join Types with examples,,bigdata,spark sql join types with examples 
t3_esrccd,5 reasons why your SEO strategy needs Big Data,,bigdata,  reasons why your seo strategy   big   
t3_esqgds,Make Your Business More Successful By Big Data Analytics,,bigdata,make your business more successful by big   analytics 
t3_esouxg,Spark Groupby Example with DataFrame,,bigdata,spark groupby example with   
t3_esorka,Spark Groupby Example with DataFrame,,bigdata,spark groupby example with   
t3_ese859,Right-sizing Kafka clusters on Kubernetes,,bigdata,rightsizing kafka clusters on kubernetes 
t3_esfyg8,5 Ways Artificial Intelligence is Changing the Software Development Industry,,bigdata,  ways artificial intelligence is changing the software     
t3_es8pyi,Is Big Data Analytics Killing Other Jobs or Creating Them? -Big Data Analytics News,,bigdata,is big   analytics killing other jobs or creating them big   analytics news 
t3_es9sz9,Statistical Knowledge from Psychology for Big Data?,"Hi together,

I studied psychology and my favourite party was always the research side (planning, conducting and working with data to make implications)

We had a lot of statistics for it but more hands on with data from our participants (correlations, significance, ANOVA, MANOVA, regressions...)

I'm really interested in the whole big data topic but I don't know how much the psychological side prepared me for this. My naive understanding is that it shouldn't be too different to work with data pools of 20 test subjects or 10000 data entries that have been collected from a software.

What skills come on top for big data? Would it be hard for me to start in big data? So far I worked pretty much only in SPSS. I wanted to start learning R because of personal interest and because I think it would be a useful tool to master.

Thanks in advance for your input :-)",bigdata,statistical   from psychology for big   hi together  i   psychology   my favourite party was always the research   planning     working with   to make implications  we   a lot of statistics for it but more   on with   from our participants correlations significance anova manova regressions  im really   in the whole big   topic but i   know how much the psychological     me for this my naive   is that it   be too   to work with   pools of    test subjects or         entries that have been   from a software  what skills come on top for big     it be   for me to start in big   so far i   pretty much only in spss i   to start learning r because of personal interest   because i think it   be a useful tool to master  thanks in   for your input 
t3_es6bsv,Top 15 Big Data Tools in 2020,,bigdata,top    big   tools in      
t3_errvag,The big data maturity levels,,bigdata,the big   maturity levels 
t3_erao5d,Questions about big data &amp; the duties of a data engineer.,"Hi everyone, 

I've been working in the big data field since 2016 as a data engineer. 

I started in the Hadoop ecosystem (Cloudera / Hortonworks distributions) and switched a couple years ago to AWS.

&amp;#x200B;

I have a feeling that while many big data products keep getting updated, there is no actual innovation in the field, there is a sprouting of new products that do the same job of already well established projects and 99% of the times they don't make it past version 0.\*.

&amp;#x200B;

In the meantime, I am also noticing that my day to day job has changed drastically:

back in 2016 I had to setup my cluster, model/ingest data on top of it and develop code (mainly Spark) to extract insights from data, basically porting prototypes developed by the data science team.

&amp;#x200B;

Nowadays I feel like the job of the data engineer is becoming less and less necessary: 

* An always increasing number of data scientists learns Spark and can directly write code. An engineer could always write better code (from a SWE best practices and algorithmic performance point of view), but I am noticing that collaboration between the two professional figures is not appreciated as it was in the past. 
* In my personal experience scientists demand to work on the rawest possible data, taking from engineers also the task to perform data preparation/modelling.
* The infrastructure setup is now in charge of DevOps teams and the switch from on-premise machines to cloud services made configurations easier. 
* Many big data products are now a commodity that can be used by SWEs with no particular skillsets.

I feel like I am at a crossroads where I should choose between becoming a data scientist and becoming a devops. 

&amp;#x200B;

These sensations seem to be confirmed by job listings too, I see data scientist positions sprouting and it becomes harder and harder to find a proper data engineering job (many of them are devops under the hood, or something completely unrelated).

&amp;#x200B;

Does anybody of you feel the same? 

How did you adapt to change? Have you ""reinvented"" your figure of data engineer?",bigdata,questions about big    the   of a   engineer hi everyone   ive been working in the big     since      as a   engineer   i   in the   ecosystem    hortonworks       a couple years ago to aws  x   b  i have a feeling that while many big     keep getting   there is no actual innovation in the   there is a sprouting of new   that   the same job of   well   projects      of the times they   make it past version    x   b  in the meantime i am also noticing that my   to   job has      back in      i   to setup my cluster     on top of it       mainly spark to extract insights from   basically porting prototypes   by the   science team  x   b    i feel like the job of the   engineer is becoming less   less necessary    an always increasing number of   scientists learns spark   can   write   an engineer   always write better   from a swe best practices   algorithmic performance point of view but i am noticing that collaboration between the two professional figures is not   as it was in the past   in my personal experience scientists   to work on the rawest possible   taking from engineers also the task to perform      the infrastructure setup is now in charge of   teams   the switch from onpremise machines to   services   configurations easier   many big     are now a   that can be   by swes with no particular skillsets  i feel like i am at a   where i   choose between becoming a   scientist   becoming a     x   b  these sensations seem to be   by job listings too i see   scientist positions sprouting   it becomes       to   a proper   engineering job many of them are     the   or something completely    x   b      of you feel the same   how   you   to change have you   your figure of   engineer
t3_erf2xq,An interview about how Mayvenn replatformed their production dataflows using Ascend and improved their ability to deliver meaningful analytics to their business,,bigdata,an interview about how mayvenn   their     using       their ability to   meaningful analytics to their business 
t3_erkk9y,Are university degrees for data science about to replaced?,,bigdata,are university   for   science about to   
t3_erb2fo,Power BI – Architecture of Power BI,,bigdata,power bi – architecture of power bi 
t3_eqmq67,What is the big data roadmap in 2020?,"Does anyone know how to start in big data field and if there are any prerequisites?
 and if there are some preferable resources i'd be thankful.",bigdata,what is the big     in        anyone know how to start in big       if there are any prerequisites    if there are some preferable resources   be thankful
t3_eq3u91,What Impact is Big Data Having on Soccer?,,bigdata,what impact is big   having on soccer 
t3_epyizd,What is DevOps? How is DevOps different from traditional IT,,bigdata,what is   how is     from   it 
t3_epmhdo,The Greatest Hedge Fund of All Time,"Renaissance Technologies is the Greatest Hedge Fund of All Time.

Founded by math genius Jim Simons, it's flagship fund Medallion has an average gross return of 66.1% since 1988.

With an average net return of 39.1% after fees.

The Medallion Fund is available only to current and past employees and their families, closing to outside investors in 1993.

Since, 1988, the Medallion Fund has racked up trading profits of more than $100 billion.

Now, I mentioned a net return of 39.1% after fees: well, the fees have been greater than the usual '2 and 20' structure (which means a 2% management fee and a 20% performance fee).

Medallion has had a 5% management fee, and from 1988 to 2001, a 20% performance fee and from 2002 until now, a 44% performance fee.

Notice in particular the return in 2007 and 2008, a time when many were completely REKT.

In 2008, a return (after these monstrous fees) of 82.4%

**To make us all feel terrible, if you had invested $1000 into Medallion in 1988 you would have today, after fees, around $23MM**.

That certainly beats inflation...

So, how did they do it and what can we take away from this story (aside from searing jealousy)?

&amp;#x200B;

**PART 1: The Early Stages**

Early on, Simons had a goal of algorithmic investing.

Remember, this was the late 1980s before the phrase big data became a household name and most investment decisions were made over the phone based on gut with the likes of Jordan Belfort trying to scam you!

“I don’t want to have to worry about the market every minute. I want models that will make money while I sleep,” Simons said. “A pure system without humans interfering.”

Simons hired Sandor Straus to help him collect historic commodity information

Straus’ was essential to Renaissance Technologies early success in commodities trading.

He became somewhat of a data guru ensuring pricing was consistent and accurate, checking his numbers matched with yearbook data provided by commodity exchanges, Wall Street Journal, other newspapers and anything else he could get his hands on.

Over time, Straus and his colleagues discovered additional historical pricing data, helping the development of new predictive models.

In fact, some of the stock market data they'd later find went back as far as the 1800s!

At the time, the team couldn't do much with the data, BUT the ability to search modern history to see how markets reacted to unusual events would later help Simon's team build models to profit from market collapses and so called 'Black Swan events'.

The return in 2008 is a prime example of that.

Commodity markets were relatively simple and RenTec found success in deploying simple trading strategies.

The fund wasn't bothered as to why these trading patterns existed - the only thing that mattered is that they occurred in a predictable and actionable way.

&amp;#x200B;

**PART 2: Intellectual Capital**

Now in order to build these quantitative models, RenTec is composed of mathematicians and physicists of the highest order and it has even been described as the ""best math department in the world"".

Therefore, their quantitative researchers are well aware of the problems with data mining, over-fitting and spurious signals.

We are taking A LOT of data: 9TB per day in fact.

RenTec originally focussed on trading commodities, currencies and futures.

The strategies were mainly trending (i.e. price will continue to move in same direction) and mean reversion (i.e. price will return to original value).

Simons was experimenting in the stock market (equities) since the late 1980s but the strategy that had worked well on futures was not working on equities.

In 1995, David Magerman, an early employee, spotted a line of simulation code used for the equity trading system showing the S&amp;P 500 at an unusually low level.

This test code appeared to use a figure from back in 1991 that was roughly half the current number.

It had been written as a static figure, rather than as a variable that updated with each move in the market.

Magerman also spotted an algebraic error elsewhere in the code.

Finally, the simulator’s algorithms could finally recommend an ideal portfolio for the trading system to execute.

The resulting portfolio seemed to generate big profits, at least according to Magerman’s calculations.

Only then did Renaissance commit significant capital into the equity markets, and since then...well, pretty good....

&amp;#x200B;

**PART 3: Infrastructure**

Now I mentioned before about the sheer amount of data RenTec is utilising.

Big data has obviously caught on, but many hedge funds continue to under-perform the market and even some hedge funds focussed on quant methods haven't fared too well.

The problem, and one of the reasons RenTec is so special, is the barrier to entry is so incredibly high:

Building a data pipeline and the infrastructure required to process that data is no trivial matter.

To then get profitable trading signals from that processed data is a mammoth task.

RenTec has been in the game for over 30 years, constantly refining their algorithms and improving the efficiency of their data processing pipeline.

They have completely automated the process of signal discovery:

They don't hire researchers to manually derive novel insights or trading models from data, and they don't really bother with exclusive sources of data. Instead, they hire researchers to improve methods for automatically processing vast amounts of arbitrary data and extracting profitable trading signals from it.

RenTec has automated the data processing and feature extraction pipeline end to end.

The data is a pure abstraction to them. They don't bother with forming hypotheses and trying to find data to test them, they allow their algorithms to actively discover new correlations from the ground up. So many quantitative funds advertise how much data they work with, and how they have all these exotic sources of data at their disposal - but the data does not matter. The models for the data do not matter.

The mathematics of *efficiently processing* that data are what matters.

&amp;#x200B;

**CONCLUSION:**

**The takeaway from this is the following: do not day trade, you will get REKT.**

**You are competing with immense infrastructure and intellectual capital of the highest level.**

[https://www.youtube.com/watch?v=jcy8QaILDJI](https://www.youtube.com/watch?v=jcy8QaILDJI)

&amp;#x200B;

BRAVE BROWSER: [https://brave.com/fin894](https://brave.com/fin894)",bigdata,the greatest     of all time renaissance technologies is the greatest     of all time    by math genius jim simons its flagship     has an average gross return of     since       with an average net return of     after fees  the     is available only to current   past employees   their families closing to   investors in       since      the     has   up   profits of more than     billion  now i   a net return of     after fees well the fees have been greater than the usual        structure which means a   management fee   a    performance fee    has   a   management fee   from      to      a    performance fee   from      until now a    performance fee  notice in particular the return in             a time when many were completely rekt  in      a return after these monstrous fees of      to make us all feel terrible if you          into   in      you   have   after fees     mm  that certainly beats inflation  so how   they   it   what can we take away from this story   from searing jealousy  x   b  part   the early stages  early on simons   a goal of algorithmic investing  remember this was the late     s before the phrase big   became a   name   most investment   were   over the phone   on gut with the likes of   belfort trying to scam you  “i  ’t want to have to worry about the market every minute i want   that will make money while i sleep” simons   “a pure system without humans interfering”  simons     straus to help him collect historic   information  straus’ was essential to renaissance technologies early success in      he became somewhat of a   guru ensuring pricing was consistent   accurate checking his numbers   with yearbook     by   exchanges wall street journal other newspapers   anything else he   get his   on  over time straus   his colleagues     historical pricing   helping the   of new      in fact some of the stock market     later   went back as far as the     s  at the time the team     much with the   but the ability to search   history to see how markets   to unusual events   later help simons team     to profit from market collapses   so   black swan events  the return in      is a prime example of that    markets were relatively simple   rentec   success in   simple   strategies  the   wasnt   as to why these   patterns    the only thing that   is that they   in a     actionable way  x   b  part   intellectual capital  now in   to   these quantitative   rentec is   of mathematicians   physicists of the highest     it has even been   as the best math   in the    therefore their quantitative researchers are well aware of the problems with   mining overfitting   spurious signals  we are taking a lot of    tb per   in fact  rentec originally   on     currencies   futures  the strategies were mainly   ie price will continue to move in same     mean reversion ie price will return to original value  simons was experimenting in the stock market equities since the late     s but the strategy that     well on futures was not working on equities  in        magerman an early employee   a line of simulation     for the equity   system showing the sp     at an unusually low level  this test     to use a figure from back in      that was roughly half the current number  it   been written as a static figure rather than as a variable that   with each move in the market  magerman also   an algebraic error elsewhere in the    finally the simulator’s algorithms   finally   an   portfolio for the   system to execute  the resulting portfolio   to generate big profits at least   to magerman’s calculations  only then   renaissance commit significant capital into the equity markets   since thenwell pretty    x   b  part   infrastructure  now i   before about the sheer amount of   rentec is utilising  big   has obviously caught on but many     continue to   the market   even some       on quant   havent   too well  the problem   one of the reasons rentec is so special is the barrier to entry is so   high    a   pipeline   the infrastructure   to process that   is no trivial matter  to then get profitable   signals from that     is a mammoth task  rentec has been in the game for over    years constantly refining their algorithms   improving the efficiency of their   processing pipeline  they have completely   the process of signal    they   hire researchers to manually   novel insights or     from     they   really bother with exclusive sources of     they hire researchers to improve   for automatically processing vast amounts of arbitrary     extracting profitable   signals from it  rentec has   the   processing   feature extraction pipeline   to    the   is a pure abstraction to them they   bother with forming hypotheses   trying to     to test them they allow their algorithms to actively   new correlations from the   up so many quantitative     how much   they work with   how they have all these exotic sources of   at their    but the     not matter the   for the     not matter  the mathematics of efficiently processing that   are what matters  x   b  conclusion  the takeaway from this is the following   not     you will get rekt  you are competing with immense infrastructure   intellectual capital of the highest level    x   b  brave browser 
t3_epyp30,I personally work in big data then,"Big data be like : 
No one knows how to do it, everybody talks about it, everybody says he already did it, so you say you did it too

Me :
Sounds a bit like sex in high-school",bigdata,i personally work in big   then big   be like   no one knows how to   it   talks about it   says he     it so you say you   it too  me    a bit like sex in highschool
t3_epd6us,"We have the results from ""What should you call a group of Data Scientists Poll""!","CLUSTER is your winner!

I really wanted kaggle gaggle to win but the people have spoken

Check out the rest of the results here : [https://greatexpectations.io/blog/datasci-counter-poll/](https://greatexpectations.io/blog/datasci-counter-poll/)",bigdata,we have the results from what   you call a group of   scientists poll cluster is your winner  i really   kaggle gaggle to win but the people have spoken  check out the rest of the results here  
t3_ephwcv,Berlin Buzzwords Call for Papers now open,"Berlin Buzzwords is looking for submissions on the latest in open source software projects in the field of big data analysis, scalability, storage and searchability.

Closing date: **16 February 2020.** More info here: [https://berlinbuzzwords.de/news/call-submissions-now-open](https://berlinbuzzwords.de/news/call-submissions-now-open)",bigdata,berlin   call for papers now open berlin   is looking for submissions on the latest in open source software projects in the   of big   analysis scalability storage   searchability  closing      february      more info here 
t3_epgu57,Learn Complete Machine Learning from Scratch 2020,,bigdata,learn complete machine learning from scratch      
t3_epes3x,"Data science ,AI entusiastics",,bigdata,  science ai entusiastics 
t3_ep20wb,"Would like to know which big data tools on AWS, Azure and GCP are good.","I am trying to learn information about big data tools (storage, processing, or analytics) that are provided by the major three cloud providers (AWS, Azure, and GCP).  
I don't have accounts with them, and neither do I have a specific application or purpose right now. I just try to have some perspective of the trend and the big picture.

- I would like to choose a few ""best/good"" tools to start to read about first. I have heard the names of a lot of tools on them especially AWS, but not really sure which tools are the best (either best in designed or the most popular or the most promising).

- AWS seems to have the longest history, I guess that some tools may be falling out of favor, or replaced by some newer better ones.
Azure and GCP are less heard of. But they are also late comers. Do their big data tools have better designs (not meaning sacrificing features and functionalities, but having more coherent programming interfaces) and better implementations (less bugs and wierd workarounds, for example).

- I guess the three providers provide both generic tools that are also available outside any providers (e.g. Spark, Cassandra) and their own specific tools.
I am not sure which are preferred in general. Personally, I like generic tools, because they won't cause vendor locking in. But I also understand that provider specific tools can do better jobs than generic tools, because they are profit motivated. 

Could you mention a few big data tools on each of the three cloud providers that are popular/promising or have good designs or implementations, so that I could read more about them?

Thanks.",bigdata,  like to know which big   tools on aws azure   gcp are   i am trying to learn information about big   tools storage processing or analytics that are   by the major three     aws azure   gcp   i   have accounts with them   neither   i have a specific application or purpose right now i just try to have some perspective of the     the big picture   i   like to choose a few   tools to start to   about first i have   the names of a lot of tools on them especially aws but not really sure which tools are the best either best in   or the most popular or the most promising   aws seems to have the longest history i guess that some tools may be falling out of favor or   by some newer better ones azure   gcp are less   of but they are also late comers   their big   tools have better   not meaning sacrificing features   functionalities but having more coherent programming interfaces   better implementations less bugs       for example   i guess the three     both generic tools that are also available   any   eg spark     their own specific tools i am not sure which are   in general personally i like generic tools because they wont cause   locking in but i also   that   specific tools can   better jobs than generic tools because they are profit       you mention a few big   tools on each of the three     that are popularpromising or have     or implementations so that i     more about them  thanks
t3_ep6ux1,Need help with research topic,"Hi, I'm a M&amp;IS student and I just started a Big Data Analytics course (for my Computer Science minor) and I would greatly appreciate any resources for learning about either of these two topics. I'm not sure which one to pick because I haven't found too much info.

The two topics are 
1) Big Data Visualization
2) Queries over (distributed) time-series data

I have an interest in algorithmic stock trading so it would be really cool to study time-series data, but I think the topic is more based on how to efficiently query it in a distributed format. (I have no idea what this means, can anyone clarify?)

Big Data Visualization is probably the better route to take anyway as a business major. If anyone is experienced in either of these two topics or can recommend newbie resources to start learning I would greatly appreciate it",bigdata,  help with research topic hi im a mis     i just   a big   analytics course for my computer science minor   i   greatly appreciate any resources for learning about either of these two topics im not sure which one to pick because i havent   too much info  the two topics are    big   visualization   queries over   timeseries    i have an interest in algorithmic stock   so it   be really cool to   timeseries   but i think the topic is more   on how to efficiently query it in a   format i have no   what this means can anyone clarify  big   visualization is probably the better route to take anyway as a business major if anyone is   in either of these two topics or can   newbie resources to start learning i   greatly appreciate it
t3_ep26rc,Is China going to overtake the US in data science research?,,bigdata,is china going to overtake the us in   science research 
t3_ep0lpr,Download and Install QlikView,,bigdata,    install qlikview 
t3_eonwt4,An interview about YugabyteDB and how it was architected to power the new generation of planet scale applications,,bigdata,an interview about     how it was   to power the new generation of planet scale applications 
t3_eoia2b,Tips to improve infrastructure Security,"* Use a firewall to prevent unauthorized Access
* Monitor your network traffic
* Find holes in your traffic
* Use SSL Encryption to protect the data

[IT Infrastructure service | Infrastructure service provider](https://mazenettech.in/network/service.php)",bigdata,tips to improve infrastructure security  use a firewall to prevent   access  monitor your network traffic    holes in your traffic  use ssl encryption to protect the     
t3_eo6sf5,Optimize Response Time of your Machine Learning API in Production,,bigdata,optimize response time of your machine learning api in   
t3_eo0m1q,What is Hadoop ? Overview of Hadoop Ecosystem and its Architecture and all Hadoop Components explained in simple terms !,,bigdata,what is    overview of   ecosystem   its architecture   all   components   in simple terms  
t3_eo0tzr,Deploying ML Models in Distributed Real-time Data Streaming Applications,[https://www.kharekartik.dev/2020/01/12/streaming-machine-learning/](https://www.kharekartik.dev/2020/01/12/streaming-machine-learning/),bigdata,  ml   in   realtime   streaming applications 
t3_eo1xxy,Why Should Hiring Big Data Talent Become a Key for Businesses in 2020?,,bigdata,why   hiring big   talent become a key for businesses in      
t3_enugjt,Serverless Big Data Pipelines,,bigdata,serverless big   pipelines 
t3_enoik5,Which public cloud is more popular in data engineering and data science teams?,,bigdata,which public   is more popular in   engineering     science teams 
t3_enm9n4,looking for a course that doesn't ask me to buy Amazon services,"I need to learn topics on Hadoop and Spark, but I want to practice them on ec2 or any other free cloud service. If anyone has done any such course, please help me here.",bigdata,looking for a course that   ask me to buy amazon services i   to learn topics on     spark but i want to practice them on ec  or any other free   service if anyone has   any such course please help me here
t3_emxkah,How to install and running Cloudera Docker Container on Ubuntu,,bigdata,how to install   running     container on ubuntu 
t3_emnkm0,Manage Your Large Data With Most Useful BiG Data Application 2020,,bigdata,manage your large   with most useful big   application      
t3_emr8xv,Algorithmic bias and biases in society,,bigdata,algorithmic bias   biases in society 
t3_emouvi,Data Science Salary Reports 2020,"Hi all, 

Our data science salary reports for both Europe and APAC are now live. Would love to hear your thoughts!

European Salary Report - [https://mailchi.mp/bigcloud.io/european-salary-report-2020](https://mailchi.mp/bigcloud.io/european-salary-report-2020)

APAC Salary Report - [https://mailchi.mp/bigcloud.io/apac-salary-report-download](https://mailchi.mp/bigcloud.io/apac-salary-report-download)",bigdata,  science salary reports      hi all   our   science salary reports for both europe   apac are now live   love to hear your thoughts  european salary report    apac salary report  
t3_emm2y6,Architecture of Qlik view,,bigdata,architecture of qlik view 
t3_emlvxe,Big Data &amp; ML Digest [telegram channel],,bigdata,big    ml     
t3_emlvv4,Thoughts on Data.World?,"I've been looking into the company and their offerings but I cant seem to find as much as I would expect in terms of user testimonials for a 5 year old (young) company.

Any insights on the company and its future potential are appreciated.",bigdata,thoughts on   ive been looking into the company   their offerings but i cant seem to   as much as i   expect in terms of user testimonials for a   year   young company  any insights on the company   its future potential are  
t3_emaqpu,"GameAnalytics + Imply Meet-up in London - January 15th, 2020","Hey Engineers! If you’re based in London, come join us at our Apache Druid meetup with Imply next Wednesday.  

We’ll be sharing a few key lessons we learned from migrating our backend systems to Apache Druid. And, there will be 🍕 and 🥤.

You can find all the details here: https://www.meetup.com/Apache-Druid-London/events/267380924",bigdata,gameanalytics  imply meetup in    january   th      hey engineers if you’re   in   come join us at our apache   meetup with imply next      we’ll be sharing a few key lessons we   from migrating our   systems to apache     there will be 🍕   🥤  you can   all the   here 
t3_em9seo,"Basics of Machine Learning with Python: Defination of Tuple , How to use Tuple , Slicing Tuple to get values :: Helping Beginners",,bigdata,basics of machine learning with python   of tuple  how to use tuple  slicing tuple to get values  helping beginners 
t3_em7azz,Pipeline to the Cloud – Streaming On-Premises Data for Cloud Analytics,,bigdata,pipeline to the   – streaming onpremises   for   analytics 
t3_em751o,"How to leverage data in word, excel, and pdf?","In the company, workers are generating documents in word, excel, and pdf format every day. There are two problems when we want to use the data.

1. files are not tagged (without metadata). This makes finding documents difficult.
2. the data/ information inside those files is not computer friendly and hard to extract. People create those files differently.  


How can we deal with that? Shall we discourage the use of word, excel, and pdf?",bigdata,how to leverage   in   excel     in the company workers are generating   in   excel     format every   there are two problems when we want to use the      files are not   without   this makes         the   information   those files is not computer       to extract people create those files       how can we   with that shall we   the use of   excel    
t3_elxzap,The Power Of Streaming ETL — Flatten JSON With ksqlDB,,bigdata,the power of streaming etl — flatten json with   
t3_elsdk6,Any suggestion to my AWS Data Architecture?,"I am a (Lead) Data Engineer in a small company. I also work as a data guy for a side-project and have my personal (side-)side-project myself.

Long story short, I am building something very similar to [newsapi.org](https://newsapi.org/) \-- a news API that basically allows you to query the news. Why am I doing a clone of it? Well:

1. Because I want to
2. It is quite of a challenge
3. I think I know how to build it
4. I want to make the price for my product 5 times cheaper comparing newsapi

So, in this post I will try to explain my outline of the architecture and I hope that someone will be able to advise me something (because I am not that experienced in the tech stack that I am going to use for this product).

DATA FLOW OUTLINE (no precise tech stuff here)

Most of the news publishers have RSS/Atom feeds (something similar to an API if you do not know what it is) to give a programatical access to the latest published news.

By checking each RSS once in a while you can see new records (here's [the one of the NY Times](https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)). Therefore, by collecting all the endpoints for all the news providers' feeds that I need, I will be able to collect my data.

Each article itself (to simplify) is title, published date, author, description, etc.

Let's say I have the place where I store all the articles. After that, I would have to query that with my API.

I plan to store only up to 7 days of recent data in the Elasticsearch.

CHOSEN TECH STACK

Now, I will describe how I see it being implemented.

1. Airflow to schedule the process of obtaining new data.
2. Postgres to store ""operational data"" -- info about feeds, endpoints, last queried time, how many new articles, etc
3. AWS Lambda (serverless function) to read the RSS feed of each news publisher (feedparser package)
4. Elasticsearch to store the article data for the API
5. AWS Lambda + Flask to deploy API
6. RapidAPI (an API marketplace) to deliver my solution to the end users
7. EXTRA. S3 + AWS Athena to store raw files and query them if needed

Airflow will schedule a batch (let's say of 100) of feeds to read. It will send 100 asynchronous calls of Lambda function. Each Lambda function will return all the articles that are now in the feed.

Then, all of those will be deduplicated with those that my solution have seen already. This deduplication will be run against the data in Postgres (not in the Elasticsearch).

After that, all new records will be inserted into my Elasticsearch cluster. In addition, a CSV file will be moved into the S3 bucket (plus I will configure Athena to query those CSV files if needed).

My serverless API will make hits to the Elasticsearch.

There is definitely many things I have looped but I hope you got the idea.

**Any suggestions/questions are more than welcome.**

I write pretty much everything about this project on my Medium account. For example, here is a [small one about RSS feeds and how to read](https://towardsdatascience.com/collecting-news-articles-through-rss-atom-feeds-using-python-7d9a65b06f70) them with Python posted on Towards Data Science.

Also, if you like you could support me by subscribing to the [Product Hunt prelaunch page](https://www.producthunt.com/upcoming/newscatcher).

If you would like to participate or become a beta tester, you are welcome!

Happy New Year everyone and I wish you all the Best!",bigdata,any suggestion to my aws   architecture i am a     engineer in a small company i also work as a   guy for a     have my personal   myself  long story short i am   something very similar to    a news api that basically allows you to query the news why am i   a clone of it well    because i want to   it is quite of a challenge   i think i know how to   it   i want to make the price for my     times cheaper comparing newsapi  so in this post i will try to explain my outline of the architecture   i hope that someone will be able to   me something because i am not that   in the tech stack that i am going to use for this      flow outline no precise tech stuff here  most of the news publishers have rssatom   something similar to an api if you   not know what it is to give a programatical access to the latest   news  by checking each rss once in a while you can see new   heres   therefore by collecting all the   for all the news     that i   i will be able to collect my    each article itself to simplify is title     author   etc  lets say i have the place where i store all the articles after that i   have to query that with my api  i plan to store only up to     of recent   in the elasticsearch  chosen tech stack  now i will   how i see it being      airflow to   the process of obtaining new     postgres to store operational    info about     last   time how many new articles etc   aws   serverless function to   the rss   of each news publisher   package   elasticsearch to store the article   for the api   aws    flask to   api     an api marketplace to   my solution to the   users   extra s   aws athena to store raw files   query them if    airflow will   a batch lets say of     of   to   it will       asynchronous calls of   function each   function will return all the articles that are now in the    then all of those will be   with those that my solution have seen   this   will be run against the   in postgres not in the elasticsearch  after that all new   will be   into my elasticsearch cluster in   a csv file will be   into the s  bucket plus i will configure athena to query those csv files if    my serverless api will make hits to the elasticsearch  there is   many things i have   but i hope you got the    any suggestionsquestions are more than welcome  i write pretty much everything about this project on my   account for example here is a   them with python   on     science  also if you like you   support me by subscribing to the    if you   like to participate or become a beta tester you are welcome  happy new year everyone   i wish you all the best
t3_elmvqi,The difference between DataOps and DevOps and other emerging technology practices.,,bigdata,the   between         other emerging technology practices 
t3_elgsrv,"Vroom! Vroom! New Dataset Rolls Out 64,000 Pictures of Cars",,bigdata,vroom vroom new   rolls out       pictures of cars 
t3_elaonj,A gentle Introduction to Big Data. Hope it helps the community !!,,bigdata,a gentle   to big   hope it helps the community  
t3_elf77g,Grafana Dashboard For Monitoring Debezium MySQL Connector,,bigdata,grafana   for monitoring   mysql connector 
t3_eld6n4,What do you call a group of Data Scientists?,"A murder of crows

A caravan of camels

A business of ferrets

A(n) \_\_\_\_\_\_\_\_ of data scientists?

Vote here to decide! [http://allourideas.org/counter\_for\_data\_scientists](http://allourideas.org/counter_for_data_scientists)

Vote multiple times, it is more fun that way. I'm personally campaigning for *n.*

Credit to this tweet for the discourse: [https://twitter.com/chrisalbon/status/1214384871491035136](https://twitter.com/chrisalbon/status/1214384871491035136)",bigdata,what   you call a group of   scientists a   of crows  a caravan of camels  a business of ferrets  an  of   scientists  vote here to     vote multiple times it is more fun that way im personally campaigning for n    to this tweet for the   
t3_elb9sv,Preliminary Results Semantic Analysis (Semanalysis),,bigdata,preliminary results semantic analysis semanalysis 
t3_el74kf,Big Data on AWS,"Anyone ever attend the AWS instructor led course: [Big Data on AWS](https://www.bespoketraining.com/aws-training/big-data-on-aws/)?

&amp;#x200B;

I am thinking of attending this but trying to justify the $2550+GST cost for a 3 day course.",bigdata,big   on aws anyone ever   the aws instructor   course    x   b  i am thinking of   this but trying to justify the     gst cost for a     course
t3_ekugz0,An interview about how the Debezium framework simplifies implementing change data capture for all of your database engines,,bigdata,an interview about how the   framework simplifies implementing change   capture for all of your   engines 
t3_ekkgx0,How can I get 2 Million XML files of a remote machine quickly?,"I have like 2 million XML files on a remote VM I need to get to my local.  File size is less than 1GB, but downloading is taking an eternity.  I tried to tar/untar but it doesn't finish even overnight.  Any advice on doing this faster?",bigdata,how can i get   million xml files of a remote machine quickly i have like   million xml files on a remote vm i   to get to my local  file size is less than  gb but   is taking an eternity  i   to taruntar but it   finish even overnight  any   on   this faster
t3_ekns9s,What is QlikView? – Big Data Path,,bigdata,what is qlikview – big   path 
t3_ekdk8b,spark-submit command builder with live preview,,bigdata,sparksubmit     with live preview 
t3_ejy88i,"Best ""database"" to handle and query large amounts of simple data","Imagine some simple structured data stored as flat TXT files.

The files have very few columns (2 or 3).  Data types are relatively short and just text.

Example:

|KeyID|Textfield1|Textfield2|
|:-|:-|:-|
|1|wqert1$2|asdfadf&amp;@|
|2|Hufykj^(#2)|asdhflskdj57893|
|... n|||

Chalenges are:

There are thousands of these flat TXT files of varying sizes. However, they are all structured in the same way as in the example above.

Some are very long - several million lines/records.

In total there are 10s of billions of records.

&amp;#x200B;

I would like to ""import"" all these files into ONE large ""database"" so they can be queried as ONE dataset.

Clearly any SQL-type transactional database will not be suitable or even needed for this.

Performance will be key and more important than features of the ""database"".  However it would be nice to have access to an easy-to-use data query and visualization tool (think Microsoft Power BI Desktop).

EDIT1:

I guess maybe my question is two-fold:

1. What ""database"" should I be using - i.e. Hadoop etc.?
2. What tools should I be using for querying - i.e. Presto, Hive, etc.?

Keep in mind that simplicity is desired given my limited experience. I am concerned about performance though given the large number of records. The total size if the data is not that large - around 2TB, but the number of records is really large.

What would be the best and easiest way to move forward?   What should I be looking at?

Thanks!",bigdata,best   to     query large amounts of simple   imagine some simple       as flat txt files  the files have very few columns   or      types are relatively short   just text  example          n  chalenges are  there are   of these flat txt files of varying sizes however they are all   in the same way as in the example above  some are very long  several million    in total there are   s of billions of    x   b  i   like to import all these files into one large   so they can be   as one    clearly any sqltype transactional   will not be suitable or even   for this  performance will be key   more important than features of the    however it   be nice to have access to an easytouse   query   visualization tool think microsoft power bi       i guess maybe my question is      what     i be using  ie   etc   what tools   i be using for querying  ie presto hive etc  keep in   that simplicity is   given my   experience i am   about performance though given the large number of   the total size if the   is not that large     tb but the number of   is really large  what   be the best   easiest way to move     what   i be looking at  thanks
t3_ejjcy2,Fighting Overfitting in Deep Learning | ActiveWizards: data science and engineering lab,,bigdata,fighting overfitting in   learning      science   engineering lab 
t3_ejsk19,"Augmented Analytics Developments 2019: Benefits, Barriers, Implementation and Vendors",,bigdata,  analytics        benefits barriers implementation     
t3_eje900,Top events in 2020 for data professionals,,bigdata,top events in      for   professionals 
t3_ejh4wa,AI in 2020: Some predictions,,bigdata,ai in      some   
t3_ej5ag0,ELI5 Apache Pulsar vs Apache Kafka,"Doing a bit of google searching and not finding a simple explanation of what the real differences are, and why to choose Pulsar over Kafka and vice versa.  Any insights?",bigdata,eli  apache pulsar vs apache kafka   a bit of google searching   not   a simple explanation of what the real   are   why to choose pulsar over kafka   vice versa  any insights
t3_ejdhyz,Effectively Store And Process Organization Large Dataset Using Big Data,,bigdata,effectively store   process organization large   using big   
t3_ejctt3,The Sigma Awards 2020,,bigdata,the sigma        
t3_ej9zqi,Benefits of Big Data,,bigdata,benefits of big   
t3_eiybxj,Learn Machine Learning from Scratch 2020,,bigdata,learn machine learning from scratch      
t3_eivxgp,Fast IPv4 to Host Lookups with ClickHouse and PostgreSQL,,bigdata,fast ipv  to host lookups with clickhouse   postgresql 
t3_ej1gnl,Big Data and AI for Real Estate CRMs?,"Hi everyone, 

I'm working on a project currently and I wanted to think of best implementations of AI and Big Data in the CRM industry. How would you implement aspects of Big Data and AI in a generic CRM or a CRM for real estate agents?

So far I have-

1. Predictive analysis to accurately forecast the market prices
2. Analyse the human settlements to understand the area using deep learning. Could help you map our water sources around and beware of natural calamities in advance thereby understanding the variables affecting the price points.

Thank you in advance!",bigdata,big     ai for real estate crms hi everyone   im working on a project currently   i   to think of best implementations of ai   big   in the crm   how   you implement aspects of big     ai in a generic crm or a crm for real estate agents  so far i have      analysis to accurately forecast the market prices   analyse the human settlements to   the area using   learning   help you map our water sources     beware of natural calamities in   thereby   the variables affecting the price points  thank you in  
t3_eiwwxp,Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot,,bigdata,  mysql snapshot for aws   aurora from backup snaphot 
t3_eidmm3,"Reality and misconceptions about big data analytics, data lakes and the future of AI",,bigdata,reality   misconceptions about big   analytics   lakes   the future of ai 
t3_ei2ipi,Debezium MySQL Snapshot From Read Replica And Resume From Master Without GTID,,bigdata,  mysql snapshot from   replica   resume from master without   
t3_ehtavz,An interview with a DataDog engineer about how they build reliable and highly available systems for processing timeseries data in real time and at massive scale,,bigdata,an interview with a   engineer about how they   reliable   highly available systems for processing timeseries   in real time   at massive scale 
t3_ehlx2u,"A new big data platform based on Mesos, Spark, Flink, Kafka and ElasticSearch ... something to follow ...",,bigdata,a new big   platform   on mesos spark flink kafka   elasticsearch  something to follow  
t3_ehq3bx,Should I dive deeper Machine Learning and Deep Learning first or should I start with a new big data technology like MapReduce or Spark ?,"A little background. I just finished university with a bachelors in Computer Science. I have had a couple of courses on Machine Learning which covered both conceptual aspect as well as implementation part and also introduced me to deep learning implementations. As far as big data is concerned, I have had a very brief introduction to Spark and Hadoop. Read about the concepts, tried to create a cluster and ran some basic MapReduce  Jobs. So now, I'm in a dilemma and would really appreciate some advice as to which path to tread upon now such that it would be beneficial both in learning and job opportunities.",bigdata,  i     machine learning     learning first or   i start with a new big   technology like   or spark  a little   i just   university with a bachelors in computer science i have   a couple of courses on machine learning which   both conceptual aspect as well as implementation part   also   me to   learning implementations as far as big   is   i have   a very brief   to spark       about the concepts   to create a cluster   ran some basic    jobs so now im in a       really appreciate some   as to which path to   upon now such that it   be beneficial both in learning   job opportunities
t3_ehotel,"Celebrating 1,000 Employees at Confluent and Looking Towards the Path Ahead",,bigdata,celebrating      employees at confluent   looking   the path   
t3_ehiufb,Spark SQL Data Types Explained,,bigdata,spark sql   types   
t3_ehmvqv,The set of classes and Responsibilities in MapReduce,"Read More : www.facebook.com/seevecoding

Set of Classes :

The User Supply - The user supply reverse the set of java classes and the method provided by java developer for the developing Hadoop MapReduce applications .

The FrameWork Supply - The Frameworks is use for defining the work flow job which is followed by all services.

• The user provide the input location and input format as required by program logic.

• Once a ResourceManager accepts the input and hand over the job to ApplicationManager which jobs into tasks.

• Each task is then assigned to the individual NodeManager .Once the assignment is complete . The NodeManage will start the Map task it performs shuffling,partitioning and sorting for individual Map outputs.

• Once the sorting is complete the Reducer start the merging process, the output is collected.

MapReduce Responsibilities :

1. Developer Responsibilities -

• Setting up the job.
• Specifying the input location.
• Ensuring correct format and location of input.

2. Framework Responsibilities :

• Distributing Job to ApplicationMaster and NodeManager.
• Running the Map Operation.
• Performing the shuffling and sorting operation.
• Reducing Phases (optional)
• Placing output into output directory.
• Informing the user of job completion status.

Click here to Read More : www.facebook.com/seevecoding",bigdata,the set of classes   responsibilities in     more    set of classes   the user supply  the user supply reverse the set of java classes   the     by java   for the       applications   the framework supply  the frameworks is use for   the work flow job which is   by all services  • the user   the input location   input format as   by program logic  • once a resourcemanager accepts the input     over the job to applicationmanager which jobs into tasks  • each task is then   to the     once the assignment is complete  the   will start the map task it performs shufflingpartitioning   sorting for   map outputs  • once the sorting is complete the   start the merging process the output is      responsibilities       responsibilities   • setting up the job • specifying the input location • ensuring correct format   location of input    framework responsibilities   •   job to applicationmaster     • running the map operation • performing the shuffling   sorting operation •   phases optional • placing output into output   • informing the user of job completion status  click here to   more  
t3_ehiz4x,Anyone used Apache Atlas REST API?,"Hello r/bigdata . Has anyone here used Apache Atlas to create a business glossary? I’m doing that at work and wanted to use the REST API python client to make it less of a mundane task. 

However, it’s my first time trying to use a REST API and I’m struggling with it so wanted to ask if anyone has experience using the API to do things like create new terms in the glossary and link entities to those terms?",bigdata,anyone   apache atlas rest api hello    has anyone here   apache atlas to create a business glossary i’m   that at work     to use the rest api python client to make it less of a   task   however it’s my first time trying to use a rest api   i’m struggling with it so   to ask if anyone has experience using the api to   things like create new terms in the glossary   link entities to those terms
t3_eh3i00,8 Top Big Data Analytics Trends That Will Dominate 2020,,bigdata,  top big   analytics   that will        
t3_eh0vfr,Where can I find large data dumps?,I'm just curious if you had any places that I can find large dumps of data to process. I can't find good sources thar aren't behind pay walls,bigdata,where can i   large     im just curious if you   any places that i can   large   of   to process i cant     sources thar arent   pay walls
t3_egrbxm,Debezium MySQL Snapshot From Read Replica With GTID,,bigdata,  mysql snapshot from   replica with   
t3_egpffh,Apache Spark Parallel Program Flows,,bigdata,apache spark parallel program flows 
t3_egd9qe,Anyone have experience with this delta storage Vs Standard s3 or other storage layers? Curious about pros and cons and the importance of acid transactions in your use cases.,,bigdata,anyone have experience with this   storage vs   s  or other storage layers curious about pros   cons   the importance of   transactions in your use cases 
t3_eg61rw,ML – Introduction to Data in Machine Learning,,bigdata,ml –   to   in machine learning 
t3_efp9th,"Tesla's Neural Net can now identify red and green traffic lights, garbage cans and detailed road markings",,bigdata,teslas neural net can now       green traffic lights garbage cans       markings 
t3_eftz62,Big Data Processing : Apache Hive,"Read more : www.facebook.com/seevecoding

**Big Data Processing :**  **Apache Hive** 

So, as we know there are four phases in BigData Processing after data has been inserted and they are -

1. Ingest -Scoop and Flume 
2. Processing - HDFS(Hadoop distributed file system) , HBASE , Hadoop Map Reduce , Spark
3. Analyse - PIG , Apache HIVE , Impala 
4. Access - Hue , Cloudera 

**What is Apache Hive :**

**Apache Hive** is used for analyse of data . Hive Provide a SQL like interface for users to **extract data** from the Hadoop system.

***Example : (Batch Processing)***

SELECT t1.a1 as c1, t2.b1 as c2

FROM t1 JOIN t2 ON (t1.a2 = t2.b2);

Apache Hive uses HBASE or HDFS for its Resource Manage .

**Features of HIVE**  **:**

* Originally developed by Facebook around 2007.
* An open source Apache project.
* High level abstraction layer on the top of MapReduce and Apache Spark.
* Uses HiveQL
* Suitable for structured data.

**Hive Architecture :**

The Major components of Hive architecture are  :  **Hadoop Core Components, Metastore, Driver and Hive clients**.

&amp;#x200B;

**Job Execution Flow in Hive :**

 Receive SQL Query:

1. Parse Hive QL
2. Make optimisations
3. Plan execution
4. Submit job(s) to cluster 
5. Monitor progress
6. Process data in MapReduce or Apache Spark
7. Store the data in HDFS

Read more : www.facebook.com/seevecoding

[Apache Hive Architecture ](https://preview.redd.it/jnrmriaqgy641.png?width=560&amp;format=png&amp;auto=webp&amp;s=95c7c175b661ba2bf0f94e96610bd744ca6b77a6)",bigdata,big   processing  apache hive   more    big   processing   apache hive   so as we know there are four phases in   processing after   has been     they are     ingest scoop   flume    processing      file system  hbase    map    spark   analyse  pig  apache hive  impala    access  hue      what is apache hive   apache hive is   for analyse of    hive   a sql like interface for users to extract   from the   system  example  batch processing  select t a  as c  t b  as c   from t  join t  on t a   t b   apache hive uses hbase or   for its resource manage   features of hive     originally   by facebook         an open source apache project  high level abstraction layer on the top of     apache spark  uses hiveql  suitable for      hive architecture   the major components of hive architecture are      core components metastore     hive clients  x   b  job execution flow in hive    receive sql query    parse hive ql   make optimisations   plan execution   submit jobs to cluster    monitor progress   process   in   or apache spark   store the   in      more     
t3_efskx5,Why is Data Science a Need for Startups?,[https://www.analyticsinsight.net/data-science-need-startups/](https://www.analyticsinsight.net/data-science-need-startups/),bigdata,why is   science a   for startups 
t3_efsbdj,Kafka Delete Topic and its messages,,bigdata,kafka   topic   its messages 
t3_efsa79,List of Spark SQL Window Functions,,bigdata,list of spark sql   functions 
t3_efi0t4,Spark SQL Aggregate Functions,,bigdata,spark sql aggregate functions 
t3_efbr3d,Artificial intelligence vs Machine Learning vs Deep Learning,,bigdata,artificial intelligence vs machine learning vs   learning 
t3_efezbk,Free HDS questionnaire for hiring data scientists,,bigdata,free   questionnaire for hiring   scientists 
t3_effk2n,Why Big Data Developers Are Hire in Large Companies?,,bigdata,why big     are hire in large companies 
t3_eezxza,The Age of A.I. - An inside view into the diverse applications of AI,,bigdata,the age of ai  an   view into the   applications of ai 
t3_eesx3b,An episode about building Materialize for interactive analytics on continuously updated streams of data,,bigdata,an   about   materialize for interactive analytics on continuously   streams of   
t3_eeiz0p,Pondering Distributed Data Lakes Idea | My YouTube video,,bigdata,      lakes    my youtube   
t3_een3eh,Advance MapReduce,"Read More on - [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)

**Advance MapReduce** 

Hadoop MapReduce the data types when it works with user given mappers and reducers. The data is read from files into mapper and emitted by mapper to reducers.This process data is sent back by the reducer .Data emitted by reducer go to output files. **At every step data is stored into java objects.**

**Writable data types :** In the Hadoop environment object that can be put to or received from files and access the network must obey the Writable interface this help to be in Serial form for execution .

**Interfaces in Hadoop - Writable and Writable Comparable** 

**Writable interface -** A writable interface allow Hadoop to read and write the data in Serialized form for transmission . 

interface Writable {

public void readField(DataInput in);

public void write(DataOutput out);

}

**Writable Comparable interface -** A writable comparable extends the writable interface so that the data can be used as a key and not as a value. 

int compareTo(Object what)

int hashCode()

**Data Types in Hadoop :**

Data Types

Functions

Text 

Store String data

IntWritable

Stores Integer data

LongWritable

Stores Long data

FloatWritable

Stores Float data

DoubleWrirtable

Stores Double data

BooleanWritable

Stores Boolean data

ByteWritable

Stores Byte data

NullWritable

Placeholder when value is not needed

Writable

It defines a deserialisation or serialisation protocol.Every data type in Hadoop is Writable.

WritableComparable

It defines a sort order. All key must be of this type (but not value)

**Input Format in MapReduce :**

MapReduce can specify how its input is to be read by defining a InputFormat.

The Table is the lists of the classes of InputFormats provided by Hadoop Framework.

**InputFormat Classes**

**Description** 

Key Value Text Input Format

One key value pair per line.

Text Input Format

Key is the line number and value is the line.

N Line input format

Similar to TextInputFormat, but the difference is that there are N number of lines that makes an input split.

Multiple File Input Format

Input format that aggregates multiple files into one split.

Sequence File Input Formate 

The input file is a Hadoop sequence file which contains a serialised key value pair.

**Output Format in MapReduce :**

**OutputFormat Classes**

**Description** 

TextOutputFormat 

It is the default Output Format and writes  records as lines of text. Each key-value pair is separated by a TAB character.This can be customised by using the mapred.textoutputformat.separator property, The corresponding InputFormat is KeyValueTextInput Format.

SequenceFileOutputFormat

It writes sequence files to save output, it is compact and compressed.

SequenceFileAsBinaryOutputFormat

It writes key and value in raw binary format into a sequential file container.

MapFileOutputFormat

It writes MapFiles as output. The keys in MapFile must be added in an order and the reducer will emit keys in sorted order.

MultipleTextOutputFormat

It writes data to multiple files whose names are derived from output keys and values.

MultipleSequenceFileOutputFormat

It creates output in multiple files in compressed form.

**Distributed Cache :** 

Distributed cache is a Hadoop feature to cache file that are needed by the application.

The Distributed cache helps in :

1. Helps to boost efficiency when a map or reduce task need access to the common data.
2. Allow cluster nod to read the imported files from its local file system instead of retrieving the file from other cluster node.
3. Allow both single files and archived (such as zip and tar.gz).
4. Copies files only to slave nodes. If there are no slave nodes in cluster , distributed cache copies file to the master node.
5. Allow accessed to the cache file from mapper or reducer applications to make sure that the current working directory is added into application path.
6. Allow one to reference the cached files as through they are present in current working directory.

**Using Distributed Cache :**

1. Set up the cache by coping the requisites files to the FileSystem.

 $ bin/hadoop fs  -copyFromLocal   lookup.dat /my app/lookup.dat

1. Set up the application JobConf as shown below 

JobConf job = new JobConf();

DistributedCache.addCacheFile(new URL (“/my app/.lookup.dat#lookup.dat”),job);  

DistributedCache.addCacheArchive(new URL(“/myapp/map.zip”,job);

DistributedCache.addFileToClassPath(new Path(“/myapp/mylib.jar”),job);

DistributedCache.addCacheArchive(new URL (“/myapp/mytar.tar”, job);

DistributedCache.addCacheArchive(new URL (“/myapp/mytgz.tgz”, job);

DistributedCache.addCacheArchive(new URL (“/myapp/mytargz.tar.gz”, job);

1. Use Cached file in mapper or reducer 

public static class MapClass extends MapReduceBase implements Mapper&lt;K,V,K,V&gt;

{

private Path \[\] localArchives,private Path \[\] localFiles;

public void configure (JobConf job){

//Get the cache archives/files

File f = new File(“./map.zip/some/file/in/zip.txt”);

}

public void map ( K key, V value, OutputCollector&lt;K,V&gt;output, Reporter reporter)

throws IOException {

// Use data frown the cache archives/files here 

//........

//........

output.collect(k, v);

}

}

**Joins in MapReduce** **:**

Joins are relational constructs to combine relations. In MapReduce, joins are used to combine two or more datasets.A join is performed either in the map phase or in the reduce phase by taking advantage of MapReduce Sort-Merge architecture.

**The various Join pattern availability in MapReduce are** **:**

1. **Reduce side join** \- It is used for joining two or more large dataset by same foreign key using any kind of join operation.

 **Working of Reduce side join** **-**

* The mapper prepares for the join operation , it takes each input record from every dataset  and it emits a foreign key - record part.
* The reducer perform the join operation , it stores the value of each input group into temporary lists . The temporary list are then iterated over and the record from both sets are joined.

 **When to use Reduce side join -**

1. When multiple dataset are joined by a foreign key.
2. When flexibility is needed to execute any join operation.
3. When large amount of network bandwidth is available.
4. When there is no limitation on the size of dataset.

 **SQL Analogy of Reduce Side Join -**

 **SELECT** userID, user, Location, comments,upVotes **FROM**  user

 **\[INNER\]\[LEFT\]\[RIGHT\]JOIN** 

 comments **ON** users.ID =comments.UserID

 **Output of Reduce Side Join** \- 

 The number of part files equals the number of reduce tasks.

1. **Replicated Join** \- It is a map-side join that works in situations where one of the dataset is small enough to cache .

 **Working of Replicated**  **join** **-**

* It reads all files from the distributed cache and stores them in in-memory lookup tables.The mapper processes each record and joins it with the data stored in memory.
* There is no data shuffled to the Reduce phase.The mapper gives the final output part.

 **When to use Replicated join -**

1. When all dataset except for the largest one can fit into the main memory of each map task that is limited by Java Virtual Machine (JVM) heap size.
2. When there is a need for an inner join or a left outer join , within the large input dataset being the “left” part of the operation.

**SQL Analogy of Replicated Join -**

 **SELECT** userID, user, Location, comments, upVotes **FROM**  users

 **\[INNER\]\[LEFT\]\[RIGHT\]JOIN** 

 comments **ON** users.ID =comments.UserID

**Output of Reduce Side Join** \-

The number of part files equal to number of map tasks.

1. **Composite Join** **-**  It is map side join on very large formatted input datasets sorted and partitioned by a foreign key.

 **Working of Composite join** **-**

* All dataset are divided into the same number of partitions.Each partitions  of dataset is sorted by a foreign key and all the foreign keys reside in associated partition of each dataset.
* Two values are retrieved from the input tuple associated with each dataset ,they are based on the foreign key and the output to the file system.

 **When to use Replicated join -**

1. When all dataset are sufficiently large.
2. When there is need for an inner join or a full outer join.

**SQL Analogy of Replicated Join -**

 **SELECT** userID, user, Location, comments, upVotes **FROM**  users

 **\[INNER\]\[LEFT\]\[RIGHT\]JOIN** 

 comments **ON** users.ID =comments.UserID

**Output of Reduce Side Join** \-

The number of part files equal to number of map tasks.

1. **Cartesian Product** **-** It is nth map side join where every single record is paired up from another dataset.

**Working of Composite join** **-**

* Dataset are split into multiple partitions. Each partition is fed to one or more mapper . A RecordReader reads every record of input split associate with the mapper.
* The mapper simply pairs every record of dataset with every record of all other dataset.

 **When to use Composite join -**

1. When there is need to analyse relationships between all pairs of individual records.
2. When there are no constraints on execution time .

**SQL Analogy of Replicated Join -**

 **SELECT**  **\*** **FROM** user **\[CROSS\] JOIN** comments

**Output of Reduce Side Join** \-

Every possible tuple combination from the input records is represented in the final output .

Read More on - [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)",bigdata,      more on              the   types when it works with user given mappers     the   is   from files into mapper     by mapper to   process   is sent back by the       by   go to output files at every step   is   into java objects  writable   types  in the   environment object that can be put to or   from files   access the network must obey the writable interface this help to be in serial form for execution   interfaces in    writable   writable comparable   writable interface  a writable interface allow   to     write the   in   form for transmission    interface writable   public     in  public     out    writable comparable interface  a writable comparable   the writable interface so that the   can be   as a key   not as a value   int comparetoobject what  int      types in       types  functions  text   store string    intwritable  stores integer    longwritable  stores long    floatwritable  stores float       stores      booleanwritable  stores boolean    bytewritable  stores byte    nullwritable    when value is not    writable  it   a   or serialisation protocolevery   type in   is writable  writablecomparable  it   a sort   all key must be of this type but not value  input format in       can specify how its input is to be   by   a inputformat  the table is the lists of the classes of inputformats   by   framework  inputformat classes      key value text input format  one key value pair per line  text input format  key is the line number   value is the line  n line input format  similar to textinputformat but the   is that there are n number of lines that makes an input split  multiple file input format  input format that aggregates multiple files into one split  sequence file input formate   the input file is a   sequence file which contains a   key value pair  output format in     outputformat classes      textoutputformat   it is the   output format   writes    as lines of text each keyvalue pair is   by a tab characterthis can be   by using the   property the   inputformat is keyvaluetextinput format  sequencefileoutputformat  it writes sequence files to save output it is compact      sequencefileasbinaryoutputformat  it writes key   value in raw binary format into a sequential file container  mapfileoutputformat  it writes mapfiles as output the keys in mapfile must be   in an     the   will emit keys in      multipletextoutputformat  it writes   to multiple files whose names are   from output keys   values  multiplesequencefileoutputformat  it creates output in multiple files in   form    cache      cache is a   feature to cache file that are   by the application  the   cache helps in     helps to boost efficiency when a map or   task   access to the common     allow cluster   to   the   files from its local file system   of retrieving the file from other cluster     allow both single files     such as zip   targz   copies files only to slave   if there are no slave   in cluster    cache copies file to the master     allow   to the cache file from mapper or   applications to make sure that the current working   is   into application path   allow one to reference the   files as through they are present in current working    using   cache     set up the cache by coping the requisites files to the filesystem      fs  copyfromlocal     my      set up the application jobconf as shown below   jobconf job  new jobconf    url “my  ”job      url“myappmapzip”job    path“myappmylibjar”job    url “myappmytartar” job    url “myappmytgztgz” job    url “myappmytargztargz” job    use   file in mapper or     public static class mapclass     implements mapperkvkv    private path   localarchivesprivate path   localfiles  public   configure jobconf job  get the cache archivesfiles  file f  new file“mapzipsomefileinziptxt”    public   map  k key v value outputcollectorkvoutput reporter reporter  throws ioexception    use   frown the cache archivesfiles here       outputcollectk v      joins in     joins are relational constructs to combine relations in   joins are   to combine two or more   join is   either in the map phase or in the   phase by taking   of   sortmerge architecture  the various join pattern availability in   are         join  it is   for joining two or more large   by same foreign key using any   of join operation   working of     join    the mapper prepares for the join operation  it takes each input   from every      it emits a foreign key    part  the   perform the join operation  it stores the value of each input group into temporary lists  the temporary list are then   over   the   from both sets are     when to use     join     when multiple   are   by a foreign key   when flexibility is   to execute any join operation   when large amount of network   is available   when there is no limitation on the size of     sql analogy of     join    select   user location commentsupvotes from  user      join    comments on       output of     join     the number of part files equals the number of   tasks      join  it is a   join that works in situations where one of the   is small enough to cache    working of    join    it   all files from the   cache   stores them in inmemory lookup tablesthe mapper processes each     joins it with the     in memory  there is no     to the   phasethe mapper gives the final output part   when to use   join     when all   except for the largest one can fit into the main memory of each map task that is   by java virtual machine jvm heap size   when there is a   for an inner join or a left outer join  within the large input   being the “left” part of the operation  sql analogy of   join    select   user location comments upvotes from  users      join    comments on      output of     join   the number of part files equal to number of map tasks    composite join   it is map   join on very large   input         by a foreign key   working of composite join    all   are   into the same number of partitionseach partitions  of   is   by a foreign key   all the foreign keys   in   partition of each    two values are   from the input tuple   with each   they are   on the foreign key   the output to the file system   when to use   join     when all   are sufficiently large   when there is   for an inner join or a full outer join  sql analogy of   join    select   user location comments upvotes from  users      join    comments on      output of     join   the number of part files equal to number of map tasks    cartesian    it is nth map   join where every single   is   up from another    working of composite join      are split into multiple partitions each partition is   to one or more mapper  a     every   of input split associate with the mapper  the mapper simply pairs every   of   with every   of all other     when to use composite join     when there is   to analyse relationships between all pairs of       when there are no constraints on execution time   sql analogy of   join    select   from user   join comments  output of     join   every possible tuple combination from the input   is   in the final output     more on  
t3_eedboo,Is there a python package that can read parquet files using predicate pushdown?,"The two big candidates appear to be fastparquet and pyarrow. I'd like to be able to efficiently read parquet files into dataframes but filtering only on the rows I'm interested in.

Fastparquet appears to support row group filtering.

I'd rather not use Spark or any ""heavier"" tools.",bigdata,is there a python package that can   parquet files using     the two big   appear to be fastparquet   pyarrow   like to be able to efficiently   parquet files into   but filtering only on the rows im   in  fastparquet appears to support row group filtering    rather not use spark or any heavier tools
t3_eef8jt,Relationship between Data Mining and Machine Learning,,bigdata,relationship between   mining   machine learning 
t3_edg5yw,"If Data is the New Oil, How to Determine Its Value?",,bigdata,if   is the new oil how to   its value 
t3_ed74xx,How to Use Airflow without Headaches,,bigdata,how to use airflow without   
t3_edhgfl,Setup And Configure Cluster Node Hadoop Installation,,bigdata,setup   configure cluster     installation 
t3_ed79sa,"With Deep Learning, Disney Sorts Through a Universe of Content",,bigdata,with   learning   sorts through a universe of content 
t3_ed9bsr,AI vs statistics vs machine learning,,bigdata,ai vs statistics vs machine learning 
t3_ed4rzf,Career Opportunities in Big Data,,bigdata,career opportunities in big   
t3_ed6cd1,How to scale? Workflow: removing and inserting 100Ks of rows every 30 minutes and do analytics of all data,"For an api I'm using the data that is returned have no unique fields to update them.

Current workflow:

Scheduler fetches data for last 3 days every 30 minutes.

Removes all the data for last 3 days.

Insert all the new data.

&amp;#x200B;

This is the only way I can have original records without any  duplicates.

Currently using a self managed MariaDB server for storing the data.

We are creating realtime statistics from the data that is consumed by our own API.

&amp;#x200B;

Statistics (only 2.5 months):

The table has already over 1,100,000 records and table size is \~650mb.

We are expecting an increasing amount of records.

Current server is a digitalocean droplet with 16gb ram, but I know vertically scaling isn't a solution for longterm.

&amp;#x200B;

I am kinda clueless about proper solution for this kind of statistics, I'm getting lost in all the various options bigdata from google, aws? What stack implementation should be worth looking into? All sounds a lot of time to dig in. What would fit in the most for our needs?

My first thoughts for implementation:

Keep the same workflow, server but do the following:

Aggregating data from realtime to hourly or daily. Pruning all original data that's older than 1 month.

Generating realtime analytics from the aggregated data instead of the original data.

&amp;#x200B;

Any articles, ideas would be really appreciated!",bigdata,how to scale workflow removing   inserting    ks of rows every    minutes     analytics of all   for an api im using the   that is   have no unique   to   them  current workflow    fetches   for last     every    minutes  removes all the   for last      insert all the new    x   b  this is the only way i can have original   without any     currently using a self     server for storing the    we are creating realtime statistics from the   that is   by our own api  x   b  statistics only    months  the table has   over             table size is    mb  we are expecting an increasing amount of    current server is a     with   gb ram but i know vertically scaling isnt a solution for longterm  x   b  i am   clueless about proper solution for this   of statistics im getting lost in all the various options   from google aws what stack implementation   be worth looking into all   a lot of time to   in what   fit in the most for our    my first thoughts for implementation  keep the same workflow server but   the following  aggregating   from realtime to hourly or   pruning all original   thats   than   month  generating realtime analytics from the       of the original    x   b  any articles     be really  
t3_ed0m08,Detecting and Analysing SSH Attacks With ksqlDB (and Elastic and Neo4j),,bigdata,    analysing ssh attacks with     elastic   neo j 
t3_ecq2mx,Finland is making its online AI crash course free to the world,,bigdata,  is making its online ai crash course free to the   
t3_ecrwun,International Data Analysis Olympiad,,bigdata,international   analysis   
t3_ecrj8d,Get a remote job in Data Science,,bigdata,get a remote job in   science 
t3_ecrkf0,"Debezium 1.0.0 has been released, making it the most advanced CDC (Change Data Capture) Java library",,bigdata,      has been   making it the most     change   capture java library 
t3_ecrtur,Is AI about to hit a wall? Facebook's Head of AI responds,,bigdata,is ai about to hit a wall facebooks   of ai   
t3_ecrr9g,Why The Demand Of Big Data Developers Growing Worldwide?,,bigdata,why the   of big     growing   
t3_ecrnwo,Latest Developments in the Global Transportation and Logistics Industry Powered by AI,,bigdata,latest   in the global transportation   logistics     by ai 
t3_ecfut5,The art of joining in Spark,,bigdata,the art of joining in spark 
t3_ecf3kp,Overview of the different approaches to putting Machine Learning (ML) models in production,,bigdata,overview of the   approaches to putting machine learning ml   in   
t3_ecazm8,How to Setup MapReduce Environment,"For more like this - www.facebook.com/seevecoding

[ ](https://preview.redd.it/kou558ased541.png?width=1280&amp;format=png&amp;auto=webp&amp;s=057e5e5a8e677434844b58cf6f385fc53e2f80e4)

Steps :

1. Ensure all Hadoop services are live and running ,You can verify this in two steps .

 First use - $ sudo ups and search for all the five services which you need . Which is NameNode, DataNode, NodeManager,ResourceManager and SecondaryNode  maybe additional services which is run by Hadoop Server these are the one which we require as core services.

1. Uploading Big Data an Small Data .

Command for uploading any data big or small  from local filesystem to HDFS is -

Hadoop fs  -copyFromLocal   sourceFileAddress/fileName.txt  destinationFileAddress/file.txt

**Steps For Building the MapReduce Program :**

1. Determine the data (is WORM or Not) -First, determine that data can be made parallel and can be solved by using MapReduce .

* WORM - Write Once Read Many

1. Design and implement a solution - Second , Design and implement a solution as Mapper and Reducers class with your code.
2. Compile the source code : Third , Compile the source code at Hadoop Core .
3. Package the code - Package the code as a jar executable. 
4. Compile the source code - Configure the application job as the number of mapper and reducer to task and to the number of input and output streams.
5. Load the data - Load the data on usage which was previously available data .

7.  Compile the source code - Launch and Monitors the job.

1. Study the results as needed.

**Hadoop MapReduce Requirements :**

The user or developer is required to set the framework with the following parameters :

* Locations of the job in distributed file system.
* Locations of the job output.
* Input format
* Output format
* The class containing the map function.
* The class contain the reduce function. (Optional)

Read More at - [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)",bigdata,how to setup   environment for more like this       steps     ensure all   services are live   running you can verify this in two steps    first use     ups   search for all the five services which you    which is            maybe   services which is run by   server these are the one which we require as core services      big   an small       for   any   big or small  from local filesystem to   is     fs  copyfromlocal         steps for   the   program       the   is worm or not first   that   can be   parallel   can be   by using      worm  write once   many        implement a solution         implement a solution as mapper     class with your     compile the source       compile the source   at   core    package the    package the   as a jar executable    compile the source    configure the application job as the number of mapper     to task   to the number of input   output streams     the      the   on usage which was previously available        compile the source    launch   monitors the job      the results as        requirements   the user or   is   to set the framework with the following parameters    locations of the job in   file system  locations of the job output  input format  output format  the class containing the map function  the class contain the   function optional    more at  
t3_ecd7sy,Header-detail partition scheme in Hive,"Hello,

 

I need some guidance on how to model the partitions of headers and details of invoices using Hive.

 

Header tables I plan to store partitioned by year, month and day (e.g., year=2019/month=12/day=2)

 

Detail table:

Around 200 million records per day)
Have invoice number (unique) to relate to headers, but no dates included in the detail per se
 

I am thinking about 2 options for detail table:

Partitioning also by year, month, day. This needs a join to be performed in the data ingestion process, before storing. In this scenario, joins between header and details could be penalized because the join is based on the invoice number
Partitioning by a fixed amount of characters of the invoice number (unique). (e.g., for invoice number 123456789 a partition would be ""12345""). I think this approach could be better because invoice numbers are sequential and thus ""tightly"" mapped with the dates (partition scheme of header table)
I also need to manage corner (dirty) cases, like invoice details with no valid invoice id: I'm thinking about a dedicated partition for these cases

 

Does somebody have some suggestions or experience with a related scenario?

 

Thanks a lot",bigdata,  partition scheme in hive hello     i   some   on how to   the partitions of       of invoices using hive       tables i plan to store   by year month     eg         table        million   per   have invoice number unique to relate to   but no     in the   per se    i am thinking about   options for   table  partitioning also by year month   this   a join to be   in the   ingestion process before storing in this scenario joins between         be   because the join is   on the invoice number partitioning by a   amount of characters of the invoice number unique eg for invoice number           a partition   be       i think this approach   be better because invoice numbers are sequential   thus tightly   with the   partition scheme of   table i also   to manage corner   cases like invoice   with no   invoice   im thinking about a   partition for these cases         have some suggestions or experience with a   scenario     thanks a lot
t3_ec6zak,Difference between Data Warehousing and Data Mining,,bigdata,  between   warehousing     mining 
t3_ec2o5j,Introducing FlyteHub — Open Source AI That Scales.,,bigdata,  flytehub — open source ai that scales 
t3_ec2h6j,MapReduce examples,What is a practical example of the MapReduce technique? Would appreciate any book suggestions and pseudo codes.,bigdata,  examples what is a practical example of the   technique   appreciate any book suggestions      
t3_ebud1r,Big Data Applications - A manifestation of the hottest buzzword!,,bigdata,big   applications  a manifestation of the hottest   
t3_ebur2u,"How AI, Big Data and IoT Drive Growth for Applied Materials for 2020?",,bigdata,how ai big     iot   growth for   materials for      
t3_ebumv8,Real-World Examples – How AI is Revolutionizing Top Industries,,bigdata,  examples – how ai is revolutionizing top   
t3_ebpzfp,The Hidden Complexities of Realtime Data Streaming Applications,[https://www.kharekartik.dev/2019/12/11/realtime-data-applications/](https://www.kharekartik.dev/2019/12/11/realtime-data-applications/),bigdata,the   complexities of realtime   streaming applications 
t3_ebsba1,How to Learn Data Science From Scratch,,bigdata,how to learn   science from scratch 
t3_ebib0n,Podcast: Machine learning in finance,,bigdata,  machine learning in finance 
t3_ebj42l,"An interview about how the Marquez platform for metadata management powers data lineage tracking, data discovery, and health reporting at WeWork",,bigdata,an interview about how the marquez platform for   management powers   lineage tracking       health reporting at wework 
t3_ebldq2,Blockchain as a Distributed File System,"# Introduction

In this article, we propose a blockchain network that acts as a centralized **append-only** distributed file system (DFS) such as Hadoop Distributed File System (HDFS) or Google File System (GFS).  The potential advantages of blockchain as a distributed file system (BaaDFS) include:

* **High availability**: a user can tolerate failures of ⅓ full nodes and enjoy almost 100% high availability on read unless all nodes are down without worry about the single-point of failure of metadata servers (such as namenodes in HDFS) of traditional systems;
* **High data integrity**: a node could fully validate the integrity of any piece of the data written to the DFS.  The validation can be very efficient: given the position of the data (filename, offset, len), the cost of validating the data has the same order of the cost of client read operation.
* **Highly trustworthy storage for clients**: a writer can check the integrity of the written data and ensure immunity, and any reader could verify such integrity and immunity.

Currently, the blockchain network is designed for private use (as the target DFS is centralized), but it should be able to extend to public/consortium use with some modifications.

# DFS Semantics Supported

The BaaDFS supports single-writer multiple-reader append-only DFS semantics, which has been widely used in existing DFSs such as HDFS and GFS.  To be more specific, for a given file, we only allow to write from a writer at any time, but multiple readers may open and read the same file simultaneously.  The writer could only append data to a file.  Such DFS semantics is highly suitable for high-performance batch processing.

# Client Operation Semantics

The BaaDFS supports the following file operations from client perspective:

* create(filename): create a file for write operation, return a file object upon success or throw if filename exists or other IO exception.
* open(filename, isReadonly): open a file for read or read/write, return a file object upon success or throw if filename is not found or other IO exception.
* read(file\_object, buffer, offset, len): read the data of the file starting from offset and up to len bytes to buffer.  Return the number of bytes read or -1 if the EOF is reached.
* append(file\_object, buffer): append the data from buffer to the end file. Throw if a concurrent append is detected.
* flush(file\_object): flush all cached append data to the network, and wait until the data are visible to all readers.  Throw if concurrent append/flush is detected.
* close(file\_object): flush if necessary and close the file.

# Representation of the File System as a Blockchain Ledger

Similar to HDFS/GFS, a file in BaaDFS is represented as a list of data chunks

&amp;#x200B;

https://preview.redd.it/1381tefbb2541.png?width=465&amp;format=png&amp;auto=webp&amp;s=8de82b1a6a9df7667ae8194f69d07cf2f0a510d7

where the file content is equally divided into chunks with the same size (chunk\_size) except the last chunk, whose size, namely, last\_chunk\_size, is file\_size % chunk\_size.

In the proposed BaaDFS, a chunk of data is stored as part of a blockchain block, where the block consists of

* a header
* a list of write transactions, where each transaction is

tx := (filename, chunk\_idx, chunk\_num, chunk\_data\_0, chunk\_data\_1, chunk\_data\_${chunk\_num - 1}),

which means that the data chunk\_data\_0, chunk\_data\_1, …, chunk\_data\_${chunk\_num - 1} are written to the file with the offset starting from chunk\_idx \* chunk\_size.  The size of chunk\_data’s in a transaction must be chunk\_size, except the last one, whose size,  last\_chunk\_size &lt;= chunk\_size.  The resulting new file\_size after applying the write transaction becomes  (chunk\_idx + chunk\_num - 1) \* chunk\_size + last\_chunk\_size.  The hash of the list of transactions (likely in a Merkle tree way) will be stored in a field of the block header as conventional blockchain does.

Note that since we implement an append-only file system, the write transactions must satisfy the following constraints (assuming file\_size’ is the pre-write file size, and file\_size is the post-write file size)

* (Chunks unchanged except the last one) chunk\_idx &gt; file\_size’ // chunk\_size  (// is the integer division operator)
* (Last chunk write must be append) if chunk\_idx == file\_size’ // chunk\_size + 1, then chunk\_data\_0 must contain last\_chunk\_data’, where last\_chunk\_data’ is the last chunk of the file before the write transaction.

To lookup the chunk, we define a chunk\_info, which tells where the chunk data can be read as

chunk\_info := (block\_index, tx\_index, tx\_chunk\_index)

where block\_index is the height of the block that contains the corresponding write operation/transaction, tx\_index is the position of the write transaction in the block, and tx\_chunk\_index is the position of the chunk\_data in the transaction.

As a result, given the history of the ledger, i.e., blocks, a reader could fully read any part of the file by a list of chunk\_info’s together with file\_size and chunk\_size, where the list of chunk\_info can be efficiently implemented as a Merkle tree (likely an accumulator) for fast update (only the last item), append, and read.

The tuple of (file\_size, chunk\_size, and chunk\_info\_trie\_hash) is defined as the metadata of the file as:

metadata := (file\_size, chunk\_size, chunk\_info\_trie\_hash),

and the state of the ledger given a block is basically a mapping as:

\`state := filename -&gt; metadata\`

which could be implemented as another Merkle tree (e.g., Patricia Merkle Tree or Sparse Merkle Tree), whose hash value will be stored in the header of the block.

Summarizing the aforementioned details, the diagram of the ledger of a BaaDFS looks like

https://preview.redd.it/f0rt7lbdb2541.png?width=1497&amp;format=png&amp;auto=webp&amp;s=3bbd5c8e59ccf07dacd7c87c317ff0b6eaa3d5dd

A good property of such a file system representation is that given the hash of the state trie or a block hash, a reader or writer could uniquely determine a snapshot of the filesystem and check the integrity or immunity of the filesystem.

# Components of the BaaDFS Network

In this subsection, we illustrate a blockchain network and its components for the BaaDFS

* **Full node**: A node that maintains a replica of the blockchain ledger.  For better performance, the node may be implemented as a cluster - a group of servers that act as a single node in the network. The full nodes are connected via a network protocol (P2P or membership managed by a configuration service such as ZooKeeper). Upon observing a new valid block is produced, it will synchronize the block from its peers and append it to the ledger.  The nodes may not necessarily trust each other in the network.
* **Consensus**: We will employ a fast-finality consensus, which may be Paxos/Raft for private/consortium usage or Tendermint for consortium/public usage.
* **Finalized block**: A finalized block is that a block that reaches finality and will be irreversible by the consensus.  This means that if a block is finalized, all the write transactions of the block (and previous blocks) and the resulting file content, i.e., bytes in offset \[0, filesize\], will be immutable and be consistent among new readers.  Furthermore, we denote the finalized block with the highest index and its index as LAST\_FINALIZED\_BLOCK and LAST\_FINALIZED\_BLOCK\_INDEX, respectively.
* **DFSClient**: A DFSClient is a client run by a user that performs the interface of aforementioned file operations and translates the operations to blockchain operations. It will connect to one or multiple full nodes in the network.

# Example Implementations of Supported Operations

We assume a full node has the following RPC service to a DFSClient:

* QUERY\_FINALIZED\_INFO: Query and return LAST\_FINALIZED\_BLOCK or LAST\_FINALIZED\_BLOCK\_INDEX;
* QUERY\_METADATA: Given a filename and a block index/hash, return the metadata of the filename of the block;
* QUERY\_CHUNK\_INFO: Given a chunk\_info\_trie\_hash and chunk\_index, return a chunk\_info;
* QUERY\_CHUNK\_DATA: Given a chunk\_info, return a chunk data;
* SUBMIT\_TX\_AND\_WAIT: Given a transaction (tx), return success if the tx is included by a block before or equal to FINALZED\_BLOCK, otherwise error.

Given above RPCs, the file operations can be supported by a DFSClient in BaaDFS as follows:

* open(filename, isReadonly): The DFSClient will look up the metadata of file in the LAST\_FINALIZED\_BLOCK from the network, store the metadata in a file\_object, and return the file\_object.
* read(file\_object, buf, offset, len): The DFSClient will look up the chunk\_info’s associated with the chunk\_info\_trie\_hash from the cached metadata and the read range of the file.  For each chunk\_info, DFSClient will read the corresponding chunk data, write the data to the user-provided buffer, and return to the user.
* write(file\_object, buf): An optimized DFSClient will likely cache all contents from a write operation in the internal buffer and return immediately. After collecting several full chunks, the DFSClient will issue an actual write tx.  This will reduce the number of writing tx on the last chunk with size &lt; chunk\_size, which requires read-modify-write and could be expensive.
* flush(file\_object): This will forcibly issue write transactions and synchronously wait until the transaction is finalized (i.e., included by a finalized block).
* close(file\_object): The operation will call flush(file\_object) and destroy the file\_object.

# Advantages over HDFS/GFS

* **High availability**: Consider the network adopts a BFT consensus that tolerates up to f byzantine failures with 3f + 1 full nodes
   * The network can continuously perform writing even f full nodes have byzantine failures;
   * For consistent read, as long as the DFSClient reads the metadata of the file in the LAST\_FINALIZED\_BLOCK from the network and read the content of the file from a full node with LAST\_FINALIZED\_BLOCK, the network will continue serving reading.  Suppose a reader needs to read at least f + 1 full nodes to determine LAST\_FINALIZED\_BLOCK, read operation can tolerate 2f byzantine failures;
   * If the consistency of read can be relaxed, the DFSClient may just read the metadata of the file in the last FINALIZED\_BLOCK from any node and read the content of the file from the same full node, the network will continue serving reading until all full nodes are down.
* **High data integrity**: Given the position of the data to be validated (filename, offset, len), the node could validate the integrity of the data as follows (assuming the LAST\_FINALIZED\_BLOCK and its hash is valid (agreed by consensus)):

1. Read the metadata of the file from the state\_trie\_hash of LAST\_FINALIZED\_BLOCK, and validate all cryptographic proofs that the metadata is indeed in the state\_trie.
2. Read the chunk\_info’s from the chunk\_info\_trie\_hash in the metadata, and validate all cryptographic proofs that the chunk\_info’s are included in the chunk\_info\_trie.
3. Read the data chunk for each chunk\_info, and verify that each data chunk is included in the corresponding blocks.
4. Validate the blocks containing the data chunks are part of the history of the ledger (i.e., previous blocks of LAST\_FINALIZED\_BLOCK)

Note that obtaining cryptographic proofs of steps 1-3 only traverses O(log(|tree|)) elements in the Merkle tree, while for step 4, a standard hashed linked list of blocks may take linear time to cryptographically verify if a block is ahead of LAST\_FINALIZED\_BLOCK in the ledger.  An improvement can be done by using a Merkle tree accumulator to store all the blocks as also adopted by Facebook Libra, and as a result, verifying the block relationship can be done in O(log(|blocks|)) time.

* **Highly trustworthy storage for clients**: A writer can check the integrity of the written data according the cryptographic proofs of the blockchain without trusting the node that writes the data. For readers, assuming no writer challenges the proofs, any reader could use the proofs and verify such integrity and immunity of data from any node.  Again, a reader only needs to trust the consensus and the ledger instead of a specific node.  As a comparison, for traditional systems such as HDFS/GFS, we have to trust the systems are properly implemented and operated (e.g., a chunk of a file in a datanode in HDFS is corrupted (with checksum disabled) or the datanode is hacked, and the DFSClient has no way to verify the corrupted data after reading the chunk from the datanode).

# Scalability

## Scalability on Storage

To scale storage, a full node can be implemented as a cluster (server farms) and the block can be distributedly stored on the servers in the cluster. The cluster may or may not implement HA.  If an HA feature is implemented, the full node itself may replicate the blocks to multiple servers in the cluster.

## Scalability on Read

Similar to HDFS/GFS, scalability on read can be achieved in the way that a full node will respond to a data chunk read request with an IP address of the server that stores the actual data chunk in the cluster, and the following data read operation will be performed on that server instead of the full node itself.

## Scalability on Write

All full nodes must reach the same view on all the blocks, and if the amount of requests of write operation is high and the blocks are large, synchronizing the blocks among the full nodes may be costly or even prohibited.  To optimize write performance, we could have the following optimizations:

* **(Transaction Submit Optimization)**. Instead of submitting the write transactions to the full node, a full node can instruct a DFSClient with a server in the cluster that receives the write transactions.
* **(Transaction Broadcast Optimization)**.  Conventional public blockchain network using Gossip protocol to broadcast transactions, which create extra traffic and is inefficiency.  For a private/consortium blockchain, we could implement a pipeline protocol between the full nodes, where each full node assigns a server in its cluster to receive a write transaction and forwards the write transaction to another server in the next cluster in the pipeline similar to HDFS/GFS does.
* **(Block Broadcast Optimization)**.  When broadcasting a valid block, a block producer may assume most of the write transactions are already synchronized among full nodes, and thus it will only broadcast a compact version of the block where all transactions are replaced by their hashes in the compacted block.  Upon receiving a compacted block, a node will check the existence of the transactions in the cluster, and if a transaction is missing, the node will instruct one of its servers to download the transaction from the peer. As a result, if multiple transactions are missing, downloading the missing transactions can be done in a parallel fashion.
* **(Parallel Blockchains)**. Multi-chain/Sharding technology can be employed to increase the throughput and storage capacity (such as Boson Consensus).  One extreme case is that each file is represented by a chain and a root chain only collects the hashes of the updated tips of the sub-chains for newly changed files.

# Further Enhancements

* **Mutual exclusion between writers upon open**: When opening a file for write or create operation, the operation will be prohibited if the file has  already been open for write by another writer.
* **Deletion**: Support delete(file\_object).  This operation prevents the file from opening, but the content of the file may still be accessible in the blockchain ledger.
* **Access control**: The network can define the access (read/write) rights of each file.
* **Directory:** Directory objects can be implemented in addition to file objects.
* **Multiple writers**:  We may allow multiple writers that append to a file concurrently. If multiple appends are called by multiple writers, we only ensure that the append is atomic (i.e., the appended data will not be interleaved by other data) if the data size is smaller than a threshold (e.g., chunk size).
* **Write-any semantics**:  Support write(file\_object, buf, offset, len) operation.  Note that any write operation with len greater than a threshold may not be atomic if multiple writers write the same part.  Similarly, truncate(file\_object) operation may also supported.
* **Quota management**: The system can limit the space used by a user and prevent spamming.
* **Garbage collect (GC)**: Garbage data on blocks may be produced if:
   * A file is deleted; or
   * A write transaction overwrites the last chunk of previous write transaction, and thus the overwritten last chunk can be discarded.

To discard the garbage and reclaim the storage, a GC can be implemented by creating a new block that reclaims the space of the first unGCed block.  This will replay the transactions of the first unGCed block (starting from genesis block) by updating the chunk\_info trie accordingly without performing actual write.  This will also increase the index of the first unGCed, which can be written in a field in the block header.

# Further Extensions

We may extend the idea to blockchain as a distributed key-value store (BaaDKYS).",bigdata,blockchain as a   file system     in this article we propose a blockchain network that acts as a       file system   such as     file system   or google file system gfs  the potential   of blockchain as a   file system       high availability a user can tolerate failures of ⅓ full     enjoy almost     high availability on   unless all   are   without worry about the singlepoint of failure of   servers such as   in   of   systems  high   integrity a     fully   the integrity of any piece of the   written to the    the   can be very efficient given the position of the   filename offset len the cost of   the   has the same   of the cost of client   operation  highly trustworthy storage for clients a writer can check the integrity of the written     ensure immunity   any     verify such integrity   immunity  currently the blockchain network is   for private use as the target   is   but it   be able to   to publicconsortium use with some       semantics    the   supports singlewriter       semantics which has been     in existing   such as     gfs  to be more specific for a given file we only allow to write from a writer at any time but multiple   may open     the same file simultaneously  the writer   only     to a file  such   semantics is highly suitable for highperformance batch processing   client operation semantics  the   supports the following file operations from client perspective   createfilename create a file for write operation return a file object upon success or throw if filename exists or other io exception  openfilename   open a file for   or   return a file object upon success or throw if filename is not   or other io exception    buffer offset len   the   of the file starting from offset   up to len bytes to buffer  return the number of bytes   or   if the eof is      buffer   the   from buffer to the   file throw if a concurrent   is    flushfileobject flush all       to the network   wait until the   are visible to all    throw if concurrent   is    closefileobject flush if necessary   close the file   representation of the file system as a blockchain    similar to   a file in   is   as a list of   chunks  x   b    where the file content is equally   into chunks with the same size chunksize except the last chunk whose size namely lastchunksize is filesize  chunksize  in the     a chunk of   is   as part of a blockchain block where the block consists of   a    a list of write transactions where each transaction is  tx  filename   chunknum           which means that the       …      are written to the file with the offset starting from    chunksize  the size of  ’s in a transaction must be chunksize except the last one whose size  lastchunksize  chunksize  the resulting new filesize after applying the write transaction becomes     chunknum     chunksize  lastchunksize  the hash of the list of transactions likely in a merkle tree way will be   in a   of the block   as conventional blockchain    note that since we implement an   file system the write transactions must satisfy the following constraints assuming filesize’ is the prewrite file size   filesize is the postwrite file size   chunks   except the last one    filesize’  chunksize   is the integer   operator  last chunk write must be   if    filesize’  chunksize    then   must contain  ’ where  ’ is the last chunk of the file before the write transaction  to lookup the chunk we   a chunkinfo which tells where the chunk   can be   as  chunkinfo         where   is the height of the block that contains the   write operationtransaction   is the position of the write transaction in the block     is the position of the   in the transaction  as a result given the history of the   ie blocks a     fully   any part of the file by a list of chunkinfo’s together with filesize   chunksize where the list of chunkinfo can be efficiently   as a merkle tree likely an accumulator for fast   only the last item        the tuple of filesize chunksize   chunkinfotriehash is   as the   of the file as     filesize chunksize chunkinfotriehash    the state of the   given a block is basically a mapping as  state  filename     which   be   as another merkle tree eg patricia merkle tree or sparse merkle tree whose hash value will be   in the   of the block  summarizing the     the   of the   of a   looks like    a   property of such a file system representation is that given the hash of the state trie or a block hash a   or writer   uniquely   a snapshot of the filesystem   check the integrity or immunity of the filesystem   components of the   network  in this subsection we illustrate a blockchain network   its components for the     full   a   that maintains a replica of the blockchain    for better performance the   may be   as a cluster  a group of servers that act as a single   in the network the full   are   via a network protocol p p or membership   by a configuration service such as zookeeper upon observing a new   block is   it will synchronize the block from its peers     it to the    the   may not necessarily trust each other in the network  consensus we will employ a fastfinality consensus which may be paxosraft for privateconsortium usage or   for consortiumpublic usage    block a   block is that a block that reaches finality   will be irreversible by the consensus  this means that if a block is   all the write transactions of the block   previous blocks   the resulting file content ie bytes in offset   will be immutable   be consistent among new    furthermore we   the   block with the highest     its   as       respectively    a   is a client run by a user that performs the interface of   file operations   translates the operations to blockchain operations it will connect to one or multiple full   in the network   example implementations of   operations  we assume a full   has the following rpc service to a       query   return   or      given a filename   a block   return the   of the filename of the block  querychunkinfo given a chunkinfotriehash     return a chunkinfo    given a chunkinfo return a chunk      given a transaction tx return success if the tx is   by a block before or equal to   otherwise error  given above rpcs the file operations can be   by a   in   as follows   openfilename   the   will look up the   of file in the   from the network store the   in a fileobject   return the fileobject    buf offset len the   will look up the chunkinfo’s   with the chunkinfotriehash from the       the   range of the file  for each chunkinfo   will   the   chunk   write the   to the   buffer   return to the user  writefileobject buf an     will likely cache all contents from a write operation in the internal buffer   return   after collecting several full chunks the   will issue an actual write tx  this will   the number of writing tx on the last chunk with size  chunksize which requires       be expensive  flushfileobject this will forcibly issue write transactions   synchronously wait until the transaction is   ie   by a   block  closefileobject the operation will call flushfileobject     the fileobject     over     high availability   the network   a bft consensus that tolerates up to f byzantine failures with  f    full       the network can continuously perform writing even f full   have byzantine failures     for consistent   as long as the     the   of the file in the   from the network     the content of the file from a full   with   the network will continue serving    suppose a     to   at least f    full   to       operation can tolerate  f byzantine failures     if the consistency of   can be   the   may just   the   of the file in the last   from any       the content of the file from the same full   the network will continue serving   until all full   are    high   integrity given the position of the   to be   filename offset len the       the integrity of the   as follows assuming the     its hash is     by consensus      the   of the file from the statetriehash of       all cryptographic proofs that the   is   in the statetrie     the chunkinfo’s from the chunkinfotriehash in the       all cryptographic proofs that the chunkinfo’s are   in the chunkinfotrie     the   chunk for each chunkinfo   verify that each   chunk is   in the   blocks     the blocks containing the   chunks are part of the history of the   ie previous blocks of    note that obtaining cryptographic proofs of steps    only traverses ologtree elements in the merkle tree while for step   a       list of blocks may take linear time to cryptographically verify if a block is   of   in the    an improvement can be   by using a merkle tree accumulator to store all the blocks as also   by facebook libra   as a result verifying the block relationship can be   in ologblocks time   highly trustworthy storage for clients a writer can check the integrity of the written     the cryptographic proofs of the blockchain without trusting the   that writes the   for   assuming no writer challenges the proofs any     use the proofs   verify such integrity   immunity of   from any    again a   only   to trust the consensus   the     of a specific    as a comparison for   systems such as   we have to trust the systems are properly       eg a chunk of a file in a   in   is   with checksum   or the   is     the   has no way to verify the     after   the chunk from the     scalability   scalability on storage  to scale storage a full   can be   as a cluster server farms   the block can be     on the servers in the cluster the cluster may or may not implement ha  if an ha feature is   the full   itself may replicate the blocks to multiple servers in the cluster   scalability on    similar to   scalability on   can be   in the way that a full   will   to a   chunk   request with an ip   of the server that stores the actual   chunk in the cluster   the following     operation will be   on that server   of the full   itself   scalability on write  all full   must reach the same view on all the blocks   if the amount of requests of write operation is high   the blocks are large synchronizing the blocks among the full   may be costly or even    to optimize write performance we   have the following optimizations   transaction submit optimization   of submitting the write transactions to the full   a full   can instruct a   with a server in the cluster that receives the write transactions  transaction   optimization  conventional public blockchain network using gossip protocol to   transactions which create extra traffic   is inefficiency  for a privateconsortium blockchain we   implement a pipeline protocol between the full   where each full   assigns a server in its cluster to receive a write transaction     the write transaction to another server in the next cluster in the pipeline similar to      block   optimization  when   a   block a block   may assume most of the write transactions are     among full     thus it will only   a compact version of the block where all transactions are   by their hashes in the   block  upon receiving a   block a   will check the existence of the transactions in the cluster   if a transaction is missing the   will instruct one of its servers to   the transaction from the peer as a result if multiple transactions are missing   the missing transactions can be   in a parallel fashion  parallel blockchains   technology can be   to increase the throughput   storage capacity such as boson consensus  one extreme case is that each file is   by a chain   a root chain only collects the hashes of the   tips of the subchains for newly   files   further enhancements   mutual exclusion between writers upon open when opening a file for write or create operation the operation will be   if the file has    been open for write by another writer    support    this operation prevents the file from opening but the content of the file may still be accessible in the blockchain    access control the network can   the access   rights of each file      objects can be   in   to file objects  multiple writers  we may allow multiple writers that   to a file concurrently if multiple   are   by multiple writers we only ensure that the   is atomic ie the     will not be   by other   if the   size is smaller than a   eg chunk size  writeany semantics  support writefileobject buf offset len operation  note that any write operation with len greater than a   may not be atomic if multiple writers write the same part  similarly truncatefileobject operation may also    quota management the system can limit the space   by a user   prevent spamming  garbage collect gc garbage   on blocks may be   if     a file is   or     a write transaction overwrites the last chunk of previous write transaction   thus the overwritten last chunk can be    to   the garbage   reclaim the storage a gc can be   by creating a new block that reclaims the space of the first   block  this will replay the transactions of the first   block starting from genesis block by   the chunkinfo trie   without performing actual write  this will also increase the   of the first   which can be written in a   in the block     further extensions  we may   the   to blockchain as a   keyvalue store  
t3_ebiyc0,Realtime Data in Apache Druid — Choosing the Right Strategy,[https://www.kharekartik.dev/2019/12/12/realtime-data-in-apache-druid/](https://www.kharekartik.dev/2019/12/12/realtime-data-in-apache-druid/),bigdata,realtime   in apache   — choosing the right strategy 
t3_ebefsu,MapReduce Jobs and Essentials,"Facebook page : [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)

Blog (Medium) : [www.medium.com/@seevee](https://www.medium.com/@seevee)

&amp;#x200B;

https://preview.redd.it/1libm3pzlz441.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=9da7d5ac416761cc962821a65398ede05fd8343a

**MapReduce Essentials** 

The job input is specified in key - value pairs ,  Each job consist of two stages 

1. A user define Map function is applied to each input record to produce the list of intermediate  key - value pairs.
2. A user define reduce function is called once for each distinct key in the map output.

The essentials for each MapReduce phase are as follows :

1. The number of reduce tasks can be defined by the users.
2. Each reduced task is assigned a set of record groups that is, intermediate records corresponding to a group of keys .
3. For each group , a user defined reduce function is applied to the record values.
4. The record tasks are read from every map task, and each read returns the groups for that reduce task.

Reduce phase cannot start until all mappers has finished processing.  


**Map Reduce Job :**

A job is a MapReduce program that causes multiple map and functions to run parallelly over the life of program. Many copies of Map and Many copies of Reduce  function are forked for parallel processing across the input dataset .

A task is Map or Reduce function executed on the subset of this data .

* Application master is responsible for the execution of a single application or MapReduce job.
* It divide the job requests into tasks and assigns them to NodeManagers running on the slave nodes.
* The NodeManager has dynamical created resource containers.The size of the container depends upon the number of the resources it contains such as memory,CPU, Disk I/O .
* It executes each active Map reduce task by launching this container when instructed to by MapReduce Application master.

**MapReduce And Associated Tasks :**

* Map process : An initial injection and transform step where initial input records are processed in parallel.
* Reduce process : An aggregation summarization step in which all associate records are processed together.
* Node Manager : keeps track of individual map tasks and can run in parallel.
* Application Master : Keep track of all MapReduce task.

&amp;#x200B;

Read More :

Facebook page : [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)

Blog (Medium) : www.medium.com/@seevee",bigdata,  jobs   essentials facebook page    blog      x   b      essentials   the job input is   in key  value pairs   each job consist of two stages     a user   map function is   to each input   to   the list of    key  value pairs   a user     function is   once for each   key in the map output  the essentials for each   phase are as follows     the number of   tasks can be   by the users   each   task is   a set of   groups that is       to a group of keys    for each group  a user     function is   to the   values   the   tasks are   from every map task   each   returns the groups for that   task    phase cannot start until all mappers has   processing     map   job   a job is a   program that causes multiple map   functions to run parallelly over the life of program many copies of map   many copies of    function are   for parallel processing across the input     a task is map or   function   on the subset of this      application master is responsible for the execution of a single application or   job  it   the job requests into tasks   assigns them to   running on the slave    the   has     resource containersthe size of the container   upon the number of the resources it contains such as memorycpu   io   it executes each active map   task by launching this container when   to by   application master        tasks    map process  an initial injection   transform step where initial input   are   in parallel    process  an aggregation summarization step in which all associate   are   together    manager  keeps track of   map tasks   can run in parallel  application master  keep track of all   task  x   b    more   facebook page    blog    
t3_ebcq4p,What is a Data Catalog and Why Should You Even Care?,"**Two data scientists walk into a library at the end of a long day…**

**Data scientist #1 to the librarian**: “Can I get a copy of this book on statistical methods?” *Goes on to share the name of the obscure book.*

**Data scientist #2 to Data scientist #1**: “They’ll never be able to find that book.”

The librarian clacks away on the keyboard for a couple of seconds before replying:

“Found it! Here are the details of its author, publishing house and borrowing history. Oh, and someone left a comment saying they found it super useful for understanding logistic regressions. I can grab it for you in a jiffy.”

**Data scientist #1 to Data scientist #2:** “Ummmm… why can’t the same thing happen with our data?”

That's what a data catalog is here for! With the need of data and metadata management and collaboration, data catalogs are increasingly becoming relevant. Read what are data catalogs and why should data teams care about them here: [https://humansofdata.atlan.com/2019/11/what-is-data-catalog/](https://humansofdata.atlan.com/2019/11/what-is-data-catalog/)",bigdata,what is a   catalog   why   you even care two   scientists walk into a library at the   of a long  …    scientist   to the librarian “can i get a copy of this book on statistical  ” goes on to share the name of the obscure book    scientist   to   scientist   “they’ll never be able to   that book”  the librarian clacks away on the   for a couple of   before replying  “  it here are the   of its author publishing house   borrowing history oh   someone left a comment saying they   it super useful for   logistic regressions i can grab it for you in a jiffy”    scientist   to   scientist   “ummmm… why can’t the same thing happen with our  ”  thats what a   catalog is here for with the   of       management   collaboration   catalogs are increasingly becoming relevant   what are   catalogs   why     teams care about them here 
t3_ebf36z,"Envoy protocol filter for Kafka, meshed",,bigdata,envoy protocol filter for kafka   
t3_ebce2l,"As someone with a non CS degree, coursing a master's in Big Data. What technologies/skills should I learn on the side?","So I am an aerospace engineer, but I am making a change to data science/programming. During my career my programming experience was mostly limited to math and physics, so I don't know much about tech stacks and such. However, I have done both OOP Java courses from Helskinki's MOOC, played around with Python, lots of matlab and some R now in clas. and as I mentioned I am currently studying a master's in big data.

My question is, what else can I learn to complement this master's and be more marketable? I have considered completing The Odin Project as I think it gets you to a point where you can be a software developer. But would that mesh well with the data science industry, or would I be better off complementing my master's some other way? 

I would love to hear your opinions.",bigdata,as someone with a non cs   coursing a masters in big   what technologiesskills   i learn on the   so i am an aerospace engineer but i am making a change to   scienceprogramming   my career my programming experience was mostly   to math   physics so i   know much about tech stacks   such however i have   both oop java courses from helskinkis mooc     with python lots of matlab   some r now in clas   as i   i am currently   a masters in big    my question is what else can i learn to complement this masters   be more marketable i have   completing the   project as i think it gets you to a point where you can be a software   but   that mesh well with the   science   or   i be better off complementing my masters some other way   i   love to hear your opinions
t3_ebabu1,What is the difference between big data and cloud computing?,,bigdata,what is the   between big       computing 
t3_eaz94b,"Data-informed, data-driven or data centric? What are the differences?",,bigdata,    or   centric what are the   
t3_eaxyov,Distributed Processing : Map Reduce,"[Distributed Processing : Map Reduce](https://www.facebook.com/seevecoding)

MapReduce is the programming model that simultaneously process and analyses the huge data set logically into separate clusters. While “Map Sort the data “ , “Reduce segregates it into logical clusters,” thus removing g the bad data and retaining the necessary information.

&amp;#x200B;

[ MapReduce Analogy ](https://preview.redd.it/7p4jwu4u8s441.jpg?width=1488&amp;format=pjpg&amp;auto=webp&amp;s=05211c14e830c96baf9d61ed4582e12673bc29d2)

**Why MapReduce :**

Problems before MapReduce :

1. Huge amount of data were stored into single server.
2. The threat of data loss,challenge the data backup and reduce scalability resulted into issue snowballing into crisis of sorts.

To counter this Google introduce MapReduce in 2004 and the analysis of data was done into 8-10 mins. Rather taking 8-10 days.  


Queries could run simultaneously on multiple servers, search results could belogically  integrated and data could be analysed in real time .

USP of MapReduce is it’s fault tolerance and scalability.

The key reason for Mapping and then Reducing is to speed up the job, this enables the parallelism.

&amp;#x200B;

[ Word Count Example ](https://preview.redd.it/hnaov4ly8s441.png?width=1344&amp;format=png&amp;auto=webp&amp;s=673dc82ab86ad754225478ee29928f4af575dbc7)

**Map Execution Phases :**

Map Execution phases consist of five phases :

1. Map Phase 

* Read assigned input split from HDFS .
* Parses input into records as key-value pairs.
* Applies map function to each record 
* Inform master node of its completion.

 2. Partition phase

*  Each mapper must determine which reducer will receive each of the outputs.
*  For any key, the destination partition in the same.
*  Number of partitions = Number of reducers.

1. Shuffle phase

* Fetches input data from all map tasks for the portion corresponding to reduce task bucket 

1. Sort phase

* Merge sorts all map outputs into a single run.

1. Reduce phase

* Applies user-defined reduce function to the merged run .

&amp;#x200B;

https://preview.redd.it/wkw408g49s441.png?width=999&amp;format=png&amp;auto=webp&amp;s=a2dbdc17064438b3cb11ee25690791ed94e24d15",bigdata,  processing  map        is the programming   that simultaneously process   analyses the huge   set logically into separate clusters while “map sort the   “  “  segregates it into logical clusters” thus removing g the       retaining the necessary information  x   b     why     problems before       huge amount of   were   into single server   the threat of   losschallenge the   backup     scalability   into issue snowballing into crisis of sorts  to counter this google     in        the analysis of   was   into     mins rather taking           queries   run simultaneously on multiple servers search results   belogically          be   in real time   usp of   is it’s fault tolerance   scalability  the key reason for mapping   then   is to   up the job this enables the parallelism  x   b     map execution phases   map execution phases consist of five phases     map phase        input split from     parses input into   as keyvalue pairs  applies map function to each     inform master   of its completion     partition phase    each mapper must   which   will receive each of the outputs   for any key the   partition in the same   number of partitions  number of      shuffle phase   fetches input   from all map tasks for the portion   to   task bucket     sort phase   merge sorts all map outputs into a single run      phase   applies     function to the   run   x   b  
t3_eaxv1r,Learn Machine Learning from Scratch 2019,,bigdata,learn machine learning from scratch      
t3_eahtrf,Big Challenges with Big Data,,bigdata,big challenges with big   
t3_ea3s2f,Create your first sales dashboard in Apache Superset,,bigdata,create your first sales   in apache superset 
t3_eaadxf,We need a new MONIAC: Visualizing the Flow of Money to Design a Sustainable Future,,bigdata,we   a new moniac visualizing the flow of money to   a sustainable future 
t3_ea3cmv,How Big Data And Artificial Intelligence Work Together -Big Data Analytics News,,bigdata,how big     artificial intelligence work together big   analytics news 
t3_e9yvg4,"The Big Data World: Big, Bigger and Biggest",,bigdata,the big     big bigger   biggest 
t3_ea2g9e,Tips To Successfully Crack Big Data Engineer Interview,,bigdata,tips to successfully crack big   engineer interview 
t3_ea068o,VITech Lab Deep Learning Container,,bigdata,vitech lab   learning container 
t3_e9o9mu,The Best of AI: New Articles Published This Month (November 2019),"Hey, we made a selection of the [best AI articles published last month](https://www.sicara.ai/blog/11-2019-best-of-ai-november-2019)! Have a look at it!",bigdata,the best of ai new articles   this month november      hey we   a selection of the   have a look at it
t3_e9vnb1,NeurIPS 2019 | The Numbers,,bigdata,neurips       the numbers 
t3_e9q42e,Growing career with Psychology degree and Statistics background,I have a Master's in Psyc and a deep background in Inferential Statistics. I have studied multivariate statistics in graduate school and done contracts as a data scientist basically for NGO's. I want to take my career to the next level and get into a full-time data analysis type role and I am wondering whether I should do a boot-camp like Coursera or go back to school for a stem degree.,bigdata,growing career with psychology     statistics   i have a masters in psyc   a     in inferential statistics i have   multivariate statistics in   school     contracts as a   scientist basically for ngos i want to take my career to the next level   get into a fulltime   analysis type role   i am   whether i     a bootcamp like coursera or go back to school for a stem  
t3_e9q3q1,Azure Data Lake Storage (Gen 2) Introduction | Best storage solution for big data analytics in Azure,,bigdata,azure   lake storage gen      best storage solution for big   analytics in azure 
t3_e9g7ex,"Masters in EE, went through a coding bootcamp, currently in a big data engineering program","Hello everyone,

I finished my masters in EE last year, I went through a coding bootcamp and am currently in a big data engineering program, we're covering apache hadoop ecosystem, hive, spark, a little kafka, python, numpy, pandas, jupyter, etc. I had research experience with high performance computing/distributed storage/processing, 4 years of data analysis. I pretty much learned everything around machine learning except machine learning; I am very familiar with neuromorphic computing, the analog version of machine learning, how the neural network works, the training process, which is an n-dimensional optimization problem. I know most of that. 

&amp;#x200B;

I'm looking for help with my resume and career advice in getting into big data engineering and data science. Please help!",bigdata,masters in ee went through a   bootcamp currently in a big   engineering program hello everyone  i   my masters in ee last year i went through a   bootcamp   am currently in a big   engineering program were covering apache   ecosystem hive spark a little kafka python numpy   jupyter etc i   research experience with high performance   storageprocessing   years of   analysis i pretty much   everything   machine learning except machine learning i am very familiar with neuromorphic computing the analog version of machine learning how the neural network works the training process which is an   optimization problem i know most of that   x   b  im looking for help with my resume   career   in getting into big   engineering     science please help
t3_e9jcd0,"Read &amp; Write HBase using ""hbase-spark"" Connector",,bigdata,   write hbase using hbasespark connector 
t3_e9cioy,Need help in converting timestamp having milliseconds to Unix Timestamp (Unix epoch) in milliseconds ( 13 digit bigint ) in HIVE,"Hi everyone! I was working on a manual source ingestion and came across this scenario where the timestamp has to be converted to Unix timestamp format ( which is of bigint ).
Traditionally, I used unix_timestamp(&lt;timestamp_col_name&gt;) to convert it.  
Please note-the timestamp format was this: YYYY-MM-dd HH:mm:ss.SSS 

The above function returned value as a 10 digit bigint which is a Unix timestamp in seconds. 

But I need the Unix timestamp as a 13 digit bigint which is in milliseconds. 

Any help folks? I tried all ways possible and I’m stuck!",bigdata,  help in converting timestamp having   to unix timestamp unix epoch in         bigint  in hive hi everyone i was working on a manual source ingestion   came across this scenario where the timestamp has to be   to unix timestamp format  which is of bigint    i   unixtimestamptimestampcolname to convert it   please notethe timestamp format was this   hhmmsssss   the above function   value as a      bigint which is a unix timestamp in     but i   the unix timestamp as a      bigint which is in     any help folks i   all ways possible   i’m stuck
t3_e98djr,The Periodic Table of Data Scientists,,bigdata,the   table of   scientists 
t3_e92zxg,"Do you know there are multiple ways to create a Spark DataFrame, In this tutorial I've explained different ways to create a DataFrame. Hope you like it. Happy Learning !!",,bigdata,  you know there are multiple ways to create a spark   in this tutorial ive     ways to create a   hope you like it happy learning  
t3_e93z19,SMALL DATA: A NEW DATA REVOLUTION?,,bigdata,small   a new   revolution 
t3_e93sgi,Future of Media and Entertainment Industry Lies with Big Data Analytics,,bigdata,future of     entertainment   lies with big   analytics 
t3_e91sbt,The World of Big Data,,bigdata,the   of big   
t3_e8n1qq,Spark SQL Date and Time Functions,,bigdata,spark sql     time functions 
t3_e8silv,5 key benefits of nearshore travel software development,Why are so many companies and entrepreneurs turning to South America for their custom software development needs? [Here are just 5 reasons!](https://www.dynamia.us/blog/benefits-nearshore-travel-software-development),bigdata,  key benefits of nearshore travel software   why are so many companies   entrepreneurs turning to south america for their custom software      
t3_e8nd66,How Social Networking Big Data Provides Opportunities to Capture More Engagement,,bigdata,how social networking big     opportunities to capture more engagement 
t3_e8mri9,How US Government Agencies Are Exploring the Potentials of Data Science?,,bigdata,how us government agencies are exploring the potentials of   science 
t3_e8dj1c,Best Reddit Subreddits,,bigdata,best     
t3_e8jncg,Usage of Spark SQL StructType on DataFrame,,bigdata,usage of spark sql structtype on   
t3_e8b1s6,Usage of Spark SQL Map functions,,bigdata,usage of spark sql map functions 
t3_e8bsci,An interview about how SnowflakeDB was built to provide a performant and flexible data platform for the cloud era,,bigdata,an interview about how   was built to   a performant   flexible   platform for the   era 
t3_e8gc5v,What is the right way to build a recommender system?,,bigdata,what is the right way to   a   system 
t3_e854yy,Spark RDD Transformations with examples,,bigdata,spark   transformations with examples 
t3_e86evc,Spark DataFrame - How to select the first row of each group?,,bigdata,spark    how to select the first row of each group 
t3_e7uyat,The advent of the (Big) Data Architect,,bigdata,the   of the big   architect 
t3_e7cgdj,Map Reduce Algorithm Blog,[https://medium.com/@ujjawaldixit099/map-reduce-algorithm-392e6d5a5f22?source=friends\_link&amp;sk=a26561981a2ee7458d0afbef7b1e8bae](https://medium.com/@ujjawaldixit099/map-reduce-algorithm-392e6d5a5f22?source=friends_link&amp;sk=a26561981a2ee7458d0afbef7b1e8bae),bigdata,map   algorithm blog 
t3_e7dzu5,Hadoop Distributed File System,"www.medium.com/@seevee
[HDFS](http://www.medium.com/@seevee)

Hadoop Distributed File System (HDFS) :

• HDFS is the distributed file system that provides a access to data across the Hadoop Cluster .
• A Cluster is the group of computer that works together.
• HDFS is the key tools that manages and support analysis of very large volumes of big data .

Why HDFS :

• Cost : HDFS is the open source and that’s why it can be used with zero licensing and support cost.
• Speed : Large Hadoop clusters can read and write more than one terabytes of data in one second.It can easy gives 2 gigabytes of per second per computer.
• Reliability : HDFS copies data multiple times and gives compiles to individual nodes . — Node is the commodity server which is interconnected with the network of devices.
• HDFS puts at least one copy of data to the different server in case any data get deleted from any of the nodes it can be found on the server.

HDFS Storage :

• HDFS stores files in the numbers of blocks
• Each Blocks is replicated to few separate computers
• Dats is divided into 128 MB per blocks and get replicated to the local disk of nodes.

Metadata controls the the physical location of a block and location of its replication within the cluster and it is stored in NameNode.

Characteristics Of HDFS :

1. Fault Tolerant
2. Scalable
3. Rack - Aware
4. Support for heterogenous cluster
5. Built for large datasets.

HDFS Architecture and Component :

• HDFS has Master - Slave architecture.
• In a typical Hadoop cluster two separate machine are configured as NameNodes any point in time exactly one is in Active state and Other is in Standby state .
• The Active NameNodes is responsible for all the client operation in the cluster while the standby is simply acting as the slave maintaining enough space providing the fast fail over if necessary.
• The MasterNode that is NameNode insure that data require is loaded an segregated into data blocks into slave knows as Data nodes.
• A Data Nodes serves read or write requests.It also creates delete and replicated blocks based on gathering instruction from the NameNode.
• The Standby NameNode and Active NameNode keep in sync with each other with shared edit logs or Metadata.
• The Active NameNodes updates the edit logs which is Metadata information about the storage this is done by name base modification like block location , status of the nodes and so on.
• The Standby nodes watch’s changes made to the edit logs (Metadata) update name space in consistent manner until the fail over.
• The Standby ensures that it has read all the edits before promoting itself to active state .it is manual fail over process which has to be performed by an Administrator.
• For automatic fail over we have to use service known as Zookeeper.
• The Zookeeper failover controller (ZKFC) keeps an open session with the periodically pinning it with a health check command .
• If the node has crashed, frozen or otherwise entered in unhealthy state, the health monitor will mark it un-healthy an elect a new NameNode.


High Availability Cluster Implementations (HA implementation) :

Hadoop support two types of HA implementations for storing Metadata :

Quorum-based Storage
• HA implementation that uses Quorum Journal Manager (QJM) . During this state the Standby Node keep synchronised with the Active Node by separate “DEMONS” called “JournalNodes”.
• Demons are long running processes that typically start with the system and listen for request from client processes.
• Each Demons runs it’s own Java Virtual Machine (JVM) .When any name space modifications done in active node it Durably log it into modification to majority of Journal Nodes .
• The Standby Nodes read the edit in Journal mode and consistently watches change in edit log.As the Standby Nodes seeds the edit it applies them to it own NameSpace .
• In case of failure the Standby ensures that it has read all the edit from Journal Nodes before it promote itself to Active State.
• This ensure that Name Space is fully synced before a failover occurs .

Shared storage using NFS

• A Standby State keeps its space synchronised with a Active Node through the access to the directory on a shared storage device


HDFS Component :

1. NameNode :
• The NameNode is the core component of HDFS server there is only one NameNode in entire cluster .
• The NameNode maintains and executes the Filesystem NameSpace operation such as opening,closing and renaming of files and directory which are present in HDFS file system.
• The NameSpace Image and edit log stores the information of the data and Metadata .
• NameNodes also determines the linking of blocks to DataNodes.
• NameNode is the single point of failure.




2. DataNode :
• The DataNode is multiple instance server.
• There can be “N” numbers of Data Node servers. The Numbers depends upon the type of Network and Storage system.
• The DataNode server stores and maintains the Data Blocks.
• The NameNode server permissions the DataBlocks on the bases of the type of jobs submitted by the clients .
• The DataNodes also stores and retrieve the Blocks when asked by the clients or the NameNode.
• The also read and write the requests perform block creation, deletion and Block replication on instruction from the NameNode.



3. Zookeeper :

• Zookeeper allow distributed process to coordinate with each other with the shared , hierarchical Name Space.
• The Zookeeper implementation is simple and replicated which puts a premium on high performance, high availability, and strictly ordered access.
• The Zookeeper maintains the in memory image along with the transaction log and snapshots for persistent stores.
• As long as the majority of the servers available the Zookeeper service will be available.",bigdata,    file system         file system     •   is the   file system that   a access to   across the   cluster  • a cluster is the group of computer that works together •   is the key tools that manages   support analysis of very large volumes of big     why     • cost    is the open source   that’s why it can be   with zero licensing   support cost •    large   clusters can     write more than one terabytes of   in one   can easy gives   gigabytes of per   per computer • reliability    copies   multiple times   gives compiles to      —   is the   server which is   with the network of   •   puts at least one copy of   to the   server in case any   get   from any of the   it can be   on the server    storage   •   stores files in the numbers of blocks • each blocks is   to few separate computers •   is   into     mb per blocks   get   to the local   of      controls the the physical location of a block   location of its replication within the cluster   it is   in    characteristics of       fault tolerant   scalable   rack  aware   support for heterogenous cluster   built for large      architecture   component   •   has master  slave architecture • in a typical   cluster two separate machine are   as   any point in time exactly one is in active state   other is in   state  • the active   is responsible for all the client operation in the cluster while the   is simply acting as the slave maintaining enough space   the fast fail over if necessary • the   that is   insure that   require is   an   into   blocks into slave knows as     • a     serves   or write requestsit also creates       blocks   on gathering instruction from the   • the       active   keep in sync with each other with     logs or   • the active     the   logs which is   information about the storage this is   by name base   like block location  status of the     so on • the     watch’s changes   to the   logs     name space in consistent manner until the fail over • the   ensures that it has   all the   before promoting itself to active state it is manual fail over process which has to be   by an   • for automatic fail over we have to use service known as zookeeper • the zookeeper failover controller zkfc keeps an open session with the   pinning it with a health check    • if the   has   frozen or otherwise   in unhealthy state the health monitor will mark it unhealthy an elect a new     high availability cluster implementations ha implementation     support two types of ha implementations for storing       storage • ha implementation that uses quorum journal manager qjm    this state the     keep   with the active   by separate “ ”   “ ” •   are long running processes that typically start with the system   listen for request from client processes • each   runs it’s own java virtual machine jvm when any name space     in active   it   log it into   to majority of journal    • the       the   in journal     consistently watches change in   logas the       the   it applies them to it own namespace  • in case of failure the   ensures that it has   all the   from journal   before it promote itself to active state • this ensure that name space is fully   before a failover occurs     storage using nfs  • a   state keeps its space   with a active   through the access to the   on a   storage       component        • the   is the core component of   server there is only one   in entire cluster  • the   maintains   executes the filesystem namespace operation such as openingclosing   renaming of files     which are present in   file system • the namespace image     log stores the information of the        •   also   the linking of blocks to   •   is the single point of failure          • the   is multiple instance server • there can be “n” numbers of     servers the numbers   upon the type of network   storage system • the   server stores   maintains the   blocks • the   server permissions the   on the bases of the type of jobs   by the clients  • the   also stores   retrieve the blocks when   by the clients or the   • the also     write the requests perform block creation     block replication on instruction from the        zookeeper   • zookeeper allow   process to   with each other with the    hierarchical name space • the zookeeper implementation is simple     which puts a premium on high performance high availability   strictly   access • the zookeeper maintains the in memory image along with the transaction log   snapshots for persistent stores • as long as the majority of the servers available the zookeeper service will be available
t3_e6txu8,Big Data and Its Impact On Business and Mobile Apps,,bigdata,big     its impact on business   mobile apps 
t3_e6vlvw,Data Management 101: Five Things Every Human of Data Should Know,,bigdata,  management     five things every human of     know 
t3_e6gxa2,What is Canonical URL and why it is so Important?,,bigdata,what is canonical url   why it is so important 
t3_e6tr0b,This beautiful future depends on data and AI,,bigdata,this beautiful future   on     ai 
t3_e6lxnk,Is AI suffering from misinformation?,,bigdata,is ai suffering from misinformation 
t3_e6gdoe,Big Data Formats Explained Using Spark on Azure &amp; GCP,,bigdata,big   formats   using spark on azure  gcp 
t3_e6jc3o,Geospatial Data Management in Apache Spark,,bigdata,geospatial   management in apache spark 
t3_e6gw1e,What is the best cloud storage for my case (huge data amount),"I want to store around 10 TB of satellite images every month (new data that is added to the old) in the cloud. This data will not be changed ever. This data shall be downloaded frequently by users of an app.
Which storage option is the right one? AWS s3 standard? Glacier?",bigdata,what is the best   storage for my case huge   amount i want to store      tb of satellite images every month new   that is   to the   in the   this   will not be   ever this   shall be   frequently by users of an app which storage option is the right one aws s    glacier
t3_e6fo2e,Buckle Up: Grab a Job as a Senior Data Analyst Before It’s Too Late,,bigdata,buckle up grab a job as a senior   analyst before it’s too late 
t3_e65kfy,Free Presto event in NYC next week,"Hi all, I work with the team behind the open-source distributed SQL query engine, Presto. If you've heard of it, great! 

Anyways, we're hosting the largest summit for the project next week in NYC, completely free, educational, and community driven. 

If you're around and want to hear from some of the largest companies (Slack, Pinterest, Comcast, etc) on how they're using the tech to drive their interactive analytics then we'd love to have you join us. The event will be filled with project contributors and industry leaders in this space.

Here's a link to the event page. [https://www.eventbrite.com/e/presto-summit-nyc-registration-75232673953](https://www.eventbrite.com/e/presto-summit-nyc-registration-75232673953)",bigdata,free presto event in nyc next week hi all i work with the team   the opensource   sql query engine presto if youve   of it great   anyways were hosting the largest summit for the project next week in nyc completely free     community     if youre     want to hear from some of the largest companies slack pinterest comcast etc on how theyre using the tech to   their interactive analytics then   love to have you join us the event will be   with project contributors       in this space  heres a link to the event page 
t3_e5xaaq,"CDP Data Center: Better, Safer Data Analytics from the Edge to AI",,bigdata,    center better safer   analytics from the   to ai 
t3_e5yulx,Snowflake Renewals,"Does anyone know how snowflake handles renewals if you underused your current contract?

Will they let you rollover your unused portion and downsize your contract? Is the unused portion lost? Will they let you right size your next contract?",bigdata,snowflake renewals   anyone know how snowflake   renewals if you   your current contract  will they let you rollover your   portion     your contract is the   portion lost will they let you right size your next contract
t3_e5pp2p,China is about to overtake the US in AI research,,bigdata,china is about to overtake the us in ai research 
t3_e5i98e,An interview about building a successful data team and managing their career growth to power a successful financial business,,bigdata,an interview about   a successful   team   managing their career growth to power a successful financial business 
t3_e550uq,"Hello to this community, I from Cuba, 26, mathematician and now i live in spain. I am very excited because i got a job in one of the big4 as data scientists. My parents are in cuba and i dont have friend to celebrates this hit on my life. I very very happy!!! Greetings for everyone!!!!",,bigdata,hello to this community i from cuba    mathematician   now i live in spain i am very   because i got a job in one of the big  as   scientists my parents are in cuba   i   have   to celebrates this hit on my life i very very happy greetings for everyone 
t3_e5j8pv,AWS MSK &amp; Lenses.io - Kafka in Days not Months. New Support for JMX Open Monitoring with Prometheus,,bigdata,aws msk  lensesio  kafka in   not months new support for jmx open monitoring with prometheus 
t3_e5cwag,Difference between Big Data Hadoop and SAS,,bigdata,  between big       sas 
t3_e53mrj,"MountainTop Data's Sky Cassidy Warns of ""Data Vampires"" in Light of 1.2-B Record Data Breach",,bigdata,mountaintop   sky   warns of   vampires in light of   b     breach 
t3_e513j3,Kafka Summit London CfP closes in two weeks,,bigdata,kafka summit   cfp closes in two weeks 
t3_e4smnm,Microsoft: Big Data Architectures,,bigdata,microsoft big   architectures 
t3_e4glp7,Introduction to Pyspark join types,,bigdata,  to pyspark join types 
t3_e4116s,"ytwrk, a YouTube Exporter: Dumps comments, live chat, channels and videos","I decided to open-source my Go YouTube crawler/exporter.  
It's pretty reliable and requires no API key to run.

GitHub: [https://github.com/terorie/ytwrk](https://github.com/terorie/ytwrk)

**Features**:

* Comment export: Mass extract comments from millions of videos (Used to create [the 10 billion comment dump](https://www.reddit.com/r/DataHoarder/comments/dveb7b/xpost_10_billion21_tb_youtube_comments/))
* Livechat export: Live exporter for chats on livestreams
* Video/channel export: Mass export metadata in YouTube's internal JSON representation (i.e. lossless) to tar archives
* Pipe+stream-friendly: Most commands stream via stdin/stderr/stdout (instead of creating files) and can be easily integrated as long-running tasks in big data pipelines",bigdata,ytwrk a youtube exporter   comments live chat channels     i   to opensource my go youtube crawlerexporter   its pretty reliable   requires no api key to run  github   features   comment export mass extract comments from millions of     to create    livechat export live exporter for chats on livestreams    export mass export   in youtubes internal json representation ie lossless to tar archives    most   stream via     of creating files   can be easily   as longrunning tasks in big   pipelines
t3_e3gew0,Why Data Science Projects Fail and How to Make Yours a Success,,bigdata,why   science projects fail   how to make yours a success 
t3_e3i73j,Big data and data science,,bigdata,big       science 
t3_e3ankq,How Small Businesses Look to Leverage Big Data and Data Analytics,,bigdata,how small businesses look to leverage big       analytics 
t3_e32sod,"Herring, Not Herring: Deep Learning Accelerates Detection and Classification of Underwater Species",,bigdata,herring not herring   learning accelerates     classification of   species 
t3_e3a7op,Exchange - We train &amp; connect people to data science jobs.,"Hey I’m Krish and for the past 6 months, we’ve been working on [Exchange](https://datatrain.gettechtrained.com/): we connect data science job seekers with professional data scientists who help them land their dream data science job. 

I’m a recent NYU grad (Class ’19) and during my senior year, I attended a bootcamp for software engineering interviews hosted by Facebook. The knowledge and practice that I received was immense! There were Facebook engineers on-site to help us with algo questions, communications, and other useful interview tips. My co-founder Daniel and I thought it would be a genius idea to create this kind of training process for everyone and every tech job. Right now we’re focusing mainly on engineers and data science.

We’re posting here today because we thought this community would like to hear about us. We’re also looking for feedback on how could improve.

Thanks for your time 📷",bigdata,exchange  we train  connect people to   science jobs hey i’m krish   for the past   months we’ve been working on   we connect   science job seekers with professional   scientists who help them   their     science job   i’m a recent nyu   class ’       my senior year i   a bootcamp for software engineering interviews   by facebook the     practice that i   was immense there were facebook engineers onsite to help us with algo questions communications   other useful interview tips my       i thought it   be a genius   to create this   of training process for everyone   every tech job right now we’re focusing mainly on engineers     science  we’re posting here   because we thought this community   like to hear about us we’re also looking for   on how   improve  thanks for your time 📷
t3_e39jj0,Difference Between Cloud Computing and Big Data,,bigdata,  between   computing   big   
t3_e349g2,Is Cassandra the most advanced and favorable database system?,,bigdata,is   the most     favorable   system 
t3_e2fskv,New big data algorithms improve earthquake detection and monitor livestock health,,bigdata,new big   algorithms improve earthquake     monitor livestock health 
t3_e2j2zo,Data privacy in Brain-Computer Interfaces,,bigdata,  privacy in braincomputer interfaces 
t3_e2i82d,What do people think about AI?,,bigdata,what   people think about ai 
t3_e2hrd6,Cypress.io Plugin for WebSocket Integration Test of Streaming Apps,,bigdata,cypressio plugin for websocket integration test of streaming apps 
t3_e2aa7r,How AI and Big Data Are Connected,,bigdata,how ai   big   are   
t3_e23w35,An interview about how Sentry used Clickhouse to build an event data warehouse and pay down their architecture debt,,bigdata,an interview about how sentry   clickhouse to   an event   warehouse   pay   their architecture   
t3_e1ztv9,"When people talk about ""traditional methods"" which specific methods they usually refer to?","Many definitions of big data usually mention sometime along the lines of "" datasets that are too large for traditional/conventional methods/ systems""

I'm not here to argue about the correct definition of Big data, but  which specific methods are usually thought of? Sometime like excel spreadsheets?",bigdata,when people talk about     which specific   they usually refer to many   of big   usually mention sometime along the lines of    that are too large for     systems  im not here to argue about the correct   of big   but  which specific   are usually thought of sometime like excel  
t3_e1tdlb,The New Face of Digital Disruption - Technology Landscape 2020,,bigdata,the new face of      technology        
t3_e1koxr,Hadoop Ecosystem Tool for fast transactional lookup,"&amp;#x200B;

I run spark jobs to do ETL on a bunch of different logs to produce mappings of timestamp, user, ip address, and hostname. given a timestamp and one of the able fields, I want to be able to do rapid lookups on this information. I'd like to be able to lookup from Spark jobs as well as over the network via rest or sql. I'm wondering what tool people would recommend to support the lookups.  I initially tried Hive but the fact that it is running map reduce jobs behind the scenes makes it to slow for these quick transactions. We have a redis cluster which would be very fast, but I was hoping to keep this within the Hadoop Ecosystem if possible.",bigdata,  ecosystem tool for fast transactional lookup x   b  i run spark jobs to   etl on a bunch of   logs to   mappings of timestamp user ip     hostname given a timestamp   one of the able   i want to be able to     lookups on this information   like to be able to lookup from spark jobs as well as over the network via rest or sql im   what tool people     to support the lookups  i initially   hive but the fact that it is running map   jobs   the scenes makes it to slow for these quick transactions we have a   cluster which   be very fast but i was hoping to keep this within the   ecosystem if possible
t3_e1g7jf,Are there any competitors to Snowflake?,"There is so much hype around snowflake - was wondering if there are any *real* competitors out there? 

I used to work at a startup where we had been using RedShift and BigQuery (on different teams) but I was too far removed to comment on how each performed- would love to hear what others think about this.",bigdata,are there any competitors to snowflake there is so much hype   snowflake  was   if there are any real competitors out there   i   to work at a startup where we   been using     bigquery on   teams but i was too far   to comment on how each     love to hear what others think about this
t3_e1alps,Artificial Intelligence vs. Machine Learning vs. Data Mining 101 – What’s the Big Difference?,,bigdata,artificial intelligence vs machine learning vs   mining     – what’s the big   
t3_e1fqlt,How to do dynamic pricing using the PAO framework,,bigdata,how to     pricing using the pao framework 
t3_e1cnxs,"Have access to shopping and prescription data, as well as logistics and planning, what to do?","So i happen to have data for ~million user shopping patterns (pharmacy, drugs, child care in one network) plus logistical data from the same company and possibly have access to the prescription / insurance data of the same company (company owns pharmacies, insurance and few hospitals) all anonymous of course. What would be the first steps to use this data for ML and what patterns would i target first for quick buck and for the long term profit?",bigdata,have access to shopping   prescription   as well as logistics   planning what to   so i happen to have   for million user shopping patterns pharmacy     care in one network plus logistical   from the same company   possibly have access to the prescription  insurance   of the same company company owns pharmacies insurance   few hospitals all anonymous of course what   be the first steps to use this   for ml   what patterns   i target first for quick buck   for the long term profit
t3_e1641q,Big Data Overview,"Hi everyone,
Recently i'va made a short video in which i summarized big data in 4 minutes.
Link : https://youtu.be/KRJ4bs5Db4o
Don't hesitate to share your thoughts.
PS: it's an educative video.",bigdata,big   overview hi everyone recently iva   a short   in which i   big   in   minutes link     hesitate to share your thoughts ps its an    
t3_e0vb54,Top 10 Countries &amp; Regions Leading the Big Data Adoption in 2019,,bigdata,top    countries  regions   the big     in      
t3_e0l85t,An interesting introduction to Machine Learning!,,bigdata,an interesting   to machine learning 
t3_e05fvo,Watson Studio environments compute usage question,"Big data newbie here!

I am doing an online course on using Spark for ML and they use IBM Watson Studio for running the notebook. The way IBM charges the users of this cloud based system is using something called capacity unit hours (CUH) , I failed to understand what that is, as in what does 1 CUH mean? I tried to understand what it meant from their [learn more](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/track-runtime-usage.html) page, but they don't explain what CUHs are , they just use CUH as a basic unit to explain their packages. Could someone explain what CUH is exactly ? Analogies would be helpful. Thanks.",bigdata,watson   environments compute usage question big   newbie here  i am   an online course on using spark for ml   they use ibm watson   for running the notebook the way ibm charges the users of this     system is using something   capacity unit hours cuh  i   to   what that is as in what     cuh mean i   to   what it meant from their   page but they   explain what cuhs are  they just use cuh as a basic unit to explain their packages   someone explain what cuh is exactly  analogies   be helpful thanks
t3_e038lk,An Open Source Stack (Data Version Control and Cortex) for Managing and Deploying Models - Step-By-Step Tutorial,,bigdata,an open source stack   version control   cortex for managing        stepbystep tutorial 
t3_dzzq8b,Big Data In Big Business: How to Use Big Data in Business Successfully?,,bigdata,big   in big business how to use big   in business successfully 
t3_dzyur5,What is a Data Catalog and Why Should You Even Care?,,bigdata,what is a   catalog   why   you even care 
t3_dzzlds,"Comparing Database Query Languages in MySQL, Couchbase, and MongoDB",,bigdata,comparing   query languages in mysql couchbase     
t3_dzxsib,Considering Top Pros and Cons While Managing Big Data,,bigdata,  top pros   cons while managing big   
t3_e00vfa,The importance of data strategy,,bigdata,the importance of   strategy 
t3_dzzpob,Who is a Big Data Engineer? Why 2020 Will Be a Desirable Year for Data Professionals,"Haven’t you heard? 2020 is going to be a year where there will be inextricably high demand for big data skills. Data scientists, data engineers, and data analysts with experience in Python, R, and SQL top the list.

You may still think everyone knows what big data is by now, but you may still be wrong. Despite being the hottest buzzword for quite some time now, the misconception remains.

[https://medium.com/@michael.lyamm/who-is-a-big-data-engineer-why-2020-will-be-a-desirable-year-for-data-professionals-180abd6dd590](https://medium.com/@michael.lyamm/who-is-a-big-data-engineer-why-2020-will-be-a-desirable-year-for-data-professionals-180abd6dd590)",bigdata,who is a big   engineer why      will be a   year for   professionals haven’t you        is going to be a year where there will be inextricably high   for big   skills   scientists   engineers     analysts with experience in python r   sql top the list  you may still think everyone knows what big   is by now but you may still be wrong   being the hottest   for quite some time now the misconception remains  
t3_dzvkok,Difference Between Big Data Analytics and Machine Learning,,bigdata,  between big   analytics   machine learning 
t3_dzutvv,"What are these things, is there a resource?","i understand sql dbs and redis key/value.

but what are spark, looker, snowflake, etc.... where do they fit in system design, whats wrong with regular postgresql...??",bigdata,what are these things is there a resource i   sql       keyvalue  but what are spark looker snowflake etc where   they fit in system   whats wrong with regular postgresql
t3_dzjy8r,Introducing ksqlDB,,bigdata,    
t3_dzlvjp,Kafka Tutorial - How to Visualize Data in Apache Kafka Using D3.js,,bigdata,kafka tutorial  how to visualize   in apache kafka using   
t3_dzg59k,How much data did / do we generate every year?," I remember slighty that I read an article about the development of generated data every year with the observation that we generate more every year then in all past years cumulated.

I cant find the article/graph anymore. There are this ""60 seconds in the internet"" graphs but I wanted to see if I can find sth that also includes data generated outside of the internet and maybe in bytes. Do you maybe know where I can find the numbers generated each year? Maybe even categorized, i.e. x% is structured data etc.?",bigdata,how much        we generate every year  i remember slighty that i   an article about the   of     every year with the observation that we generate more every year then in all past years    i cant   the articlegraph anymore there are this      in the internet graphs but i   to see if i can   sth that also         of the internet   maybe in bytes   you maybe know where i can   the numbers   each year maybe even   ie x is     etc
t3_dz454g,Introducing ksqlDB,,bigdata,    
t3_dz321e,Anyone tried Cloudera's CDP?,"Hey y'all. My org is considering Cloudera's CDP, and we're approaching a deadline. Cloudera's been hyping it up for a while now (I'm even getting ads for it on YouTube). I'd just love to hear if it has met your expectations. Thanks!",bigdata,anyone       hey yall my org is         were approaching a     been hyping it up for a while now im even getting   for it on youtube   just love to hear if it has met your expectations thanks
t3_dze7dh,big AND substantive data,,bigdata,big   substantive   
t3_dz6c59,Big data is serving top tennis players a match-winning advantage,,bigdata,big   is serving top tennis players a matchwinning   
t3_dz17jk,Is it worth learning Hadoop now?,I am trying to switch my career to Big Data. I have Sales Background and I am currently learning Machine Learning. Just wanted to know if there are any latest developments in the domain thats worth learning instead of outdated ones.,bigdata,is it worth learning   now i am trying to switch my career to big   i have sales     i am currently learning machine learning just   to know if there are any latest   in the   thats worth learning   of   ones
t3_dz3t58,How to Build Slim Docker Images Fast,[https://towardsdatascience.com/how-to-build-slim-docker-images-fast-ecc246d7f4a7](https://towardsdatascience.com/how-to-build-slim-docker-images-fast-ecc246d7f4a7),bigdata,how to   slim   images fast 
t3_dywbex,Hadoop MapReduce vs. Apache Spark,,bigdata,    vs apache spark 
t3_dyi45m,From Data Oops to DataOps: 5 Things You Need to Know,,bigdata,from   oops to     things you   to know 
t3_dyaodm,An interview about data virtualization and data engineering automation with AtScale and the value of abstractions for your data platform architecture,,bigdata,an interview about   virtualization     engineering automation with atscale   the value of abstractions for your   platform architecture 
t3_dy10hg,Glimpse into Spark 3.0 [Early Access],,bigdata,glimpse into spark      
t3_dy2mfi,Big Data Analytics: A Weapon Against Rising Cyber Security Attacks -Big Data Analytics News,,bigdata,big   analytics a weapon against rising cyber security attacks big   analytics news 
t3_dxlayl,"FREE EVENT AND WORKSHOP: AI and Creativity (London, 2nd December 2019)",,bigdata,free event   workshop ai   creativity            
t3_dx2avh,Which of these 4 ETL tools is the best?,"Hi all. So I’ve very recently started a job as a Big Data Engineer and my very first task is to do some research on the following tools for data integration and ETL (Spark, SSIS, Apache Nifi, Talend) and present to them team so we can decide which one to use for a project.

My background is more to do with machine learning and maths so I was hoping someone here who has some relevant experience could shed some light on which of these tools they would use over the others and why?",bigdata,which of these   etl tools is the best hi all so i’ve very recently   a job as a big   engineer   my very first task is to   some research on the following tools for   integration   etl spark ssis apache nifi     present to them team so we can   which one to use for a project  my   is more to   with machine learning   maths so i was hoping someone here who has some relevant experience     some light on which of these tools they   use over the others   why
t3_dx6q7g,Python Programming Language Is Considered Better Than Other Languages - DataWider,,bigdata,python programming language is   better than other languages    
t3_dwwxek,How is the Block size calculated in MapReduce jobs?,"Here's a post about calculation of Block Size in MapReduce jobs.

[https://link.medium.com/QTwaIqD3D1](https://link.medium.com/QTwaIqD3D1)",bigdata,how is the block size   in   jobs heres a post about calculation of block size in   jobs  
t3_dwp6ot,John Carmack is planning to create general AI,,bigdata,john carmack is planning to create general ai 
t3_dwizux,How LinkedIn customizes Apache Kafka for 7 trillion messages per day,,bigdata,how   customizes apache kafka for   trillion messages per   
t3_dwqlce,How to setup awesome python evironments without pain,,bigdata,how to setup awesome python evironments without pain 
t3_dwae40,Tutorial on how to use Airflow without pain,[https://medium.com/@simon.hawe/how-to-use-airflow-without-headaches-4e6e37e6c2bc](https://medium.com/@simon.hawe/how-to-use-airflow-without-headaches-4e6e37e6c2bc),bigdata,tutorial on how to use airflow without pain 
t3_dwlz2i,What should I learn for this Job?,"I got a training opportunity in a company. This company is just starting. They have some plans like creating a big database based on some open source existing databases in order to create a search engine in their niche. They are willing to start with this project in a few months, as it is not their priority right now.

I am just starting in data-science (worked a little with pandas).

Even though my lack of knowledge was obvious, we ended up having a nice interview. The boss told me that he is in no hurry and that he will give me a desk in the office to learn there on my own. So when the time comes and he is willing to hire a team I will be competent enough to be a part of it.

I tried to know more about what exactly he needs but he doesn't really know himself and nobody in the company does because as I said he hasn't started yet his data project so he didn't hire anybody related to the issue.

My question is what should I learn now? Is it Big data? data science? Python or SQL?

I feel like this an interesting opportunity for me and I hope I won't miss it. And even if it didn't work out, in the end, I will still be learning a lot.",bigdata,what   i learn for this job i got a training opportunity in a company this company is just starting they have some plans like creating a big     on some open source existing   in   to create a search engine in their niche they are willing to start with this project in a few months as it is not their priority right now  i am just starting in     a little with    even though my lack of   was obvious we   up having a nice interview the boss   me that he is in no hurry   that he will give me a   in the office to learn there on my own so when the time comes   he is willing to hire a team i will be competent enough to be a part of it  i   to know more about what exactly he   but he   really know himself     in the company   because as i   he hasnt   yet his   project so he   hire     to the issue  my question is what   i learn now is it big     science python or sql  i feel like this an interesting opportunity for me   i hope i wont miss it   even if it   work out in the   i will still be learning a lot
t3_dwkwle,Big Data Analytics Vs. Data Mining,,bigdata,big   analytics vs   mining 
t3_dw9or8,7 Guidelines for Ingesting Big Data to Data Lakes,,bigdata,    for ingesting big   to   lakes 
t3_dw9yal,How to accurately scope analytics projects,,bigdata,how to accurately scope analytics projects 
t3_dw896i,"What Is Data Profiling: Process, Best Practices and Tools - Overview","Data profiling is a process of reviewing source data for content and quality. As data gets bigger and infrastructure moves to the cloud, data profiling is increasingly important. The following overview explains how to achieve big data profiling with limited time and resources: [What Is Data Profiling? Process, Best Practices and Tools](https://panoply.io/analytics-stack-guide/data-profiling-best-practices/)",bigdata,what is   profiling process best practices   tools  overview   profiling is a process of reviewing source   for content   quality as   gets bigger   infrastructure moves to the     profiling is increasingly important the following overview explains how to achieve big   profiling with   time   resources  
t3_dw6nbn,Finding HDFS Paths of Hive tables,"One of the simplest ways to extract the HDFS Paths of all the Hive tables directly from Hive Metastore!

[https://medium.com/@gomzvicky/finding-hdfs-paths-of-hive-tables-a42dcab161d7](https://medium.com/@gomzvicky/finding-hdfs-paths-of-hive-tables-a42dcab161d7)",bigdata,    paths of hive tables one of the simplest ways to extract the   paths of all the hive tables   from hive metastore  
t3_dw56o4,Data engineering channel in Telegram,"I have stated a Telegram channel about Data engineering, Scala, Python and privacy. Content is 99% in English with relevant hashtags, sometimes in Russian, comments are in Armenian ( to boost local community which is very small). Feel free to subscribe[https://t.me/data1984](https://t.me/data1984)",bigdata,  engineering channel in telegram i have   a telegram channel about   engineering scala python   privacy content is    in english with relevant hashtags sometimes in russian comments are in armenian  to boost local community which is very small feel free to subscribe
t3_dw68sy,QV12BA: QlikView Business Analyst (QVBA),,bigdata,qv  ba qlikview business analyst qvba 
t3_dvz5mk,Nice article about how to write better and simpler data science code using funcy,,bigdata,nice article about how to write better   simpler   science   using funcy 
t3_dvvnva,Finding Total Size of Hive Database’s data in HDFS,"Here's a quick article on How to Find Total size of Hive Database's/Table's data residing in HDFS!

[https://link.medium.com/p3fnT5oxA1](https://link.medium.com/p3fnT5oxA1)",bigdata,  total size of hive  ’s   in   heres a quick article on how to   total size of hive       in    
t3_dvym33,How to export simples cloudera manager datas,"Hello,

i need to export a very simples datas from CM : **hostname, service name and status of the service** and then put those datas in grafana.

In CM if you go the main host's page there's a table that can be the perfect example of what i need:

[in my case i would like export only: nome, funçoes and the status of them](https://preview.redd.it/taskqttptiy31.jpg?width=1018&amp;format=pjpg&amp;auto=webp&amp;s=056bbf4933f57919742f98df4b887d9c3388c5af)

&amp;#x200B;

can anyone help me? thanks!",bigdata,how to export simples   manager   hello  i   to export a very simples   from cm  hostname service name   status of the service   then put those   in grafana  in cm if you go the main hosts page theres a table that can be the perfect example of what i       x   b  can anyone help me thanks
t3_dveafo,10 billion/2.1 TB YouTube comments,"I crawled YouTube comments for my friends that are into big data. Well, I got an unexpectedly large dataset together.Because of the enormous size, I haven't had time to analyze video and author coverage yet, unfortunately.

Hope you enjoy. Seeders as always appreciated.

More Info: [https://files.mine.terorie.dev/yt/](https://files.mine.terorie.dev/yt/)

`magnet:?xt=urn:btih:18bc22ee0017fb056794f3d7821a942b5c08cc91&amp;dn=YT%5FCOMMENTS%5FTERORIE%5F2019%5F10.ndjson.zst`

*Update:* Added list of the 576,551,936 unique author IDs who posted all those comments.  
*Update #2*: Direct download link is available!",bigdata,   billion   tb youtube comments i   youtube comments for my   that are into big   well i got an   large   togetherbecause of the enormous size i havent   time to analyze     author coverage yet unfortunately  hope you enjoy   as always    more info          list of the           unique author   who   all those comments           link is available
t3_dvru5q,Nice article on tensorflow 2 and Learning Rates,,bigdata,nice article on tensorflow     learning rates 
t3_dvlc51,An interview about data protection regulations and how they can influence the design of your data platform,,bigdata,an interview about   protection regulations   how they can influence the   of your   platform 
t3_dvlnzw,Achieve IBM Big Data Architect Certification: Study Tips,,bigdata,achieve ibm big   architect certification   tips 
t3_dvishx,Cheeriojs. Learn Web Scraping - JavaScript Web Scraping Guy,,bigdata,cheeriojs learn web scraping  javascript web scraping guy 
t3_dubjcb,The 5-minute guide to using bucketing in Pyspark,,bigdata,the  minute   to using bucketing in pyspark 
t3_duripe,I'm selling my side projects for very small price because of sudden cash requirement 👨‍💻,"I'm selling my side projects for very small price because of sudden cash requirement 👨‍💻

https://tabnote.co 299$

https://url.devro.club 199$

We can discuss about the prices 💙
I have to pay for my hosting but not having enough money for that, you can check my other products here https://devrolabs.com 🙏

Follow this tweet for more info 👉 https://twitter.com/kesara_wimal/status/1193858285679579137?s=19",bigdata,im selling my   projects for very small price because of   cash requirement 👨‍💻 im selling my   projects for very small price because of   cash requirement 👨‍💻              we can   about the prices 💙 i have to pay for my hosting but not having enough money for that you can check my other   here  🙏  follow this tweet for more info 👉 
t3_du48jy,Preview release of Spark 3.0,,bigdata,preview release of spark    
t3_du7xrb,10 Parameters for Big Data Assessment,,bigdata,   parameters for big   assessment 
t3_dtdkep,Hadoop Ecosystem,,bigdata,  ecosystem 
t3_dth978,How to use Lenses as a Secure Data Layer to Produce Data into Apache Kafka,,bigdata,how to use lenses as a secure   layer to     into apache kafka 
t3_dtbtui,Top 6 Big Data Frameworks," 

## Big Data Frameworks-

There are many frameworks available in the market. Some of them are more popular and those are Spark, Hadoop, Hive and Storm. Whereas Presto score high on utility index and Flink has great potential. Also there are some others which need some mention like the Samza, Impala, Apache Pig, etc. Here we will discuss some of them.  


### 1. ApacheHadoop-

 

Hadoop is a Java- based platform. This is an open-source framework which provides batch data processing and data storage services across a group of hardware machines arranged in clusters. Hadoop is great for reliable, scalable and distributed calculations also. However, it can also be exploited as common purpose file storage. It can store and process petabytes of information. Hadoop consists of three main components.  


1. HDFS file system- It is responsible for the data storage in the Hadoop cluster;
2. MapReduce system- It is intended to process large volumes of data in a cluster;
3. YARN- It is a core that handles resource management.

#### Pros-

It gives cost-effective solution, high throughput, multi-language support, compatibility with most rising technologies in Big Data services. Also supports high scalability, fault tolerance, better suited for R&amp;D, high availability through amazing failure handling mechanism.

#### Cons-

It includes vulnerability to security breaks, doesn’t perform in-memory calculation hence suffers handling overheads, not appropriate for stream processing and real-time processing, issues in processing small files in huge numbers.

Organisations like Amazon, Adobe, AOL, Alibaba, EBay, and Facebook also uses Hadoop.

### 2. Apache Spark- 

 

The Spark framework was formed at the University of California, Berkeley. It is a batch processing framework with improved data streaming processing. With full in-memory computation and also handling optimization, it guarantees an extremely quick cluster computing system.

Spark framework is composed of five layers.

* HDFS and HBASE: They form the first layer of data storage systems. 
* YARN and Mesos: They form the resource management layer. 
* Core engine: This forms the third layer.
* Library: This structures the fourth layer containing Spark SQL for SQL queries while stream processing, GraphX and Spark R utilities for handling graph data and MLlib for machine learning algorithms.
* The fifth layer contains an application program interface, for example, Java or Scala.

Spark can work as an independent cluster alongside a capable storage layer or it can give consistent integration with Hadoop. It supports some popular languages like Python, R, Java and Scala also.

#### Pros-

1. Speed
2. Ease of Use
3. Advanced Analytics
4. Dynamic in Nature
5. Multilingual
6. Apache Spark is powerful
7. Increased access to Big data
8. Demand for Spark Developers
9. Open-source community

#### Cons-

Spark poses some cons like complexity of setup and implementation, language support limitation, not a genuine streaming engine.

### 3. Storm-

 

Apache Storm is another noticeable solution, focused on working with a huge real-time data flow. The key highlights of Storm are scalability and prompt restoring ability after downtime. You can work with this solution with the assistance of Java, Python, Ruby, and Fancy. Storm includes a few components that make it fundamentally not the same as analogs. The first is Tuple — a key data representation element that supports serialization. Then there is Stream that incorporates the scheme of naming fields in the Tuple. Spout gets data from external sources, forms the Tuple out of them, and sends them to the Stream. There is additionally Bolt, a data processor, and Topology, a package of elements with the description of their interrelation. When combined, all these elements help engineers to oversee huge flows of unstructured data.

Talking about performance, Storm gives better latency over both Flink and Spark. Notwithstanding, it has more terrible throughput. Recently Twitter moved to another framework Heron. Storm is as yet utilized by big organizations like Yelp, Yahoo!, Alibaba, and some others. It’s as yet going to have a huge client base and support in 2020.

### 4. Apache Flink-

 

Apache Flink is an open source framework, good for both batch and stream data processing also. It is best suited for cluster environments. This framework is based on transformations – streams concept. It is additionally the 4G of Big Data. It is the100 times faster than Hadoop – Map Reduce.

Flink framework consists of multiple layers-

* Deploy Layer
* Runtime Layer
* Library Layer

#### Pros-

Low latency, high throughput, fault tolerance, entry by entry processing, ease of batch and also stream data processing, compatibility with Hadoop.

#### Cons-

Few scalability issues.

### 5. Presto-

 

It is the open- source distributed SQL tool most appropriate for smaller datasets. Presto engine incorporates a coordinator and also various workers. When client submits queries, these are parsed, analysed, their execution planned and distributed for handling among the workers by the coordinator.

#### Pros-

1. least query degradation even in the event of increased concurrent query workload.
2. It has a query execution rate that is three times faster than Hive.
3. Ease in adding images and embedding links. 
4. Highly user-friendly.  

#### Cons-

1. Reliability issues

### 6. Samza-

Apache Samza is a stateful stream preparing Big Data system that was co-developed with Kafka. Kafka gives data serving, buffering, and fault tolerance. Both are combinedly  proposed to be utilized where rapid single-stage processing is required. With Kafka, it can be utilized with low latencies. Samza also saves local states during processing that give additional fault tolerance. It was designed for Kappa architecture but can be used in other architectures. Samza uses YARN to arrange resources. So it needs a Hadoop cluster to work, so that implies you can depend on highlights provided by YARN. This Big Data processing framework was developed for Linkedin and is also utilized by eBay and TripAdvisor for fraud discovery. A sizeable part of its code was utilized by Kafka to create a competing data processing framework Kafka streams.

## Conclusion-

There is no single framework that is best fit for all business needs. But, to feature some frameworks, Storm appears to be most appropriate for streaming while Spark is the winner for batch processing. For each organization or business, one’s very own data is most significant. Putting resources into Big Data structures includes spending. Numerous frameworks are freely accessible while some accompanied a cost. Contingent upon the project needs, benefit of preliminary versions offered. For appropriate choice, understand the objectives of the business. You can experiment with the framework on a smaller scale project to understand functioning more precisely. Investing in the right framework leads to the success of a business.",bigdata,top   big   frameworks     big   frameworks  there are many frameworks available in the market some of them are more popular   those are spark   hive   storm whereas presto score high on utility     flink has great potential also there are some others which   some mention like the samza impala apache pig etc here we will   some of them                is a java   platform this is an opensource framework which   batch   processing     storage services across a group of   machines   in clusters   is great for reliable scalable     calculations also however it can also be   as common purpose file storage it can store   process petabytes of information   consists of three main components         file system it is responsible for the   storage in the   cluster     system it is   to process large volumes of   in a cluster   yarn it is a core that   resource management   pros  it gives costeffective solution high throughput multilanguage support compatibility with most rising technologies in big   services also supports high scalability fault tolerance better   for   high availability through amazing failure   mechanism   cons  it   vulnerability to security breaks  ’t perform inmemory calculation hence suffers     not appropriate for stream processing   realtime processing issues in processing small files in huge numbers  organisations like amazon   aol alibaba ebay   facebook also uses       apache spark      the spark framework was   at the university of california berkeley it is a batch processing framework with     streaming processing with full inmemory computation   also   optimization it guarantees an extremely quick cluster computing system  spark framework is   of five layers       hbase they form the first layer of   storage systems   yarn   mesos they form the resource management layer   core engine this forms the   layer  library this structures the fourth layer containing spark sql for sql queries while stream processing graphx   spark r utilities for   graph     mllib for machine learning algorithms  the fifth layer contains an application program interface for example java or scala  spark can work as an   cluster   a capable storage layer or it can give consistent integration with   it supports some popular languages like python r java   scala also   pros        ease of use     analytics     in nature   multilingual   apache spark is powerful     access to big       for spark     opensource community   cons  spark poses some cons like complexity of setup   implementation language support limitation not a genuine streaming engine     storm     apache storm is another noticeable solution   on working with a huge realtime   flow the key highlights of storm are scalability   prompt restoring ability after   you can work with this solution with the assistance of java python ruby   fancy storm   a few components that make it   not the same as analogs the first is tuple — a key   representation element that supports serialization then there is stream that incorporates the scheme of naming   in the tuple spout gets   from external sources forms the tuple out of them     them to the stream there is   bolt a   processor   topology a package of elements with the   of their interrelation when   all these elements help engineers to oversee huge flows of      talking about performance storm gives better latency over both flink   spark   it has more terrible throughput recently twitter   to another framework heron storm is as yet   by big organizations like yelp yahoo alibaba   some others it’s as yet going to have a huge client base   support in          apache flink     apache flink is an open source framework   for both batch   stream   processing also it is best   for cluster environments this framework is   on transformations – streams concept it is   the  g of big   it is the    times faster than   – map    flink framework consists of multiple layers     layer  runtime layer  library layer   pros  low latency high throughput fault tolerance entry by entry processing ease of batch   also stream   processing compatibility with     cons  few scalability issues     presto     it is the open source   sql tool most appropriate for smaller   presto engine incorporates a     also various workers when client submits queries these are     their execution       for   among the workers by the     pros    least query   even in the event of   concurrent query     it has a query execution rate that is three times faster than hive   ease in   images     links    highly       cons    reliability issues     samza  apache samza is a stateful stream preparing big   system that was   with kafka kafka gives   serving buffering   fault tolerance both are      to be   where   singlestage processing is   with kafka it can be   with low latencies samza also saves local states   processing that give   fault tolerance it was   for kappa architecture but can be   in other architectures samza uses yarn to arrange resources so it   a   cluster to work so that implies you can   on highlights   by yarn this big   processing framework was   for     is also   by ebay     for     a sizeable part of its   was   by kafka to create a competing   processing framework kafka streams   conclusion  there is no single framework that is best fit for all business   but to feature some frameworks storm appears to be most appropriate for streaming while spark is the winner for batch processing for each organization or business one’s very own   is most significant putting resources into big   structures     numerous frameworks are freely accessible while some   a cost contingent upon the project   benefit of preliminary versions   for appropriate choice   the objectives of the business you can experiment with the framework on a smaller scale project to   functioning more precisely investing in the right framework   to the success of a business
t3_dta9r3,Publishing and running models in Hadoop on SAS Viya,,bigdata,publishing   running   in   on sas viya 
t3_dtdeqc,Learn Machine Learning from Scratch,,bigdata,learn machine learning from scratch 
t3_dtbpf4,Listing Tables And Columns In Postgres (Tutorial),"Data exploration, or data profiling, is the first step in sound data analysis. The point here is to get familiar with and understand the data you are working with. The following tutorial walks you through the steps of exploring your data using PostgreSQL: [Listing Tables And Columns In Postgres: A PG_TABLE_DEF Tutorial](https://blog.panoply.io/exploring-data-in-postgres)

PG_TABLE_DEF is a table (actually a view) that contains metadata about the tables in a database. It is kind of like a directory for all of the data in your database and gives you all of the schemas, tables and columns and helps you to see the relationships between them.",bigdata,listing tables   columns in postgres tutorial   exploration or   profiling is the first step in     analysis the point here is to get familiar with     the   you are working with the following tutorial walks you through the steps of exploring your   using postgresql      is a table actually a view that contains   about the tables in a   it is   of like a   for all of the   in your     gives you all of the schemas tables   columns   helps you to see the relationships between them
t3_dst5pw,Comparing 5 Data Preparation Tools vs. Automated Data Platform,"Data preparation, part of the data management process, involves collecting raw data from multiple sources and consolidating it into a file or database for analysis. Data preparation is an initial step in data warehousing, data mining, and machine learning projects.

The data preparation process includes the following activities:

* Data ingestion—copying or loading data from different data sources.
* Data fusion—integrating multiple data sources to create one consistent representation.
* Data cleansing—ensuring data is valid, complete, consistent, uniform, and accurate.
* Data augmentation—adding information that increases value—for example, enriching sales leads data with contact details.

Full article: [5 Data Preparation Tools &amp; 1 Automated Data Platform](https://blog.panoply.io/5-data-preparation-tools-1-automated-data-platform)",bigdata,comparing     preparation tools vs     platform   preparation part of the   management process involves collecting raw   from multiple sources     it into a file or   for analysis   preparation is an initial step in   warehousing   mining   machine learning projects  the   preparation process   the following activities     ingestion—copying or     from     sources    fusion—integrating multiple   sources to create one consistent representation    cleansing—ensuring   is   complete consistent uniform   accurate    augmentation—  information that increases value—for example enriching sales     with contact    full article  
t3_dsng7z,pySpark and small files problem on google Cloud Storage,"I am facing the following task: I have individual files (like Mb) stored in Google Cloud Storage Bucket grouped in directories by date (each directory contains around 5k files). I need to look at each file (xml) , filter proper one and put them into Mongo or write back to Google Cloud Storage in lets say parquet format. I wrote a simple pySpark program that looks like this:

    import pyspark
    from pyspark.sql import SparkSession
    from pyspark.sql.types import *
    
    spark = (
        SparkSession
        .builder
        .appName('myApp')
        .config(""spark.mongodb.output.uri"", ""mongodb://&lt;mongo_connection&gt;"") 
        .config(""spark.mongodb.output.database"", ""test"") 
        .config(""spark.mongodb.output.collection"", ""test"")
        .config(""spark.hadoop.google.cloud.auth.service.account.enable"", ""true"")
        .config(""spark.dynamicAllocation.enabled"", ""true"")
        .getOrCreate()
    )
    
    spark_context = spark.sparkContext
    spark_context.setLogLevel(""INFO"")
    sql_context   = pyspark.SQLContext(spark_context)
    
    # configure Hadoop
    hadoop_conf = spark_context._jsc.hadoopConfiguration()
    hadoop_conf.set(""fs.gs.impl"", ""com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"")
    hadoop_conf.set(""fs.AbstractFileSystem.gs.impl"", ""com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"")
    
    
    # DataFrame schema
    schema = StructType([
        StructField('filename', StringType(), True),
        StructField(""date"", DateType(), True),
        StructField(""xml"", StringType(), True)
    ])
    
    # -------------------------
    # Main operation
    # -------------------------
    # get all files
    files = spark_context.wholeTextFiles('gs://bucket/*/*.gz')
    
    rows = files \
        .map(lambda x: custom_checking_map(x)) \
        .filter(lambda x: x is not None)
    
    # transform to DataFrame 
    df = sql_context.createDataFrame(rows, schema)
    
    # write to mongo
    df.write.format(""mongo"").mode(""append"").save()
    
    # write back to Cloud Storage
    df.write.parquet('gs://bucket/test.parquet')
    
    spark_context.stop()

I tested it on a subset (single directory gs://bucket/20191010/\*.gz  
) and it works. I deploy it on Google Dataproc cluster, but doubt anything is happening single the logs stop after 19/11/06 15:41:40 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application\_1573054807908\_0001

I am running 3 worker cluster with 4 cores and 15GB RAM + 500GB HDD. Spark version 2.3.3, scala 2.11 mongo-connector-spark\_2.11-2.3.3. I am new to Spark so any suggestions are appreciated. Normally, I would write this work using Python multiprocessing, but wanted to move to something ""better"", but now I am not sure.",bigdata,pyspark   small files problem on google   storage i am facing the following task i have   files like mb   in google   storage bucket   in   by   each   contains    k files i   to look at each file xml  filter proper one   put them into mongo or write back to google   storage in lets say parquet format i wrote a simple pyspark program that looks like this      import pyspark     from pysparksql import sparksession     from pysparksqltypes import           spark           sparksession                   appnamemyapp                        test            test           true           true         getorcreate               sparkcontext  sparksparkcontext     sparkcontextsetloglevelinfo     sqlcontext    pysparksqlcontextsparkcontext           configure                                             schema     schema  structtype           stringtype true             true           stringtype true                      main operation            get all files     files  sparkcontextwholetextfilesgsbucketgz          rows  files            x customcheckingmapx            x x is not none           transform to             schema           write to mongo                 write back to   storage                sparkcontextstop  i   it on a subset single   gsbucket        gz      it works i   it on google   cluster but   anything is happening single the logs stop after               info     application application                   i am running   worker cluster with   cores     gb ram     gb   spark version     scala     mongoconnectorspark       i am new to spark so any suggestions are   normally i   write this work using python multiprocessing but   to move to something better but now i am not sure
t3_dsgwgy,Datameer raises $40 million for data set prep and analysis tools,,bigdata,  raises    million for   set prep   analysis tools 
t3_dsav5c,"Big Data As A Service: IaaS, PaaS And SaaS","The article compares three models for big data infrastructure in the cloud mirror the three models of cloud computing: [Big Data As A Service: IaaS, PaaS And SaaS](https://blog.panoply.io/big-data-as-a-service-iaas-paas-and-saas)

* Big Data Infrastructure as a Service (IaaS)—“bare bones” data services from a cloud provider
* Big Data Platform as a Service (PaaS)—cloud-based offerings like Amazon S3 and Redshift or EMR provide a complete big data stack, except for ETL and BI
* Big Data Software as a Service (SaaS)—an end-to-end big data stack in one tool",bigdata,big   as a service iaas paas   saas the article compares three   for big   infrastructure in the   mirror the three   of   computing     big   infrastructure as a service iaas—“bare bones”   services from a      big   platform as a service paas—  offerings like amazon s      or emr   a complete big   stack except for etl   bi  big   software as a service saas—an   big   stack in one tool
t3_dsapgb,Winning in retail with IBM Watson Knowledge Catalog,,bigdata,winning in retail with ibm watson   catalog 
t3_drxt82,Top 10 Big Data Tools that you should know about,,bigdata,top    big   tools that you   know about 
t3_drtvzw,Self-promotion and blogspam,"I typically lurk but the links to the poster's own blog and promoting their own pages has been really bad for a while now. 

I know that the blatant links are against the rules but that doesn't seem to be enforced much. Could we flair bloggers/self promoters so we know who is pushing their own media? Maybe put self-promotion in a single thread. 

The front page of /r/bigdata has 5 of the top posts submitted by their writers, one of them has 2 self-promoted posts.

It's interesting, but not really constructive. Especially when the posts are regurgitation the same things we've see hundreds of times before... are there really people here that don't know what a histogram is? Why color matters? That knowing your audience is of value?",bigdata,selfpromotion   blogspam i typically lurk but the links to the posters own blog   promoting their own pages has been really   for a while now   i know that the blatant links are against the rules but that   seem to be   much   we flair bloggersself promoters so we know who is pushing their own   maybe put selfpromotion in a single     the front page of   has   of the top posts   by their writers one of them has     posts  its interesting but not really constructive especially when the posts are regurgitation the same things weve see   of times before are there really people here that   know what a histogram is why color matters that knowing your   is of value
t3_dru9v3,How Data Warehouses Make Data Mining Easier - Overview,"The article explains the benefits of how an automated data warehouse make data mining easier: [How Your Data Warehouse Can Make Data Mining Easier And More Efficient](https://blog.panoply.io/how-your-data-warehouse-can-make-data-mining-easier-and-more-efficient)

A data warehouse is a system that collates data from a wide range of sources within an organization. Data warehouses are used as centralized data repositories for analytical and reporting purposes.",bigdata,how   warehouses make   mining easier  overview the article explains the benefits of how an     warehouse make   mining easier    a   warehouse is a system that collates   from a   range of sources within an organization   warehouses are   as     repositories for analytical   reporting purposes
t3_drhhxe,An interview about how the Ascend platform provides an autonomous data orchestration platform to simplify your production dataflows,,bigdata,an interview about how the   platform   an autonomous   orchestration platform to simplify your     
t3_drccha,4 Steps to Building an Awesome Big Data Solution on Microsoft Azure,,bigdata,  steps to   an awesome big   solution on microsoft azure 
t3_dqxi2m,Spark tips. Don't collect data on driver,,bigdata,spark tips   collect   on   
t3_dqydg9,"File Formats for Machine Learning: : Columnar, Training, and Inferencing",,bigdata,file formats for machine learning  columnar training   inferencing 
t3_dqyczf,Delta vs. Hudi vs. Iceberg with a Feature Store [video],,bigdata,  vs   vs iceberg with a feature store   
t3_dqsm7w,Industrial Internet of Things &amp; Factories of the Future,,bigdata,  internet of things  factories of the future 
t3_dqxb9h,Cloudera and Hortonworks announce merger,,bigdata,    hortonworks announce merger 
t3_dqhlbo,Help setting up Spark on a single machine,"I have to implement some ML algorithms using both Hadoop and Spark and compare their running times. I have been asked to setup a ""1 master 4 slave configuration"" on a single machine so that I can simulate a multi node environment. I have been able to do this on Hadoop, with 1 namenode and 4 datanodes running on my PC. What I cannot figure out is the Spark equivalent of namenodes and datanodes. Most Spark tutorials I have found online just start a SparkContext and start coding, so I am assuming that these are using 1 datanode by default. Can someone link to a tutorial that explains how to do this for Spark?",bigdata,help setting up spark on a single machine i have to implement some ml algorithms using both     spark   compare their running times i have been   to setup a   master   slave configuration on a single machine so that i can simulate a multi   environment i have been able to   this on   with           running on my pc what i cannot figure out is the spark equivalent of       most spark tutorials i have   online just start a sparkcontext   start   so i am assuming that these are using     by   can someone link to a tutorial that explains how to   this for spark
t3_dq5ias,Top 8 Data Science Use Cases in Support,,bigdata,top     science use cases in support 
t3_dpzlju,Top Pros and Cons of Hadoop,,bigdata,top pros   cons of   
t3_dpi0h1,"MariaDB Vs MySQL In 2019: Compatibility, Performance, And Syntax","* MySQL has generated a strong following since it was started in 1995. Some organizations that use MySQL include GitHub, US Navy, NASA, Tesla, Netflix, WeChat, Facebook, Zendesk, Twitter, Zappos, YouTube, Spotify.

* MariaDB is being used by many large corporations, Linux distributions, and more. Some organizations that use MariaDB include Google, Craigslist, Wikipedia, archlinux, RedHat, CentOS, and Fedora.

Full article: [MariaDB Vs MySQL In 2019: Compatibility, Performance, And Syntax](https://blog.panoply.io/a-comparative-vmariadb-vs-mysql)

Since MariaDB is a fork of MySQL, the database structure and indexes of MariaDB are the same as MySQL. This allows you to switch from MySQL to MariaDB without having to alter your applications.

Gradually, MySQL and MariaDB will diverge. A noteworthy example is the internal data dictionary that is currently under development for MySQL 8. This is a major change to the way metadata is stored and used within the server. MariaDB doesn't have an equivalent feature. This may mark the end of datafile-level compatibility between MySQL and MariaDB.",bigdata,  vs mysql in      compatibility performance   syntax  mysql has   a strong following since it was   in      some organizations that use mysql   github us navy nasa tesla netflix wechat facebook   twitter zappos youtube spotify     is being   by many large corporations linux     more some organizations that use     google craigslist   archlinux   centos      full article    since   is a fork of mysql the   structure     of   are the same as mysql this allows you to switch from mysql to   without having to alter your applications    mysql     will   a noteworthy example is the internal     that is currently     for mysql   this is a major change to the way   is       within the server     have an equivalent feature this may mark the   of   compatibility between mysql    
t3_dplozz,Data journalism: a guide for editors,,bigdata,  journalism a   for   
t3_dp4y4v,How Big Data is Changing the Finance Industry -Big Data Analytics News,,bigdata,how big   is changing the finance   big   analytics news 
t3_dp41q5,Why most companies that use AI fail,,bigdata,why most companies that use ai fail 
t3_dp4nu0,Big Data Business Impacts,What are some of the big data business impacts and what are some uncommon ones that businesses would aim to have in their company which would help them to increase both their profit margin and have an effective frontline department where a data-driven culture can be achieved? Thanks!! :),bigdata,big   business impacts what are some of the big   business impacts   what are some uncommon ones that businesses   aim to have in their company which   help them to increase both their profit margin   have an effective frontline   where a   culture can be   thanks 
t3_dp2quv,Big Data Is Too Big Without AI,,bigdata,big   is too big without ai 
t3_dovd0k,What application for query Hive database?,"Hello. Im trying to find a proper application to work with hql queries on a Hive db, similar to nay other sql editor (sql developer, plsql developer,  Aginity) .
So far I'm using the usual terminal but it represents it's difficulties when a simple query and get that information to an excel file is needed.

Any open source tools out there?

Thanks in advance",bigdata,what application for query hive   hello im trying to   a proper application to work with hql queries on a hive   similar to nay other sql   sql   plsql    aginity  so far im using the usual terminal but it represents its   when a simple query   get that information to an excel file is    any open source tools out there  thanks in  
t3_doma9s,Workshops/conferences related to big data,I just wanted to know if there's a website or something which has such info.,bigdata,workshopsconferences   to big   i just   to know if theres a website or something which has such info
t3_dorbjv,Artificial Neural Networks FAQs answered," https://www.techslang.com/how-does-an-artificial-neural-network-work/

This article might be useful for beginners. It explains how ANNs work in simple terms (no scary jargon!). Hope this will help those who are interested in NNs but were too afraid to ask.",bigdata,artificial neural networks faqs      this article might be useful for beginners it explains how anns work in simple terms no scary jargon hope this will help those who are   in nns but were too   to ask
t3_doos94,6 Ways Big Data is Changing the Healthcare Industry,,bigdata,  ways big   is changing the healthcare   
t3_dokdrw,5 Opportunities To Scale Your Business With Data,"Using your company’s data is imperative to understanding how to scale, and at what pace. The following overview looks at ways that data can be leveraged for business scaling and explains 5 ways on how to tap your data to do it successfully: [5 Opportunities To Scale Your Business With Data](https://blog.panoply.io/5-tips-on-leveraging-your-data-to-scale-your-business)

* Improve Productivity
* Increase Opportunities
* Reduce Bottlenecks
* Streamline
* Achieve Personalization at Scale",bigdata,  opportunities to scale your business with   using your company’s   is imperative to   how to scale   at what pace the following overview looks at ways that   can be   for business scaling   explains   ways on how to tap your   to   it successfully     improve    increase opportunities    bottlenecks  streamline  achieve personalization at scale
t3_do8t4i,The Complete Data Science LinkedIn Profile Guide,https://bigcloud.io/the-complete-data-science-linkedin-profile-guide/,bigdata,the complete   science   profile   
t3_do98bj,An interview about the Dagster framework and how you can use it to build testable and maintainable data applications,,bigdata,an interview about the   framework   how you can use it to   testable   maintainable   applications 
t3_do8a2v,Top 5 Big Data Trends To Look Out For In 2019 [Infographic],,bigdata,top   big     to look out for in        
t3_dnb144,Post Graduation In Analytics,Hi All. Can anyone guide me to a good PG course in Analytics available in India?,bigdata,post   in analytics hi all can anyone   me to a   pg course in analytics available in  
t3_dnae64,https://www.analyticsinsight.net/big-data-can-improve-health-mothers-children/,,bigdata, 
t3_dmyo57,How to Migrate Self Managed Kafka to HDInsight via Gitops - Walkthrough,,bigdata,how to migrate self   kafka to   via gitops  walkthrough 
t3_dn24ql,Milestone: BERT Boosts Google Search,,bigdata,milestone bert boosts google search 
t3_dmkvte,FastSpark: A New Fast Native Implementation of Spark from Scratch,,bigdata,fastspark a new fast native implementation of spark from scratch 
t3_dmje32,How to use SQL and Lenses Data API to explore Streaming Data in Apache Kafka,,bigdata,how to use sql   lenses   api to explore streaming   in apache kafka 
t3_dmrhpz,Cloud Pak for Data: The Developer’s journey in a data and AI platform,,bigdata,  pak for   the  ’s journey in a     ai platform 
t3_dmr3du,What is Big Data?,"It is often defined as a way to respond to a massive volume of data, hence the term BIG DATA (or massive data in French).

      The problem with this definition is that we forget a fundamental concept of Big Data, because Big Data is to deal with large volumes of data, but the main issue of Big Data is to  value these data regardless of their volume.

      Technological transformations necessary to valorize these data.

      Today, companies are facing an exponential increase in data.  To give you a more precise idea, know that this mass of data can reach up to several petabytes of data, and that moreover these data are of various natures.

      For example, we can have data from logs, social networks, e-commerce transactions, data analysis, internet of things, image, audio,  video, etc.
        Original site .. https://dailytechmonde.blogspot.com/2019/10/big-data.html?m=1",bigdata,what is big   it is often   as a way to   to a massive volume of   hence the term big   or massive   in french        the problem with this   is that we forget a   concept of big   because big   is to   with large volumes of   but the main issue of big   is to  value these     of their volume        technological transformations necessary to valorize these            companies are facing an exponential increase in    to give you a more precise   know that this mass of   can reach up to several petabytes of     that moreover these   are of various natures        for example we can have   from logs social networks ecommerce transactions   analysis internet of things image      etc         original site  
t3_dma5kd,A Beginner’s Guide To Formatting Dates In SQL,"The purpose for the following overview is to create a guide that is easy to understand even for a beginner and to include queries you can copy and paste into your SQL editor right now: [A Beginner’s Guide To Formatting Dates In SQL](https://blog.panoply.io/a-beginners-guide-to-formatting-dates-in-sql)

The information is mostly specific to PostgreSQL, but many of the basic commands are the same in different varieties (Microsoft SQL Server, MySQL, PostgreSQL etc).",bigdata,a beginner’s   to formatting   in sql the purpose for the following overview is to create a   that is easy to   even for a beginner   to   queries you can copy   paste into your sql   right now    the information is mostly specific to postgresql but many of the basic   are the same in   varieties microsoft sql server mysql postgresql etc
t3_dm5cof,[New Book] Data-intensive Systems - Principles and Fundamentals using Hadoop and Spark | Tomasz Wiktorski | Springer,,bigdata,    systems  principles     using     spark  tomasz wiktorski  springer 
t3_dlxu34,The fake news of big data- an inability to reproduce,,bigdata,the fake news of big   an inability to   
t3_dlz2ly,Question: Data Validation For Financial Instruments in Investment Management System,"Task:
I am in charge of validating and providing sign off for a large data enhancement (security characteristics such as duration, delta, notional values, etc.) for financial instruments (primarily derivatives) in a compliance monitoring system (Charles River). 

Problem:
I have already identified incorrect data mapping for some elements, but I don’t have enough time to analyze the data at the speed I have been moving. 

Variables:
The securities whose data is being enhanced comes from an accounting system. An identifier for each security is then used to pull in data characteristics from a convoluted web of numerous different data tables that I do not have direct access to. 

I need to match shares, market values, notional values from our compliance monitoring system to our accounting system. 

I’m struggling to find a way to analyze the accuracy of such a large data set. Does anyone have any advice or can you point me to some examples or share some best practices? 

 Thanks!",bigdata,question     for financial instruments in investment management system task i am in charge of       sign off for a large   enhancement security characteristics such as     notional values etc for financial instruments primarily   in a compliance monitoring system charles river   problem i have     incorrect   mapping for some elements but i  ’t have enough time to analyze the   at the   i have been moving   variables the securities whose   is being   comes from an accounting system an   for each security is then   to pull in   characteristics from a   web of numerous     tables that i   not have   access to   i   to match shares market values notional values from our compliance monitoring system to our accounting system   i’m struggling to   a way to analyze the accuracy of such a large   set   anyone have any   or can you point me to some examples or share some best practices    thanks
t3_dltvij,5 Ways Big Data is Transforming Artificial Intelligence,,bigdata,  ways big   is transforming artificial intelligence 
t3_dls9ru,Free Webcast to build 3-D maps in R,"Want to learn how to build engaging 3-D maps in R.  [Register](https://www.eventbrite.com/e/musamasterclass-featuring-tyler-morgan-wall-3d-mapping-and-dataviz-in-r-tickets-74481308599) for a FREE Masterclass, on-campus and live webcast, featuring [Tyler Morgan-Wall](https://twitter.com/tylermorganwall) (creator of RayShader).  [Register here!](https://www.eventbrite.com/e/musamasterclass-featuring-tyler-morgan-wall-3d-mapping-and-dataviz-in-r-tickets-74481308599) 

My masters program in [Urban Spatial Analysis](https://www.design.upenn.edu/musa/about) at the University of Pennsylvania is running the event and you can join through the webcast on Friday, November 15th from 1-5 PM.  

https://preview.redd.it/997mo8rk37u31.jpg?width=3300&amp;format=pjpg&amp;auto=webp&amp;s=43e91b42c9497f07210ef0b192ce12e5e682b09d",bigdata,free webcast to     maps in r want to learn how to   engaging   maps in r    for a free masterclass oncampus   live webcast featuring   creator of        my masters program in   at the university of pennsylvania is running the event   you can join through the webcast on   november   th from    pm    
t3_dll7zn,Harvard &amp; Google Seismic Paper Hit With Rebuttals: Is Deep Learning Suited to Aftershock Prediction?,,bigdata,   google seismic paper hit with rebuttals is   learning   to aftershock   
t3_dlsj3v,When is the old data in an ORC ACID table really gone?,"We're looking at how to handle data deletion requests, and some of our data is in ORC ACID tables. Reading up, I think I understand how this works.  I don't see anything specifically addressing the privacy aspect, though, so I wanted to check.

Here's my understanding:

* When you delete a row or update columns to be null or masked, that is recorded in a delta file, but the original value in the base file is still retrievable if you're determined.
* When a minor compaction is run, performance will improve as there are fewer delta files, but the original values are probably still retrievable. (maybe not, if they were only ever in the deltas)
* When a major compaction is run, the partition is rebuilt, and the old version is deleted after any pending queries are completed. At that point, the old values are really gone (apart from backups).

Is that right?",bigdata,when is the     in an orc   table really gone were looking at how to       requests   some of our   is in orc   tables   up i think i   how this works  i   see anything specifically   the privacy aspect though so i   to check  heres my     when you   a row or   columns to be null or   that is   in a   file but the original value in the base file is still retrievable if youre    when a minor compaction is run performance will improve as there are fewer   files but the original values are probably still retrievable maybe not if they were only ever in the    when a major compaction is run the partition is rebuilt   the   version is   after any   queries are   at that point the   values are really gone apart from backups  is that right
t3_dlh42z,An interview about the emerging category of data orchestration platforms and how they can be used to bridge the gap between modern and legacy analytics systems,,bigdata,an interview about the emerging category of   orchestration platforms   how they can be   to   the gap between     legacy analytics systems 
t3_dllol5,Static analysis of CERN's ROOT data analysis framework source code,,bigdata,static analysis of cerns root   analysis framework source   
t3_dlinnv,Data — from objects to assets,,bigdata,  — from objects to assets 
t3_dli3k6,Topic modeling with Gensim and Python using Amazon S3 data,,bigdata,topic   with gensim   python using amazon s    
t3_dlh6v1,Who is a bigger threat: Humans or AI?,,bigdata,who is a bigger threat humans or ai 
t3_dleppr,10 Top Big Data Companies in USA 2020,,bigdata,   top big   companies in usa      
t3_dl2eo0,"Databricks introduces MLflow Model Registry, brings Delta Lake to Linux Foundation",,bigdata,    mlflow   registry brings   lake to linux   
t3_dkycza,Big Data in Banking - Spectacular Case Studies &amp; Applications,,bigdata,big   in banking  spectacular case    applications 
t3_dkwbeq,A Quick Comparison of the Five Best Big Data Frameworks,,bigdata,a quick comparison of the five best big   frameworks 
t3_dkl3g8,Big data questionnare for diploma thesis [Help and feedback needed],,bigdata,big   questionnare for   thesis   
t3_dkuhci,MySQL As A Data Warehouse: Is It A Best Option - Pros &amp; Cons,"The following overview looks at how MySQL performs for data warehousing and analytics tasks, and how dedicated data warehouses are better than MySQL databases when it comes to analysis: [MySQL As A Data Warehouse: Is It Really Your Best Option?](https://blog.panoply.io/mysql-as-a-data-warehouse-is-it-really-your-best-option)",bigdata,mysql as a   warehouse is it a best option  pros  cons the following overview looks at how mysql performs for   warehousing   analytics tasks   how     warehouses are better than mysql   when it comes to analysis  
t3_dkkghj,Faster ClickHouse Imports,,bigdata,faster clickhouse imports 
t3_dkhf59,Dynamic pricing through machine learning,,bigdata,  pricing through machine learning 
t3_dkh1b3,Learn Complete Machine Learning from Scratch,,bigdata,learn complete machine learning from scratch 
t3_dk4fod,What areas are there where big data is helping out HR?,,bigdata,what areas are there where big   is helping out hr 
t3_dk1go1,How does Apache Ignite get access to memory regions outside the JVM? Does it pre-allocate some memory? How will the OS give access to other parts of memory to an Ignite client?,,bigdata,how   apache ignite get access to memory regions   the jvm   it preallocate some memory how will the os give access to other parts of memory to an ignite client 
t3_dk66vu,How to Use Big Data Analytics to Grow Your Marketing ROI,,bigdata,how to use big   analytics to grow your marketing roi 
t3_djz5ye,Disco: Erlang/Python MapReduce Part 2 (Python Payload),,bigdata,  erlangpython   part   python   
t3_djp3r9,What is Exascale Computing?,,bigdata,what is exascale computing 
t3_djqwwb,Top Analytics &amp; Business Intelligence Trends for 2019,,bigdata,top analytics  business intelligence   for      
t3_djjg57,Big Data in Retail Industry [Case Studies] - Take your Business to Next Level!!,,bigdata,big   in retail      take your business to next level 
t3_dji8zf,A Brief Introduction Of Big Data Framework,,bigdata,a brief   of big   framework 
t3_djcgf9,Simplifying the data pipeline,,bigdata,simplifying the   pipeline 
t3_dj2vmp,Mainframe to Big Data - Why you should switch your career today?,,bigdata,mainframe to big    why you   switch your career   
t3_dj66ew,Ideas for Big Data presentation,"I don't know if this is the right place to ask about this, but I am learning about big data and I was tasked to come up with a presentation where we use big data to solve a problem and ""sell"" the idea to a sponsor (be it inside your own company as someone to push for your project or to an investor as a business idea)

The point they're trying to get to is to have us think in more practical terms on where Big Data actually is needed and how we can present our ideas to get an actual project moving. Basically dispel the idea that we should use Big Data because it is the new thing and actually come up with a legitimate use case which would benefit from using these technologies.

Because of my background I gravitated towards some potential use cases in aerospace, but I don't think these are great choices for this. I'd say it's stressed that the data could reasonably be obtained, they actually could use Big Data and not just traditional analysis and that there is reasonable value generated (coming back to the idea of getting a sponsor)

Some of the things I've thought about could be:

-prediction of traffic

-airline delay analysis

-Tourism analysis

He gave two examples, one being a digital mirror that could say give suggestions on what clothes to wear/buy according to similarity to your tastes or another project about a green city movement that I think had to do with maintaining green areas.

TLDR: I was just hoping to get some ideas on more grounded projects that could be interesting to explore and see how Big Data could be useful and implemented.",bigdata,  for big   presentation i   know if this is the right place to ask about this but i am learning about big     i was   to come up with a presentation where we use big   to solve a problem   sell the   to a sponsor be it   your own company as someone to push for your project or to an investor as a business    the point theyre trying to get to is to have us think in more practical terms on where big   actually is     how we can present our   to get an actual project moving basically   the   that we   use big   because it is the new thing   actually come up with a legitimate use case which   benefit from using these technologies  because of my   i     some potential use cases in aerospace but i   think these are great choices for this   say its   that the     reasonably be   they actually   use big     not just   analysis   that there is reasonable value   coming back to the   of getting a sponsor  some of the things ive thought about   be    of traffic  airline   analysis  tourism analysis  he gave two examples one being a   mirror that   say give suggestions on what clothes to wearbuy   to similarity to your tastes or another project about a green city movement that i think   to   with maintaining green areas    i was just hoping to get some   on more   projects that   be interesting to explore   see how big     be useful    
